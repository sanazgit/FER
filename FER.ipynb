{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanazgit/FER/blob/main/FER.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/Gabrella/QOT/blob/main/main_Upload.py"
      ],
      "metadata": {
        "id": "ERn_LJEI16nk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sByzx98bI4HG",
        "outputId": "e64db66c-4391-47f2-fb7a-fd29f94d476c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Github commands**"
      ],
      "metadata": {
        "id": "GTO6g1pNhw2F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.email \"sani.enmail@gmail.com\"\n",
        "!git config --global user.name \"sanazgit\""
      ],
      "metadata": {
        "id": "mmp-aIukFz-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://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@github.com/sanazgit/FER.git"
      ],
      "metadata": {
        "id": "08tNJ2knF-il",
        "outputId": "537a5118-b244-4fba-a63d-0e9736f332ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'FER'...\n",
            "remote: Enumerating objects: 74, done.\u001b[K\n",
            "remote: Counting objects: 100% (74/74), done.\u001b[K\n",
            "remote: Compressing objects: 100% (67/67), done.\u001b[K\n",
            "remote: Total 74 (delta 25), reused 15 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (74/74), 68.43 KiB | 265.00 KiB/s, done.\n",
            "Resolving deltas: 100% (25/25), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/FER/"
      ],
      "metadata": {
        "id": "xqeFViWnGIiK",
        "outputId": "378f3493-9667-42ac-e201-3ffb5dbade62",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/FER\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!touch main_upload.py"
      ],
      "metadata": {
        "id": "KIbr4fKlGKw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git add option.py"
      ],
      "metadata": {
        "id": "QDtowGq-J4yD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git status"
      ],
      "metadata": {
        "id": "NM8rg2W0J_go",
        "outputId": "901f5c09-b052-4fb7-ea56-92741b6649c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "On branch main\n",
            "Your branch is up to date with 'origin/main'.\n",
            "\n",
            "Untracked files:\n",
            "  (use \"git add <file>...\" to include in what will be committed)\n",
            "\t\u001b[31mmain_upload.py\u001b[m\n",
            "\n",
            "nothing added to commit but untracked files present (use \"git add\" to track)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git log --oneline"
      ],
      "metadata": {
        "id": "S_UQpNTPKEtP",
        "outputId": "06cdf80f-c05f-4dc9-d8ee-815ef728f90f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m568a1eb\u001b[m\u001b[33m (\u001b[m\u001b[1;36mHEAD -> \u001b[m\u001b[1;32mmain\u001b[m\u001b[33m, \u001b[m\u001b[1;31morigin/main\u001b[m\u001b[33m, \u001b[m\u001b[1;31morigin/HEAD\u001b[m\u001b[33m)\u001b[m Add files via upload\n",
            "\u001b[33mea002d3\u001b[m Created using Colaboratory\n",
            "\u001b[33me511dee\u001b[m Created using Colaboratory\n",
            "\u001b[33m751ba0e\u001b[m Created using Colaboratory\n",
            "\u001b[33m3424d7c\u001b[m Created using Colaboratory\n",
            "\u001b[33m77cc4eb\u001b[m Created using Colaboratory\n",
            "\u001b[33m172a70c\u001b[m Update README.md\n",
            "\u001b[33ma968d57\u001b[m Update README.md\n",
            "\u001b[33m189cb0d\u001b[m Update README.md\n",
            "\u001b[33mfe3e4aa\u001b[m Update README.md\n",
            "\u001b[33m8307b46\u001b[m Update README.md\n",
            "\u001b[33mb8d18f8\u001b[m Update README.md\n",
            "\u001b[33mad52f99\u001b[m Update README.md\n",
            "\u001b[33m821e242\u001b[m Update README.md\n",
            "\u001b[33m6e22033\u001b[m Created using Colaboratory\n",
            "\u001b[33m195b8f0\u001b[m add option.py\n",
            "\u001b[33m175be72\u001b[m Delete p1.py\n",
            "\u001b[33mffb19fe\u001b[m Delete test directory\n",
            "\u001b[33m46a40a9\u001b[m hhii\n",
            "\u001b[33md61e891\u001b[m ch\n",
            "\u001b[33mfee0a3b\u001b[m new file\n",
            "\u001b[33mdb610ae\u001b[m Created using Colaboratory\n",
            "\u001b[33m50d3ad0\u001b[m Created using Colaboratory\n",
            "\u001b[33m96431d7\u001b[m test\n",
            "\u001b[33m66d513a\u001b[m Initial commit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git commit -m \"add option.py\""
      ],
      "metadata": {
        "id": "W3lLOlccKM3h",
        "outputId": "1044e166-708e-417e-eb74-7022bafbd861",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "On branch main\n",
            "Your branch is up to date with 'origin/main'.\n",
            "\n",
            "Untracked files:\n",
            "  (use \"git add <file>...\" to include in what will be committed)\n",
            "\t\u001b[31mmain_upload.py\u001b[m\n",
            "\n",
            "nothing added to commit but untracked files present (use \"git add\" to track)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git push"
      ],
      "metadata": {
        "id": "OjWHUtb3Krcq",
        "outputId": "2d30784f-b75f-4229-f5c5-721139503534",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Everything up-to-date\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://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@github.com/robert1818118/CFNet.git"
      ],
      "metadata": {
        "id": "llNIx5xJZLYA",
        "outputId": "ae2c2aa5-52f6-4d71-f64b-68cba6d21808",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CFNet'...\n",
            "remote: Enumerating objects: 176, done.\u001b[K\n",
            "remote: Counting objects: 100% (66/66), done.\u001b[K\n",
            "remote: Compressing objects: 100% (63/63), done.\u001b[K\n",
            "remote: Total 176 (delta 20), reused 0 (delta 0), pack-reused 110\u001b[K\n",
            "Receiving objects: 100% (176/176), 34.50 KiB | 265.00 KiB/s, done.\n",
            "Resolving deltas: 100% (63/63), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **New Method**"
      ],
      "metadata": {
        "id": "IXmCBgvxhw42"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://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@github.com/liuhw01/AMP-Net.git"
      ],
      "metadata": {
        "id": "fDA_pkMOe-dE",
        "outputId": "ca50fccf-d1aa-4fc9-d655-8de9ac22cf70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'AMP-Net'...\n",
            "remote: Enumerating objects: 294, done.\u001b[K\n",
            "remote: Counting objects: 100% (236/236), done.\u001b[K\n",
            "remote: Compressing objects: 100% (140/140), done.\u001b[K\n",
            "remote: Total 294 (delta 125), reused 191 (delta 90), pack-reused 58\u001b[K\n",
            "Receiving objects: 100% (294/294), 1.56 MiB | 4.15 MiB/s, done.\n",
            "Resolving deltas: 100% (143/143), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchsnooper"
      ],
      "metadata": {
        "id": "tU1aNkSQjYF7",
        "outputId": "b0ba95ea-a1f4-47a9-c026-da9102f998f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchsnooper\n",
            "  Downloading TorchSnooper-0.8-py3-none-any.whl (7.2 kB)\n",
            "Collecting pysnooper>=0.1.0 (from torchsnooper)\n",
            "  Downloading PySnooper-1.2.0-py2.py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchsnooper) (1.23.5)\n",
            "Installing collected packages: pysnooper, torchsnooper\n",
            "Successfully installed pysnooper-1.2.0 torchsnooper-0.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# upload a pathon file with some useful functions\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "287ffd7b-2fcf-466d-939f-ebd399f74b74",
        "id": "vO8ThopFMv1b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-66cb37ac-11d6-45b3-8f50-2d47922a857b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-66cb37ac-11d6-45b3-8f50-2d47922a857b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving resnet_pose_estimation_model.py to resnet_pose_estimation_model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('./AMP-Net/model')"
      ],
      "metadata": {
        "id": "zirx7OrlfgxN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from model_ampnet import ampnet\n",
        "import numpy as np\n",
        "import argparse\n",
        "import torch\n",
        "from torchsummary import summary\n",
        "import random\n",
        "import time\n",
        "from torchvision import datasets, transforms\n",
        "import torch.utils.data as data\n",
        "import torchvision.models as models\n",
        "import os\n",
        "import torch.nn as nn\n",
        "import torch.backends.cudnn as cudnn\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "from util import *\n",
        "from PIL import Image\n",
        "import shutil\n",
        "import torch.nn.parallel\n",
        "import torch.optim\n",
        "import random\n",
        "import numbers\n",
        "import torch.nn.functional as F\n",
        "#import resnet_v1 as resnet\n",
        "import resnet_pose_estimation_model as resnet"
      ],
      "metadata": {
        "id": "B7VgGvVhhREc"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📑**Build Dataset**"
      ],
      "metadata": {
        "id": "kiYuNXfB1oHl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/Test_FER/Dataset/RAFDB/RAFDB.zip"
      ],
      "metadata": {
        "id": "OrrBnGBqierI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# Define the source and destination folders\n",
        "src_folder = './RAFDB/train/'\n",
        "dst_folder = './RAFDB/dataset/' # make dataset fplder into RAFDB\n",
        "\n",
        "# List all files in the source folder\n",
        "files = os.listdir(src_folder)\n",
        "\n",
        "# Loop through each file and move it to the destination folder\n",
        "for file in files:\n",
        "    # Construct full file path\n",
        "    src_path = os.path.join(src_folder, file)\n",
        "    dst_path = os.path.join(dst_folder, file)\n",
        "\n",
        "    # Check if the file is an image (e.g., .jpg, .png)\n",
        "    if file.endswith(('.jpg', '.png', '.jpeg')):\n",
        "        shutil.move(src_path, dst_path)\n",
        "\n",
        "print(\"Images moved successfully!\")\n"
      ],
      "metadata": {
        "id": "g1SSptcOn8Z-",
        "outputId": "6de192c0-55cd-4210-fd21-5dbb85544375",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Images moved successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_folder = './RAFDB/dataset/'\n",
        "image_files = [f for f in os.listdir(image_folder) if f.endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff'))]\n",
        "print(f\"There are {len(image_files)} images in the folder.\")"
      ],
      "metadata": {
        "id": "Q3quVgEePrA5",
        "outputId": "e86818df-327a-402a-f179-ad389a242459",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 15339 images in the folder.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Do for dataset AND data_facial\n",
        "\n",
        "# Specify the path to your image folder\n",
        "image_folder = '/content/RAFDB/dataset/'\n",
        "\n",
        "# Iterate over the images in the folder\n",
        "for image_file in os.listdir(image_folder):\n",
        "    # Check if \"_aligned\" is in the image name\n",
        "    if \"_aligned\" in image_file:\n",
        "        # Create the new name by replacing \"_aligned\" with an empty string\n",
        "        new_image_name = image_file.replace(\"_aligned\", \"\")\n",
        "\n",
        "        # Rename the image\n",
        "        os.rename(os.path.join(image_folder, image_file), os.path.join(image_folder, new_image_name))"
      ],
      "metadata": {
        "id": "mEO9iMBbqtLG"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init():\n",
        "\n",
        "  parser = argparse.ArgumentParser(description=\"PyTorch\")\n",
        "\n",
        "  parser.add_argument('--data', type=str, default='/content/RAFDB/dataset/')\n",
        "  parser.add_argument('--data_label', type=str, default='/content/drive/MyDrive/Test_FER/Dataset/RAFDB/data_label.txt')\n",
        "  parser.add_argument('--land_marks', type=str, default='/content/drive/MyDrive/Test_FER/Dataset/RAFDB/land_marks.npy')\n",
        "  parser.add_argument('--checkpoint_path', type=str, default='/content/drive/MyDrive/Test_FER/checkpoint_cnn/RAFDB/' + time_str +  'model.pth.tar')\n",
        "  parser.add_argument('--best_checkpoint_path', type=str, default='/content/drive/MyDrive/Test_FER/checkpoint_cnn/RAFDB/' +time_str + 'model_best.pth.tar')\n",
        "  parser.add_argument('--log_path', type=str, default='/content/drive/MyDrive/Test_FER/log/RAFDB/') #..save_path in ampnet\n",
        "  parser.add_argument('-j', '--workers', default=2, type=int, metavar='N', help='number of data loading workers') #..num_workers in ampnet\n",
        "  parser.add_argument('--epochs', default=100, type=int, metavar='N', help='number of total epochs to run')\n",
        "  parser.add_argument('--start-epoch', default=0, type=int, metavar='N', help='manual epoch number (useful on restarts)')\n",
        "  parser.add_argument('-b', '--batch-size', default=16, type=int, metavar='N')\n",
        "  parser.add_argument('--lr', '--learning-rate', default=0.001, type=float, metavar='LR', dest='lr')\n",
        "  parser.add_argument('--factor', default=0.1, type=float, metavar='FT')\n",
        "  parser.add_argument('--beta1',default=0.5,type=float,metavar='M', help='hyper-parameter ')\n",
        "  parser.add_argument('--af', '--adjust-freq', default=20, type=int, metavar='N', help='adjust learning rate frequency') #..print_freq in ampnet\n",
        "  parser.add_argument('-p', '--print-freq', default=10, type=int, metavar='N', help='print frequency')\n",
        "  parser.add_argument('--momentum', default=0.9, type=float, metavar='M')\n",
        "  parser.add_argument('--wd', '--weight-decay', default=1e-4, type=float, metavar='W', dest='weight_decay')\n",
        "  parser.add_argument('--resume', default=False, type=str, metavar='PATH', help='path to checkpoint')\n",
        "  parser.add_argument('--range', default=5, type=int, metavar='N', help='Intercept radius of AP-Module ')\n",
        "  parser.add_argument('--dataset', type=str, default='RAF')\n",
        "  parser.add_argument('--evaluate_path', type=str, default='' + time_str + 'model.pth.tar')\n",
        "  parser.add_argument('-e', '--evaluate', default=False, action='store_true', help='evaluate model on test set')\n",
        "\n",
        "  args = parser.parse_args(args=[])\n",
        "  return args"
      ],
      "metadata": {
        "id": "tdDg_pYmlS_y"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ✅**Check New Method**"
      ],
      "metadata": {
        "id": "a2iM6-zB2TOC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "now = datetime.datetime.now()\n",
        "time_str = now.strftime(\"[%m-%d]-[%H-%M]-\")"
      ],
      "metadata": {
        "id": "TrSI0kFf8r_R"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def select_data(label_path, data_facial):\n",
        "    data_facial_files = [item[0] for item in data_facial]\n",
        "\n",
        "    file = open(label_path)\n",
        "    lines = file.readlines()\n",
        "    train_label = []\n",
        "    test_label = []\n",
        "    for i in range(len(lines)):\n",
        "        num = int(lines[i][-2]) - 1\n",
        "        s1 = list(lines[i])\n",
        "        s1[-2] = str(num)\n",
        "        transformed_file = ''.join(s1)\n",
        "\n",
        "        if transformed_file.split()[0] not in data_facial_files:\n",
        "            continue\n",
        "\n",
        "        if lines[i][0:3] == 'tra':\n",
        "            train_label.append(transformed_file)\n",
        "        elif lines[i][0:3] == 'tes':\n",
        "            test_label.append(transformed_file)\n",
        "\n",
        "    return train_label, test_label  # output the list and delvery it into ImageFolder"
      ],
      "metadata": {
        "id": "4cGwF907tTyD"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args= init()\n",
        "\n",
        "data_root = args.data\n",
        "save_path=args.log_path\n",
        "lr=args.lr\n",
        "momentum=args.momentum\n",
        "weight_decay=args.weight_decay\n",
        "epochs=args.epochs\n",
        "batch_size = args.batch_size\n",
        "\n",
        "data_label =  args.data_label\n",
        "data_facial_path=args.land_marks\n",
        "data_facial= np.load(data_facial_path,allow_pickle=True)\n",
        "\n",
        "# Remove \"_aligned\" from each string in the array\n",
        "for i in range(data_facial.shape[0]):\n",
        "    data_facial[i, 0] = data_facial[i, 0].replace('_aligned', '')\n",
        "\n",
        "if args.dataset=='RAF':\n",
        "  train_label, test_label = select_data(data_label, data_facial)\n",
        "else:\n",
        "  train_label, test_label = random_choose_data(data_label)\n",
        "\n",
        "\n",
        "# RAF-DB\n",
        "normalize = transforms.Normalize(mean=[0.5758095, 0.4500876, 0.40176094],\n",
        "                                 std=[0.20888616, 0.19142343, 0.18289249])\n",
        "\n",
        "mytransform = transforms.Compose([transforms.Resize((224, 224)),\n",
        "                                  transforms.RandomHorizontalFlip(),\n",
        "                                  transforms.ToTensor(),\n",
        "                                  normalize])\n",
        "\n",
        "mytransform1 = transforms.Compose([transforms.Resize((224, 224)),\n",
        "                                   transforms.ToTensor(),\n",
        "                                   normalize])\n",
        "\n",
        "\n",
        "\n",
        "train_data=myImageFloder(root=data_root, label=train_label, transform=mytransform)\n",
        "test_data=myImageFloder(root=data_root, label=test_label, transform=mytransform1)\n",
        "val_data=myImageFloder(root=data_root, label=test_label, transform=mytransform1)\n",
        "\n",
        "# load\n",
        "train_loader = torch.utils.data.DataLoader(train_data,batch_size= batch_size, shuffle=True, num_workers= args.workers, pin_memory=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data,batch_size= batch_size, shuffle=True, num_workers=  args.workers, pin_memory=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_data,batch_size= batch_size, shuffle=True, num_workers= args.workers, pin_memory=True)\n"
      ],
      "metadata": {
        "id": "1zcukcdwfy7t",
        "outputId": "cc6e1d74-2228-4ad9-c7ee-9f65ce40564c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the total image is 11354\n",
            "['anger', 'disgust', 'fear', 'happy', 'neural', 'sad', 'surprise']\n",
            "the total image is 2827\n",
            "['anger', 'disgust', 'fear', 'happy', 'neural', 'sad', 'surprise']\n",
            "the total image is 2827\n",
            "['anger', 'disgust', 'fear', 'happy', 'neural', 'sad', 'surprise']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "✔**To ensure consistency:**\n",
        "\n",
        "```\n",
        " data_name = [item[0] for item in data_facial]\n",
        "train_filenames = [item[0] for item in train_data.imgs]\n",
        "\n",
        "missing_files = [f for f in train_filenames if f not in data_name]\n",
        "\n",
        "if missing_files:\n",
        "    print(\"Files in train_data but not in data_facial:\", missing_files)\n",
        "else:\n",
        "    print(\"All files in train_data are present in data_facial.\")\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "fol-RgiZuB6r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**test pose class**"
      ],
      "metadata": {
        "id": "f3-aSQobsTRK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self, name, fmt=':f'):\n",
        "        self.name = name\n",
        "        self.fmt = fmt\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "    def __str__(self):\n",
        "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
        "        return fmtstr.format(**self.__dict__)\n"
      ],
      "metadata": {
        "id": "E9WTWUsEfLDd"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ProgressMeter(object):\n",
        "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
        "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
        "        self.meters = meters\n",
        "        self.prefix = prefix\n",
        "\n",
        "    def display(self, batch, args):\n",
        "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
        "        entries += [str(meter) for meter in self.meters]\n",
        "        print_txt = '\\t'.join(entries)\n",
        "        print(print_txt)\n",
        "        txt_name = args.log_path + '-log.txt'\n",
        "        with open(txt_name, 'a') as f:\n",
        "            f.write(print_txt + '\\n')\n",
        "\n",
        "    def _get_batch_fmtstr(self, num_batches):\n",
        "        num_digits = len(str(num_batches // 1))\n",
        "        fmt = '{:' + str(num_digits) + 'd}'\n",
        "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'\n",
        "\n",
        "\n",
        "def adjust_learning_rate(optimizer, epoch, args):\n",
        "    lr = args.lr * (args.factor ** (epoch // args.af))\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "    return lr\n",
        "\n",
        "\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
        "    with torch.no_grad():\n",
        "        maxk = max(topk)\n",
        "        batch_size = target.size(0)\n",
        "        _, pred = output.topk(maxk, 1, True, True)\n",
        "        pred = pred.t()\n",
        "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "        res = []\n",
        "        for k in topk:\n",
        "            # .contiguous\n",
        "            correct_k = correct[:k].contiguous().view(-1).float().sum(0, keepdim=True)\n",
        "            res.append(correct_k.mul_(100.0 / batch_size))\n",
        "        return res"
      ],
      "metadata": {
        "id": "x8Ne6M9gfMsl"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RecorderMeter(object):\n",
        "    \"\"\"Computes and stores the minimum loss value and its epoch index\"\"\"\n",
        "\n",
        "    def __init__(self, total_epoch):\n",
        "        self.reset(total_epoch)\n",
        "\n",
        "    def reset(self, total_epoch):\n",
        "        self.total_epoch = total_epoch\n",
        "        self.current_epoch = 0\n",
        "        self.epoch_losses = np.zeros((self.total_epoch, 2), dtype=np.float32)    # [epoch, train/val]\n",
        "        self.epoch_accuracy = np.zeros((self.total_epoch, 2), dtype=np.float32)  # [epoch, train/val]\n",
        "\n",
        "    def update(self, idx, train_loss, train_acc, val_loss, val_acc):\n",
        "        self.epoch_losses[idx, 0] = train_loss * 30\n",
        "        self.epoch_losses[idx, 1] = val_loss * 30\n",
        "        self.epoch_accuracy[idx, 0] = train_acc\n",
        "        self.epoch_accuracy[idx, 1] = val_acc\n",
        "        self.current_epoch = idx + 1\n",
        "\n",
        "    def plot_curve(self, save_path):\n",
        "\n",
        "        title = 'the accuracy/loss curve of train/val'\n",
        "        dpi = 80\n",
        "        width, height = 1800, 800\n",
        "        legend_fontsize = 10\n",
        "        figsize = width / float(dpi), height / float(dpi)\n",
        "\n",
        "        fig = plt.figure(figsize=figsize)\n",
        "        x_axis = np.array([i for i in range(self.total_epoch)])  # epochs\n",
        "        y_axis = np.zeros(self.total_epoch)\n",
        "\n",
        "        plt.xlim(0, self.total_epoch)\n",
        "        plt.ylim(0, 100)\n",
        "        interval_y = 5\n",
        "        interval_x = 5\n",
        "        plt.xticks(np.arange(0, self.total_epoch + interval_x, interval_x))\n",
        "        plt.yticks(np.arange(0, 100 + interval_y, interval_y))\n",
        "        plt.grid()\n",
        "        plt.title(title, fontsize=20)\n",
        "        plt.xlabel('the training epoch', fontsize=16)\n",
        "        plt.ylabel('accuracy', fontsize=16)\n",
        "\n",
        "        y_axis[:] = self.epoch_accuracy[:, 0]\n",
        "        plt.plot(x_axis, y_axis, color='g', linestyle='-', label='train-accuracy', lw=2)\n",
        "        plt.legend(loc=4, fontsize=legend_fontsize)\n",
        "\n",
        "        y_axis[:] = self.epoch_accuracy[:, 1]\n",
        "        plt.plot(x_axis, y_axis, color='y', linestyle='-', label='valid-accuracy', lw=2)\n",
        "        plt.legend(loc=4, fontsize=legend_fontsize)\n",
        "\n",
        "        y_axis[:] = self.epoch_losses[:, 0]\n",
        "        plt.plot(x_axis, y_axis, color='g', linestyle=':', label='train-loss-x30', lw=2)\n",
        "        plt.legend(loc=4, fontsize=legend_fontsize)\n",
        "\n",
        "        y_axis[:] = self.epoch_losses[:, 1]\n",
        "        plt.plot(x_axis, y_axis, color='y', linestyle=':', label='valid-loss-x30', lw=2)\n",
        "        plt.legend(loc=4, fontsize=legend_fontsize)\n",
        "\n",
        "        if save_path is not None:\n",
        "            fig.savefig(save_path, dpi=dpi, bbox_inches='tight')\n",
        "            print('Saved figure')\n",
        "        plt.close(fig)"
      ],
      "metadata": {
        "id": "ixUlTGemfR4v"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "\n",
        "    args= init()\n",
        "    #os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    best_acc = 0\n",
        "\n",
        "    print('Training time: ' + now.strftime(\"%m-%d %H:%M\"))\n",
        "\n",
        "    # create model\n",
        "    model_cla = resnet.resnet50()\n",
        "    model_cla.fc = nn.Linear(2048, 12666)\n",
        "    model_cla = torch.nn.DataParallel(model_cla).cuda()\n",
        "    model_cla.to(device)\n",
        "    # pretrianed on msceleb\n",
        "    checkpoint = torch.load('/content/drive/MyDrive/Test_FER/Pre_trained/resnet50_pretrained_on_msceleb.pth.tar')\n",
        "    #checkpoint = torch.load('/content/drive/MyDrive/Test_FER/Pre_trained/resnet50_pretrained_on_msceleb.pth.tar' , map_location=torch.device('cpu'))\n",
        "\n",
        "\n",
        "\n",
        "    pre_trained_dict = checkpoint['state_dict']\n",
        "    model_dict = model_cla.state_dict()\n",
        "    for k, v in pre_trained_dict.items():\n",
        "        if k in model_dict:\n",
        "            print(k, v.shape)\n",
        "    pretrained_dict = {k: v for k, v in pre_trained_dict.items() if k in model_dict}\n",
        "    model_dict.update(pretrained_dict)\n",
        "    model_cla.load_state_dict(model_dict)\n",
        "    model_cla.module.fc = nn.Linear(64*3, 7).cuda()\n",
        "    #model_cla.module.fc = nn.Linear(256*3, 7)\n",
        "\n",
        "    # define loss function (criterion) and optimizer\n",
        "\n",
        "    criterion_val = nn.CrossEntropyLoss().cuda()\n",
        "    criterion_train =  nn.CrossEntropyLoss().cuda()\n",
        "\n",
        "\n",
        "\n",
        "    optimizer = torch.optim.SGD(model_cla.parameters(),\n",
        "                                args.lr,\n",
        "                                momentum=args.momentum,\n",
        "                                weight_decay=args.weight_decay\n",
        "                                )\n",
        "\n",
        "    recorder = RecorderMeter(args.epochs)\n",
        "\n",
        "    # optionally resume from a checkpoint\n",
        "    if args.resume:\n",
        "        if os.path.isfile(args.resume):\n",
        "            print(\"=> loading checkpoint '{}'\".format(args.resume))\n",
        "            checkpoint = torch.load(args.resume)\n",
        "            args.start_epoch = checkpoint['epoch']\n",
        "            best_acc = checkpoint['best_acc']\n",
        "            recorder = checkpoint['recorder']\n",
        "            best_acc = best_acc.to()\n",
        "            model_cla.load_state_dict(checkpoint['state_dict'])\n",
        "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "            print(\"=> loaded checkpoint '{}' (epoch {})\".format(args.resume, checkpoint['epoch']))\n",
        "        else:\n",
        "            print(\"=> no checkpoint found at '{}'\".format(args.resume))\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "\n",
        "    if args.evaluate:\n",
        "        checkpoint = torch.load(args.evaluate_path)\n",
        "        model_cla.load_state_dict(checkpoint['state_dict'])\n",
        "        validate(val_loader, model_cla, criterion_val, args)\n",
        "        return\n",
        "\n",
        "    for epoch in range(args.start_epoch, args.epochs):\n",
        "        start_time = time.time()\n",
        "        # update learning rate\n",
        "        current_learning_rate = adjust_learning_rate(optimizer, epoch, args)\n",
        "        print('Current learning rate: ', current_learning_rate)\n",
        "        txt_name = args.log_path + '-log.txt'\n",
        "        with open(txt_name, 'a') as f:\n",
        "            f.write('Current learning rate: ' + str(current_learning_rate) + '\\n')\n",
        "\n",
        "        # train for one epoch\n",
        "        train_acc, train_los = train(train_loader, model_cla, criterion_train, optimizer, epoch, args, data_facial)\n",
        "\n",
        "        # evaluate on validation set\n",
        "        val_acc, val_los = validate(val_loader, model_cla, criterion_val, args, data_facial)\n",
        "\n",
        "        recorder.update(epoch, train_los, train_acc, val_los, val_acc)\n",
        "        curve_name = args.log_path + '-log.png'\n",
        "        recorder.plot_curve(curve_name)\n",
        "\n",
        "        # remember best acc and save checkpoint\n",
        "        is_best = val_acc > best_acc\n",
        "        best_acc = max(val_acc, best_acc)\n",
        "\n",
        "        print('Current best accuracy: ', best_acc.item())\n",
        "        txt_name = args.log_path + '-log.txt'\n",
        "        with open(txt_name, 'a') as f:\n",
        "            f.write('Current best accuracy: ' + str(best_acc.item()) + '\\n')\n",
        "\n",
        "        save_checkpoint({'state_dict': model_cla.state_dict()}, is_best, args)\n",
        "        end_time = time.time()\n",
        "        epoch_time = end_time - start_time\n",
        "        print(\"An Epoch Time: \", epoch_time)\n",
        "        txt_name = args.log_path + '-log.txt'\n",
        "        with open(txt_name, 'a') as f:\n",
        "            f.write(str(epoch_time) + '\\n')\n",
        "        # scheduler.step(val_acc)\n",
        "\n",
        "def train(train_loader, model_cla, criterion, optimizer, epoch, args, data_facial):\n",
        "    losses = AverageMeter('Loss', ':.4f')\n",
        "    top1 = AverageMeter('Accuracy', ':6.3f')\n",
        "    top1_1 = AverageMeter('Accuracy', ':6.3f')\n",
        "    top1_2 = AverageMeter('Accuracy', ':6.3f')\n",
        "    progress = ProgressMeter(len(train_loader),\n",
        "                             [losses, top1],\n",
        "                             prefix=\"Epoch: [{}]\".format(epoch))\n",
        "    soft_max = nn.Softmax(dim=1)\n",
        "\n",
        "    # switch mode\n",
        "    model_cla.train()\n",
        "    l=args.range\n",
        "    data_name = [item[0] for item in data_facial]\n",
        "\n",
        "    for i, (images, target, fn) in enumerate(train_loader):\n",
        "\n",
        "      # search\n",
        "      facial_indx = []\n",
        "      for j in range(len(fn)):\n",
        "        facial_indx.append(data_name.index(fn[j]))\n",
        "      facial=data_facial[facial_indx,1]\n",
        "      facial = np.stack(facial, axis=0)\n",
        "      images,rect,rect_local= pre_pro(images,facial,0.8,0.5,l,args.workers)\n",
        "\n",
        "      # length = len(images)\n",
        "      images = images.cuda()\n",
        "      target = target.cuda()\n",
        "\n",
        "      model_cla.module.set_rect(rect)\n",
        "      model_cla.module.set_rect_local(rect_local)\n",
        "\n",
        "\n",
        "\n",
        "      # compute output\n",
        "\n",
        "      x_gl_fc1, x_gl_fc2, x_gl_fc3, x_al_fc1, x_al_fc2, x_al_fc3, out_gl, out_al = model_cla(images)\n",
        "\n",
        "      # .... Global\n",
        "      x_gl_fc1 = soft_max(x_gl_fc1)\n",
        "      x_gl_fc2 = soft_max(x_gl_fc2)\n",
        "      x_gl_fc3 = soft_max(x_gl_fc3)\n",
        "\n",
        "      # .... Local\n",
        "      x_al_fc1 = soft_max(x_al_fc1)\n",
        "      x_al_fc2 = soft_max(x_al_fc2)\n",
        "      x_al_fc3 = soft_max(x_al_fc3)\n",
        "\n",
        "      # compute loss\n",
        "      output =  (args.beta1 * out_gl) + ((1-args.beta1) * out_al)\n",
        "\n",
        "      loss_gl_softmax = criterion(out_gl, target)\n",
        "      loss_al_softmax = criterion(out_al, target)\n",
        "\n",
        "      loss_gl_orthognal = orthognal_loss(x_gl_fc1, x_gl_fc2, x_gl_fc3)\n",
        "      loss_al_orthognal = orthognal_loss(x_al_fc1, x_al_fc2, x_al_fc3)\n",
        "\n",
        "      loss = (args.beta1 * loss_gl_softmax) + (0.2 * loss_gl_orthognal) + ((1-args.beta1) * loss_al_softmax) + (0.2 * loss_al_orthognal)\n",
        "\n",
        "      # measure accuracy and record loss\n",
        "      acc1, _ = accuracy(output, target, topk=(1, 5))\n",
        "      acc2, _ = accuracy(x_gl_out, target, topk=(1, 5))\n",
        "      acc3, _ = accuracy(out_al, target, topk=(1, 5))\n",
        "\n",
        "      losses.update(loss.item(), images.size(0))\n",
        "      top1.update(acc1[0], images.size(0))\n",
        "      top1_1.update(acc2[0], images.size(0))\n",
        "      top1_2.update(acc3[0], images.size(0))\n",
        "\n",
        "      # compute gradient and do SGD step\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # print loss and accuracy\n",
        "      if i % args.print_freq == 0:\n",
        "        progress.display(i, args)\n",
        "\n",
        "    return top1.avg, losses.avg\n",
        "\n",
        "\n",
        "def validate(val_loader, model, criterion, args, data_facial):\n",
        "    losses = AverageMeter('Loss', ':.4f')\n",
        "    top1 = AverageMeter('Accuracy', ':6.3f')\n",
        "    top1_1 = AverageMeter('Accuracy', ':6.3f')\n",
        "    top1_2 = AverageMeter('Accuracy', ':6.3f')\n",
        "    progress = ProgressMeter(len(val_loader),\n",
        "                             [losses, top1],\n",
        "                             prefix='Test: ')\n",
        "\n",
        "    # switch to evaluate mode\n",
        "    model.eval()\n",
        "    soft_max = nn.Softmax(dim=1)\n",
        "    l=args.range\n",
        "\n",
        "    with torch.no_grad():\n",
        "      data_name = [item[0] for item in data_facial]\n",
        "\n",
        "      for i, (images, target, fn) in enumerate(val_loader):\n",
        "\n",
        "        # search\n",
        "        facial_indx = []\n",
        "        for j in range(len(fn)):\n",
        "          facial_indx.append(data_name.index(fn[j]))\n",
        "        facial=data_facial[facial_indx,1]\n",
        "        facial = np.stack(facial, axis=0)\n",
        "        images,rect,rect_local= pre_pro(images,facial,0.8,0.5,l,args.workers)\n",
        "\n",
        "        images = images.cuda()\n",
        "        target = target.cuda()\n",
        "\n",
        "        model.module.set_rect(rect)\n",
        "        model.module.set_rect_local(rect_local)\n",
        "\n",
        "\n",
        "        # compute output\n",
        "        x_gl_fc1, x_gl_fc2, x_gl_fc3, x_al_fc1, x_al_fc2, x_al_fc3, out_gl, out_al = model(images)\n",
        "\n",
        "        # .... Global\n",
        "        x_gl_fc1 = soft_max(x_gl_fc1)\n",
        "        x_gl_fc2 = soft_max(x_gl_fc2)\n",
        "        x_gl_fc3 = soft_max(x_gl_fc3)\n",
        "\n",
        "        # .... Local\n",
        "        x_al_fc1 = soft_max(x_al_fc1)\n",
        "        x_al_fc2 = soft_max(x_al_fc2)\n",
        "        x_al_fc3 = soft_max(x_al_fc3)\n",
        "\n",
        "        # compute loss\n",
        "        output =  (args.beta1 * out_gl) + ((1-args.beta1) * out_al)\n",
        "\n",
        "        loss_gl_softmax = criterion(out_gl, target)\n",
        "        loss_al_softmax = criterion(out_al, target)\n",
        "\n",
        "        loss_gl_orthognal = orthognal_loss(x_gl_fc1, x_gl_fc2, x_gl_fc3)\n",
        "        loss_al_orthognal = orthognal_loss(x_al_fc1, x_al_fc2, x_al_fc3)\n",
        "\n",
        "        loss = (args.beta1 * loss_gl_softmax) + (0.2 * loss_gl_orthognal) + ((1-args.beta1) * loss_al_softmax) + (0.2 * loss_al_orthognal)\n",
        "\n",
        "        # measure accuracy and record loss\n",
        "        acc1, _ = accuracy(output, target, topk=(1, 5))\n",
        "        acc2, _ = accuracy(x_gl_out, target, topk=(1, 5))\n",
        "        acc3, _ = accuracy(out_al, target, topk=(1, 5))\n",
        "        losses.update(loss.item(), images.size(0))\n",
        "        top1.update(acc1[0], images.size(0))\n",
        "        top1_1.update(acc2[0], images.size(0))\n",
        "        top1_2.update(acc3[0], images.size(0))\n",
        "\n",
        "        if i % args.print_freq == 0:\n",
        "            progress.display(i, args)\n",
        "\n",
        "\n",
        "      print(' *** Accuracy {top1.avg:.3f}  *** '.format(top1=top1))\n",
        "      with open(args.log_path + '-log.txt', 'a') as f:\n",
        "          f.write(' * Accuracy {top1.avg:.3f}'.format(top1=top1) + '\\n')\n",
        "      with open(os.path.join(args.log_path + '-log_err_out.txt'), 'a') as f:\n",
        "            f.write(' * vail Accuracy,output1: {top1_1.avg:.3f}'.format(top1_1=top1_1) + ' * vail  Accuracy,output2: {top1_2.avg:.3f}'.format(top1_2=top1_2)  +'\\n')\n",
        "\n",
        "\n",
        "    return top1.avg, losses.avg\n",
        "\n",
        "\n",
        "def save_checkpoint(state, is_best, args):\n",
        "    torch.save(state, args.checkpoint_path)\n",
        "    if is_best:\n",
        "        shutil.copyfile(args.checkpoint_path, args.best_checkpoint_path)\n",
        "\n",
        "\n",
        "def orthognal_loss(x, y, z):\n",
        "    x = F.normalize(x, p=2, dim=1)\n",
        "    y = F.normalize(y, p=2, dim=1)\n",
        "    z = F.normalize(z, p=2, dim=1)\n",
        "    l_12 = torch.sum(x*y, dim=1)\n",
        "    l_13 = torch.sum(x*z, dim=1)\n",
        "    l_23 = torch.sum(y*z, dim=1)\n",
        "    return torch.mean((l_12+l_13+l_23)/3, dim=-1)"
      ],
      "metadata": {
        "id": "WUVTXjutLpn8"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "  main()"
      ],
      "metadata": {
        "id": "saA2iP5bMFhW",
        "outputId": "bceae76f-2362-4826-de16-1c38304b0df9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training time: 10-08 18:47\n",
            "module.conv1.weight torch.Size([64, 3, 7, 7])\n",
            "module.bn1.weight torch.Size([64])\n",
            "module.bn1.bias torch.Size([64])\n",
            "module.bn1.running_mean torch.Size([64])\n",
            "module.bn1.running_var torch.Size([64])\n",
            "module.bn1.num_batches_tracked torch.Size([])\n",
            "module.layer1.0.conv1.weight torch.Size([64, 64, 1, 1])\n",
            "module.layer1.0.bn1.weight torch.Size([64])\n",
            "module.layer1.0.bn1.bias torch.Size([64])\n",
            "module.layer1.0.bn1.running_mean torch.Size([64])\n",
            "module.layer1.0.bn1.running_var torch.Size([64])\n",
            "module.layer1.0.bn1.num_batches_tracked torch.Size([])\n",
            "module.layer1.0.conv2.weight torch.Size([64, 64, 3, 3])\n",
            "module.layer1.0.bn2.weight torch.Size([64])\n",
            "module.layer1.0.bn2.bias torch.Size([64])\n",
            "module.layer1.0.bn2.running_mean torch.Size([64])\n",
            "module.layer1.0.bn2.running_var torch.Size([64])\n",
            "module.layer1.0.bn2.num_batches_tracked torch.Size([])\n",
            "module.layer1.0.conv3.weight torch.Size([256, 64, 1, 1])\n",
            "module.layer1.0.bn3.weight torch.Size([256])\n",
            "module.layer1.0.bn3.bias torch.Size([256])\n",
            "module.layer1.0.bn3.running_mean torch.Size([256])\n",
            "module.layer1.0.bn3.running_var torch.Size([256])\n",
            "module.layer1.0.bn3.num_batches_tracked torch.Size([])\n",
            "module.layer1.0.downsample.0.weight torch.Size([256, 64, 1, 1])\n",
            "module.layer1.0.downsample.1.weight torch.Size([256])\n",
            "module.layer1.0.downsample.1.bias torch.Size([256])\n",
            "module.layer1.0.downsample.1.running_mean torch.Size([256])\n",
            "module.layer1.0.downsample.1.running_var torch.Size([256])\n",
            "module.layer1.0.downsample.1.num_batches_tracked torch.Size([])\n",
            "module.layer1.1.conv1.weight torch.Size([64, 256, 1, 1])\n",
            "module.layer1.1.bn1.weight torch.Size([64])\n",
            "module.layer1.1.bn1.bias torch.Size([64])\n",
            "module.layer1.1.bn1.running_mean torch.Size([64])\n",
            "module.layer1.1.bn1.running_var torch.Size([64])\n",
            "module.layer1.1.bn1.num_batches_tracked torch.Size([])\n",
            "module.layer1.1.conv2.weight torch.Size([64, 64, 3, 3])\n",
            "module.layer1.1.bn2.weight torch.Size([64])\n",
            "module.layer1.1.bn2.bias torch.Size([64])\n",
            "module.layer1.1.bn2.running_mean torch.Size([64])\n",
            "module.layer1.1.bn2.running_var torch.Size([64])\n",
            "module.layer1.1.bn2.num_batches_tracked torch.Size([])\n",
            "module.layer1.1.conv3.weight torch.Size([256, 64, 1, 1])\n",
            "module.layer1.1.bn3.weight torch.Size([256])\n",
            "module.layer1.1.bn3.bias torch.Size([256])\n",
            "module.layer1.1.bn3.running_mean torch.Size([256])\n",
            "module.layer1.1.bn3.running_var torch.Size([256])\n",
            "module.layer1.1.bn3.num_batches_tracked torch.Size([])\n",
            "module.layer1.2.conv1.weight torch.Size([64, 256, 1, 1])\n",
            "module.layer1.2.bn1.weight torch.Size([64])\n",
            "module.layer1.2.bn1.bias torch.Size([64])\n",
            "module.layer1.2.bn1.running_mean torch.Size([64])\n",
            "module.layer1.2.bn1.running_var torch.Size([64])\n",
            "module.layer1.2.bn1.num_batches_tracked torch.Size([])\n",
            "module.layer1.2.conv2.weight torch.Size([64, 64, 3, 3])\n",
            "module.layer1.2.bn2.weight torch.Size([64])\n",
            "module.layer1.2.bn2.bias torch.Size([64])\n",
            "module.layer1.2.bn2.running_mean torch.Size([64])\n",
            "module.layer1.2.bn2.running_var torch.Size([64])\n",
            "module.layer1.2.bn2.num_batches_tracked torch.Size([])\n",
            "module.layer1.2.conv3.weight torch.Size([256, 64, 1, 1])\n",
            "module.layer1.2.bn3.weight torch.Size([256])\n",
            "module.layer1.2.bn3.bias torch.Size([256])\n",
            "module.layer1.2.bn3.running_mean torch.Size([256])\n",
            "module.layer1.2.bn3.running_var torch.Size([256])\n",
            "module.layer1.2.bn3.num_batches_tracked torch.Size([])\n",
            "module.layer2.0.conv1.weight torch.Size([128, 256, 1, 1])\n",
            "module.layer2.0.bn1.weight torch.Size([128])\n",
            "module.layer2.0.bn1.bias torch.Size([128])\n",
            "module.layer2.0.bn1.running_mean torch.Size([128])\n",
            "module.layer2.0.bn1.running_var torch.Size([128])\n",
            "module.layer2.0.bn1.num_batches_tracked torch.Size([])\n",
            "module.layer2.0.conv2.weight torch.Size([128, 128, 3, 3])\n",
            "module.layer2.0.bn2.weight torch.Size([128])\n",
            "module.layer2.0.bn2.bias torch.Size([128])\n",
            "module.layer2.0.bn2.running_mean torch.Size([128])\n",
            "module.layer2.0.bn2.running_var torch.Size([128])\n",
            "module.layer2.0.bn2.num_batches_tracked torch.Size([])\n",
            "module.layer2.0.conv3.weight torch.Size([512, 128, 1, 1])\n",
            "module.layer2.0.bn3.weight torch.Size([512])\n",
            "module.layer2.0.bn3.bias torch.Size([512])\n",
            "module.layer2.0.bn3.running_mean torch.Size([512])\n",
            "module.layer2.0.bn3.running_var torch.Size([512])\n",
            "module.layer2.0.bn3.num_batches_tracked torch.Size([])\n",
            "module.layer2.0.downsample.0.weight torch.Size([512, 256, 1, 1])\n",
            "module.layer2.0.downsample.1.weight torch.Size([512])\n",
            "module.layer2.0.downsample.1.bias torch.Size([512])\n",
            "module.layer2.0.downsample.1.running_mean torch.Size([512])\n",
            "module.layer2.0.downsample.1.running_var torch.Size([512])\n",
            "module.layer2.0.downsample.1.num_batches_tracked torch.Size([])\n",
            "module.layer2.1.conv1.weight torch.Size([128, 512, 1, 1])\n",
            "module.layer2.1.bn1.weight torch.Size([128])\n",
            "module.layer2.1.bn1.bias torch.Size([128])\n",
            "module.layer2.1.bn1.running_mean torch.Size([128])\n",
            "module.layer2.1.bn1.running_var torch.Size([128])\n",
            "module.layer2.1.bn1.num_batches_tracked torch.Size([])\n",
            "module.layer2.1.conv2.weight torch.Size([128, 128, 3, 3])\n",
            "module.layer2.1.bn2.weight torch.Size([128])\n",
            "module.layer2.1.bn2.bias torch.Size([128])\n",
            "module.layer2.1.bn2.running_mean torch.Size([128])\n",
            "module.layer2.1.bn2.running_var torch.Size([128])\n",
            "module.layer2.1.bn2.num_batches_tracked torch.Size([])\n",
            "module.layer2.1.conv3.weight torch.Size([512, 128, 1, 1])\n",
            "module.layer2.1.bn3.weight torch.Size([512])\n",
            "module.layer2.1.bn3.bias torch.Size([512])\n",
            "module.layer2.1.bn3.running_mean torch.Size([512])\n",
            "module.layer2.1.bn3.running_var torch.Size([512])\n",
            "module.layer2.1.bn3.num_batches_tracked torch.Size([])\n",
            "module.layer2.2.conv1.weight torch.Size([128, 512, 1, 1])\n",
            "module.layer2.2.bn1.weight torch.Size([128])\n",
            "module.layer2.2.bn1.bias torch.Size([128])\n",
            "module.layer2.2.bn1.running_mean torch.Size([128])\n",
            "module.layer2.2.bn1.running_var torch.Size([128])\n",
            "module.layer2.2.bn1.num_batches_tracked torch.Size([])\n",
            "module.layer2.2.conv2.weight torch.Size([128, 128, 3, 3])\n",
            "module.layer2.2.bn2.weight torch.Size([128])\n",
            "module.layer2.2.bn2.bias torch.Size([128])\n",
            "module.layer2.2.bn2.running_mean torch.Size([128])\n",
            "module.layer2.2.bn2.running_var torch.Size([128])\n",
            "module.layer2.2.bn2.num_batches_tracked torch.Size([])\n",
            "module.layer2.2.conv3.weight torch.Size([512, 128, 1, 1])\n",
            "module.layer2.2.bn3.weight torch.Size([512])\n",
            "module.layer2.2.bn3.bias torch.Size([512])\n",
            "module.layer2.2.bn3.running_mean torch.Size([512])\n",
            "module.layer2.2.bn3.running_var torch.Size([512])\n",
            "module.layer2.2.bn3.num_batches_tracked torch.Size([])\n",
            "module.layer2.3.conv1.weight torch.Size([128, 512, 1, 1])\n",
            "module.layer2.3.bn1.weight torch.Size([128])\n",
            "module.layer2.3.bn1.bias torch.Size([128])\n",
            "module.layer2.3.bn1.running_mean torch.Size([128])\n",
            "module.layer2.3.bn1.running_var torch.Size([128])\n",
            "module.layer2.3.bn1.num_batches_tracked torch.Size([])\n",
            "module.layer2.3.conv2.weight torch.Size([128, 128, 3, 3])\n",
            "module.layer2.3.bn2.weight torch.Size([128])\n",
            "module.layer2.3.bn2.bias torch.Size([128])\n",
            "module.layer2.3.bn2.running_mean torch.Size([128])\n",
            "module.layer2.3.bn2.running_var torch.Size([128])\n",
            "module.layer2.3.bn2.num_batches_tracked torch.Size([])\n",
            "module.layer2.3.conv3.weight torch.Size([512, 128, 1, 1])\n",
            "module.layer2.3.bn3.weight torch.Size([512])\n",
            "module.layer2.3.bn3.bias torch.Size([512])\n",
            "module.layer2.3.bn3.running_mean torch.Size([512])\n",
            "module.layer2.3.bn3.running_var torch.Size([512])\n",
            "module.layer2.3.bn3.num_batches_tracked torch.Size([])\n",
            "module.layer3.0.conv1.weight torch.Size([256, 512, 1, 1])\n",
            "module.layer3.0.bn1.weight torch.Size([256])\n",
            "module.layer3.0.bn1.bias torch.Size([256])\n",
            "module.layer3.0.bn1.running_mean torch.Size([256])\n",
            "module.layer3.0.bn1.running_var torch.Size([256])\n",
            "module.layer3.0.bn1.num_batches_tracked torch.Size([])\n",
            "module.layer3.0.conv2.weight torch.Size([256, 256, 3, 3])\n",
            "module.layer3.0.bn2.weight torch.Size([256])\n",
            "module.layer3.0.bn2.bias torch.Size([256])\n",
            "module.layer3.0.bn2.running_mean torch.Size([256])\n",
            "module.layer3.0.bn2.running_var torch.Size([256])\n",
            "module.layer3.0.bn2.num_batches_tracked torch.Size([])\n",
            "module.layer3.0.conv3.weight torch.Size([1024, 256, 1, 1])\n",
            "module.layer3.0.bn3.weight torch.Size([1024])\n",
            "module.layer3.0.bn3.bias torch.Size([1024])\n",
            "module.layer3.0.bn3.running_mean torch.Size([1024])\n",
            "module.layer3.0.bn3.running_var torch.Size([1024])\n",
            "module.layer3.0.bn3.num_batches_tracked torch.Size([])\n",
            "module.layer3.0.downsample.0.weight torch.Size([1024, 512, 1, 1])\n",
            "module.layer3.0.downsample.1.weight torch.Size([1024])\n",
            "module.layer3.0.downsample.1.bias torch.Size([1024])\n",
            "module.layer3.0.downsample.1.running_mean torch.Size([1024])\n",
            "module.layer3.0.downsample.1.running_var torch.Size([1024])\n",
            "module.layer3.0.downsample.1.num_batches_tracked torch.Size([])\n",
            "module.layer3.1.conv1.weight torch.Size([256, 1024, 1, 1])\n",
            "module.layer3.1.bn1.weight torch.Size([256])\n",
            "module.layer3.1.bn1.bias torch.Size([256])\n",
            "module.layer3.1.bn1.running_mean torch.Size([256])\n",
            "module.layer3.1.bn1.running_var torch.Size([256])\n",
            "module.layer3.1.bn1.num_batches_tracked torch.Size([])\n",
            "module.layer3.1.conv2.weight torch.Size([256, 256, 3, 3])\n",
            "module.layer3.1.bn2.weight torch.Size([256])\n",
            "module.layer3.1.bn2.bias torch.Size([256])\n",
            "module.layer3.1.bn2.running_mean torch.Size([256])\n",
            "module.layer3.1.bn2.running_var torch.Size([256])\n",
            "module.layer3.1.bn2.num_batches_tracked torch.Size([])\n",
            "module.layer3.1.conv3.weight torch.Size([1024, 256, 1, 1])\n",
            "module.layer3.1.bn3.weight torch.Size([1024])\n",
            "module.layer3.1.bn3.bias torch.Size([1024])\n",
            "module.layer3.1.bn3.running_mean torch.Size([1024])\n",
            "module.layer3.1.bn3.running_var torch.Size([1024])\n",
            "module.layer3.1.bn3.num_batches_tracked torch.Size([])\n",
            "module.layer3.2.conv1.weight torch.Size([256, 1024, 1, 1])\n",
            "module.layer3.2.bn1.weight torch.Size([256])\n",
            "module.layer3.2.bn1.bias torch.Size([256])\n",
            "module.layer3.2.bn1.running_mean torch.Size([256])\n",
            "module.layer3.2.bn1.running_var torch.Size([256])\n",
            "module.layer3.2.bn1.num_batches_tracked torch.Size([])\n",
            "module.layer3.2.conv2.weight torch.Size([256, 256, 3, 3])\n",
            "module.layer3.2.bn2.weight torch.Size([256])\n",
            "module.layer3.2.bn2.bias torch.Size([256])\n",
            "module.layer3.2.bn2.running_mean torch.Size([256])\n",
            "module.layer3.2.bn2.running_var torch.Size([256])\n",
            "module.layer3.2.bn2.num_batches_tracked torch.Size([])\n",
            "module.layer3.2.conv3.weight torch.Size([1024, 256, 1, 1])\n",
            "module.layer3.2.bn3.weight torch.Size([1024])\n",
            "module.layer3.2.bn3.bias torch.Size([1024])\n",
            "module.layer3.2.bn3.running_mean torch.Size([1024])\n",
            "module.layer3.2.bn3.running_var torch.Size([1024])\n",
            "module.layer3.2.bn3.num_batches_tracked torch.Size([])\n",
            "module.layer3.3.conv1.weight torch.Size([256, 1024, 1, 1])\n",
            "module.layer3.3.bn1.weight torch.Size([256])\n",
            "module.layer3.3.bn1.bias torch.Size([256])\n",
            "module.layer3.3.bn1.running_mean torch.Size([256])\n",
            "module.layer3.3.bn1.running_var torch.Size([256])\n",
            "module.layer3.3.bn1.num_batches_tracked torch.Size([])\n",
            "module.layer3.3.conv2.weight torch.Size([256, 256, 3, 3])\n",
            "module.layer3.3.bn2.weight torch.Size([256])\n",
            "module.layer3.3.bn2.bias torch.Size([256])\n",
            "module.layer3.3.bn2.running_mean torch.Size([256])\n",
            "module.layer3.3.bn2.running_var torch.Size([256])\n",
            "module.layer3.3.bn2.num_batches_tracked torch.Size([])\n",
            "module.layer3.3.conv3.weight torch.Size([1024, 256, 1, 1])\n",
            "module.layer3.3.bn3.weight torch.Size([1024])\n",
            "module.layer3.3.bn3.bias torch.Size([1024])\n",
            "module.layer3.3.bn3.running_mean torch.Size([1024])\n",
            "module.layer3.3.bn3.running_var torch.Size([1024])\n",
            "module.layer3.3.bn3.num_batches_tracked torch.Size([])\n",
            "module.layer3.4.conv1.weight torch.Size([256, 1024, 1, 1])\n",
            "module.layer3.4.bn1.weight torch.Size([256])\n",
            "module.layer3.4.bn1.bias torch.Size([256])\n",
            "module.layer3.4.bn1.running_mean torch.Size([256])\n",
            "module.layer3.4.bn1.running_var torch.Size([256])\n",
            "module.layer3.4.bn1.num_batches_tracked torch.Size([])\n",
            "module.layer3.4.conv2.weight torch.Size([256, 256, 3, 3])\n",
            "module.layer3.4.bn2.weight torch.Size([256])\n",
            "module.layer3.4.bn2.bias torch.Size([256])\n",
            "module.layer3.4.bn2.running_mean torch.Size([256])\n",
            "module.layer3.4.bn2.running_var torch.Size([256])\n",
            "module.layer3.4.bn2.num_batches_tracked torch.Size([])\n",
            "module.layer3.4.conv3.weight torch.Size([1024, 256, 1, 1])\n",
            "module.layer3.4.bn3.weight torch.Size([1024])\n",
            "module.layer3.4.bn3.bias torch.Size([1024])\n",
            "module.layer3.4.bn3.running_mean torch.Size([1024])\n",
            "module.layer3.4.bn3.running_var torch.Size([1024])\n",
            "module.layer3.4.bn3.num_batches_tracked torch.Size([])\n",
            "module.layer3.5.conv1.weight torch.Size([256, 1024, 1, 1])\n",
            "module.layer3.5.bn1.weight torch.Size([256])\n",
            "module.layer3.5.bn1.bias torch.Size([256])\n",
            "module.layer3.5.bn1.running_mean torch.Size([256])\n",
            "module.layer3.5.bn1.running_var torch.Size([256])\n",
            "module.layer3.5.bn1.num_batches_tracked torch.Size([])\n",
            "module.layer3.5.conv2.weight torch.Size([256, 256, 3, 3])\n",
            "module.layer3.5.bn2.weight torch.Size([256])\n",
            "module.layer3.5.bn2.bias torch.Size([256])\n",
            "module.layer3.5.bn2.running_mean torch.Size([256])\n",
            "module.layer3.5.bn2.running_var torch.Size([256])\n",
            "module.layer3.5.bn2.num_batches_tracked torch.Size([])\n",
            "module.layer3.5.conv3.weight torch.Size([1024, 256, 1, 1])\n",
            "module.layer3.5.bn3.weight torch.Size([1024])\n",
            "module.layer3.5.bn3.bias torch.Size([1024])\n",
            "module.layer3.5.bn3.running_mean torch.Size([1024])\n",
            "module.layer3.5.bn3.running_var torch.Size([1024])\n",
            "module.layer3.5.bn3.num_batches_tracked torch.Size([])\n",
            "module.layer4.0.conv1.weight torch.Size([512, 1024, 1, 1])\n",
            "module.layer4.0.bn1.weight torch.Size([512])\n",
            "module.layer4.0.bn1.bias torch.Size([512])\n",
            "module.layer4.0.bn1.running_mean torch.Size([512])\n",
            "module.layer4.0.bn1.running_var torch.Size([512])\n",
            "module.layer4.0.bn1.num_batches_tracked torch.Size([])\n",
            "module.layer4.0.conv2.weight torch.Size([512, 512, 3, 3])\n",
            "module.layer4.0.bn2.weight torch.Size([512])\n",
            "module.layer4.0.bn2.bias torch.Size([512])\n",
            "module.layer4.0.bn2.running_mean torch.Size([512])\n",
            "module.layer4.0.bn2.running_var torch.Size([512])\n",
            "module.layer4.0.bn2.num_batches_tracked torch.Size([])\n",
            "module.layer4.0.conv3.weight torch.Size([2048, 512, 1, 1])\n",
            "module.layer4.0.bn3.weight torch.Size([2048])\n",
            "module.layer4.0.bn3.bias torch.Size([2048])\n",
            "module.layer4.0.bn3.running_mean torch.Size([2048])\n",
            "module.layer4.0.bn3.running_var torch.Size([2048])\n",
            "module.layer4.0.bn3.num_batches_tracked torch.Size([])\n",
            "module.layer4.0.downsample.0.weight torch.Size([2048, 1024, 1, 1])\n",
            "module.layer4.0.downsample.1.weight torch.Size([2048])\n",
            "module.layer4.0.downsample.1.bias torch.Size([2048])\n",
            "module.layer4.0.downsample.1.running_mean torch.Size([2048])\n",
            "module.layer4.0.downsample.1.running_var torch.Size([2048])\n",
            "module.layer4.0.downsample.1.num_batches_tracked torch.Size([])\n",
            "module.layer4.1.conv1.weight torch.Size([512, 2048, 1, 1])\n",
            "module.layer4.1.bn1.weight torch.Size([512])\n",
            "module.layer4.1.bn1.bias torch.Size([512])\n",
            "module.layer4.1.bn1.running_mean torch.Size([512])\n",
            "module.layer4.1.bn1.running_var torch.Size([512])\n",
            "module.layer4.1.bn1.num_batches_tracked torch.Size([])\n",
            "module.layer4.1.conv2.weight torch.Size([512, 512, 3, 3])\n",
            "module.layer4.1.bn2.weight torch.Size([512])\n",
            "module.layer4.1.bn2.bias torch.Size([512])\n",
            "module.layer4.1.bn2.running_mean torch.Size([512])\n",
            "module.layer4.1.bn2.running_var torch.Size([512])\n",
            "module.layer4.1.bn2.num_batches_tracked torch.Size([])\n",
            "module.layer4.1.conv3.weight torch.Size([2048, 512, 1, 1])\n",
            "module.layer4.1.bn3.weight torch.Size([2048])\n",
            "module.layer4.1.bn3.bias torch.Size([2048])\n",
            "module.layer4.1.bn3.running_mean torch.Size([2048])\n",
            "module.layer4.1.bn3.running_var torch.Size([2048])\n",
            "module.layer4.1.bn3.num_batches_tracked torch.Size([])\n",
            "module.layer4.2.conv1.weight torch.Size([512, 2048, 1, 1])\n",
            "module.layer4.2.bn1.weight torch.Size([512])\n",
            "module.layer4.2.bn1.bias torch.Size([512])\n",
            "module.layer4.2.bn1.running_mean torch.Size([512])\n",
            "module.layer4.2.bn1.running_var torch.Size([512])\n",
            "module.layer4.2.bn1.num_batches_tracked torch.Size([])\n",
            "module.layer4.2.conv2.weight torch.Size([512, 512, 3, 3])\n",
            "module.layer4.2.bn2.weight torch.Size([512])\n",
            "module.layer4.2.bn2.bias torch.Size([512])\n",
            "module.layer4.2.bn2.running_mean torch.Size([512])\n",
            "module.layer4.2.bn2.running_var torch.Size([512])\n",
            "module.layer4.2.bn2.num_batches_tracked torch.Size([])\n",
            "module.layer4.2.conv3.weight torch.Size([2048, 512, 1, 1])\n",
            "module.layer4.2.bn3.weight torch.Size([2048])\n",
            "module.layer4.2.bn3.bias torch.Size([2048])\n",
            "module.layer4.2.bn3.running_mean torch.Size([2048])\n",
            "module.layer4.2.bn3.running_var torch.Size([2048])\n",
            "module.layer4.2.bn3.num_batches_tracked torch.Size([])\n",
            "module.fc.weight torch.Size([12666, 2048])\n",
            "module.fc.bias torch.Size([12666])\n",
            "Current learning rate:  0.001\n",
            "Epoch: [0][  0/710]\tLoss 4.3665 (4.3665)\tAccuracy  6.250 ( 6.250)\n",
            "Epoch: [0][ 10/710]\tLoss 3.0460 (3.4532)\tAccuracy 18.750 (26.705)\n",
            "Epoch: [0][ 20/710]\tLoss 2.9781 (3.1808)\tAccuracy 43.750 (35.714)\n",
            "Epoch: [0][ 30/710]\tLoss 1.2864 (2.8880)\tAccuracy 68.750 (42.540)\n",
            "Epoch: [0][ 40/710]\tLoss 1.8807 (2.6893)\tAccuracy 75.000 (46.799)\n",
            "Epoch: [0][ 50/710]\tLoss 2.0152 (2.6232)\tAccuracy 62.500 (48.407)\n",
            "Epoch: [0][ 60/710]\tLoss 2.2065 (2.4989)\tAccuracy 43.750 (49.898)\n",
            "Epoch: [0][ 70/710]\tLoss 1.7949 (2.4318)\tAccuracy 50.000 (50.264)\n",
            "Epoch: [0][ 80/710]\tLoss 0.7479 (2.3795)\tAccuracy 87.500 (50.694)\n",
            "Epoch: [0][ 90/710]\tLoss 1.4128 (2.3251)\tAccuracy 75.000 (52.335)\n",
            "Epoch: [0][100/710]\tLoss 1.8857 (2.2837)\tAccuracy 56.250 (53.032)\n",
            "Epoch: [0][110/710]\tLoss 1.2872 (2.2322)\tAccuracy 75.000 (53.773)\n",
            "Epoch: [0][120/710]\tLoss 1.2593 (2.1845)\tAccuracy 68.750 (54.545)\n",
            "Epoch: [0][130/710]\tLoss 1.3183 (2.1322)\tAccuracy 81.250 (55.773)\n",
            "Epoch: [0][140/710]\tLoss 1.8249 (2.0873)\tAccuracy 50.000 (56.294)\n",
            "Epoch: [0][150/710]\tLoss 1.1739 (2.0380)\tAccuracy 62.500 (57.243)\n",
            "Epoch: [0][160/710]\tLoss 1.2591 (2.0065)\tAccuracy 75.000 (57.919)\n",
            "Epoch: [0][170/710]\tLoss 1.0227 (1.9738)\tAccuracy 75.000 (58.589)\n",
            "Epoch: [0][180/710]\tLoss 1.1629 (1.9412)\tAccuracy 87.500 (59.116)\n",
            "Epoch: [0][190/710]\tLoss 1.3030 (1.9049)\tAccuracy 62.500 (59.948)\n",
            "Epoch: [0][200/710]\tLoss 1.6004 (1.8829)\tAccuracy 87.500 (60.665)\n",
            "Epoch: [0][210/710]\tLoss 1.4299 (1.8627)\tAccuracy 75.000 (61.049)\n",
            "Epoch: [0][220/710]\tLoss 1.7076 (1.8445)\tAccuracy 68.750 (61.340)\n",
            "Epoch: [0][230/710]\tLoss 1.2039 (1.8280)\tAccuracy 87.500 (61.769)\n",
            "Epoch: [0][240/710]\tLoss 1.3997 (1.8155)\tAccuracy 75.000 (62.033)\n",
            "Epoch: [0][250/710]\tLoss 0.9837 (1.7954)\tAccuracy 81.250 (62.301)\n",
            "Epoch: [0][260/710]\tLoss 1.6866 (1.7886)\tAccuracy 50.000 (62.332)\n",
            "Epoch: [0][270/710]\tLoss 1.3944 (1.7794)\tAccuracy 62.500 (62.315)\n",
            "Epoch: [0][280/710]\tLoss 0.8519 (1.7607)\tAccuracy 87.500 (62.656)\n",
            "Epoch: [0][290/710]\tLoss 1.3898 (1.7480)\tAccuracy 68.750 (62.908)\n",
            "Epoch: [0][300/710]\tLoss 0.9036 (1.7332)\tAccuracy 81.250 (63.164)\n",
            "Epoch: [0][310/710]\tLoss 1.4548 (1.7236)\tAccuracy 56.250 (63.183)\n",
            "Epoch: [0][320/710]\tLoss 0.9455 (1.7096)\tAccuracy 81.250 (63.415)\n",
            "Epoch: [0][330/710]\tLoss 1.0122 (1.6967)\tAccuracy 81.250 (63.765)\n",
            "Epoch: [0][340/710]\tLoss 1.0669 (1.6841)\tAccuracy 81.250 (64.076)\n",
            "Epoch: [0][350/710]\tLoss 0.9127 (1.6681)\tAccuracy 81.250 (64.334)\n",
            "Epoch: [0][360/710]\tLoss 0.7356 (1.6549)\tAccuracy 93.750 (64.630)\n",
            "Epoch: [0][370/710]\tLoss 1.1256 (1.6437)\tAccuracy 75.000 (64.724)\n",
            "Epoch: [0][380/710]\tLoss 1.1119 (1.6302)\tAccuracy 81.250 (64.911)\n",
            "Epoch: [0][390/710]\tLoss 1.6860 (1.6257)\tAccuracy 56.250 (64.994)\n",
            "Epoch: [0][400/710]\tLoss 1.2268 (1.6179)\tAccuracy 75.000 (65.165)\n",
            "Epoch: [0][410/710]\tLoss 0.8225 (1.6069)\tAccuracy 87.500 (65.435)\n",
            "Epoch: [0][420/710]\tLoss 1.3294 (1.6014)\tAccuracy 75.000 (65.618)\n",
            "Epoch: [0][430/710]\tLoss 0.8306 (1.5924)\tAccuracy 81.250 (65.705)\n",
            "Epoch: [0][440/710]\tLoss 1.1626 (1.5879)\tAccuracy 75.000 (65.802)\n",
            "Epoch: [0][450/710]\tLoss 1.0595 (1.5783)\tAccuracy 87.500 (65.978)\n",
            "Epoch: [0][460/710]\tLoss 1.4469 (1.5698)\tAccuracy 75.000 (66.147)\n",
            "Epoch: [0][470/710]\tLoss 1.3541 (1.5596)\tAccuracy 68.750 (66.335)\n",
            "Epoch: [0][480/710]\tLoss 1.4926 (1.5527)\tAccuracy 75.000 (66.554)\n",
            "Epoch: [0][490/710]\tLoss 0.9251 (1.5421)\tAccuracy 87.500 (66.815)\n",
            "Epoch: [0][500/710]\tLoss 1.4008 (1.5345)\tAccuracy 75.000 (66.979)\n",
            "Epoch: [0][510/710]\tLoss 1.3056 (1.5265)\tAccuracy 56.250 (67.050)\n",
            "Epoch: [0][520/710]\tLoss 1.1421 (1.5207)\tAccuracy 75.000 (67.178)\n",
            "Epoch: [0][530/710]\tLoss 1.8592 (1.5143)\tAccuracy 62.500 (67.338)\n",
            "Epoch: [0][540/710]\tLoss 1.1989 (1.5105)\tAccuracy 81.250 (67.410)\n",
            "Epoch: [0][550/710]\tLoss 1.0222 (1.5063)\tAccuracy 75.000 (67.423)\n",
            "Epoch: [0][560/710]\tLoss 1.0279 (1.4989)\tAccuracy 75.000 (67.580)\n",
            "Epoch: [0][570/710]\tLoss 1.0167 (1.4948)\tAccuracy 68.750 (67.644)\n",
            "Epoch: [0][580/710]\tLoss 1.1981 (1.4888)\tAccuracy 62.500 (67.707)\n",
            "Epoch: [0][590/710]\tLoss 1.2057 (1.4851)\tAccuracy 68.750 (67.714)\n",
            "Epoch: [0][600/710]\tLoss 1.1628 (1.4796)\tAccuracy 68.750 (67.824)\n",
            "Epoch: [0][610/710]\tLoss 0.9827 (1.4740)\tAccuracy 75.000 (67.962)\n",
            "Epoch: [0][620/710]\tLoss 0.8339 (1.4667)\tAccuracy 81.250 (68.086)\n",
            "Epoch: [0][630/710]\tLoss 1.0062 (1.4605)\tAccuracy 68.750 (68.176)\n",
            "Epoch: [0][640/710]\tLoss 1.1471 (1.4548)\tAccuracy 62.500 (68.253)\n",
            "Epoch: [0][650/710]\tLoss 1.1746 (1.4482)\tAccuracy 75.000 (68.356)\n",
            "Epoch: [0][660/710]\tLoss 0.7583 (1.4427)\tAccuracy 87.500 (68.381)\n",
            "Epoch: [0][670/710]\tLoss 1.2952 (1.4394)\tAccuracy 56.250 (68.405)\n",
            "Epoch: [0][680/710]\tLoss 1.5407 (1.4350)\tAccuracy 56.250 (68.475)\n",
            "Epoch: [0][690/710]\tLoss 1.3280 (1.4315)\tAccuracy 75.000 (68.533)\n",
            "Epoch: [0][700/710]\tLoss 1.1429 (1.4272)\tAccuracy 81.250 (68.652)\n",
            "Test: [  0/177]\tLoss 0.8119 (0.8119)\tAccuracy 87.500 (87.500)\n",
            "Test: [ 10/177]\tLoss 1.0540 (1.0592)\tAccuracy 75.000 (73.295)\n",
            "Test: [ 20/177]\tLoss 0.9945 (1.0564)\tAccuracy 68.750 (76.488)\n",
            "Test: [ 30/177]\tLoss 0.9838 (1.0639)\tAccuracy 81.250 (76.210)\n",
            "Test: [ 40/177]\tLoss 1.5116 (1.0495)\tAccuracy 56.250 (77.134)\n",
            "Test: [ 50/177]\tLoss 0.8700 (1.0474)\tAccuracy 87.500 (76.838)\n",
            "Test: [ 60/177]\tLoss 1.2681 (1.0445)\tAccuracy 75.000 (77.049)\n",
            "Test: [ 70/177]\tLoss 1.0308 (1.0291)\tAccuracy 87.500 (77.465)\n",
            "Test: [ 80/177]\tLoss 1.0099 (1.0344)\tAccuracy 75.000 (77.778)\n",
            "Test: [ 90/177]\tLoss 1.0901 (1.0320)\tAccuracy 81.250 (77.885)\n",
            "Test: [100/177]\tLoss 0.7712 (1.0327)\tAccuracy 75.000 (77.785)\n",
            "Test: [110/177]\tLoss 1.0788 (1.0513)\tAccuracy 81.250 (77.421)\n",
            "Test: [120/177]\tLoss 0.9760 (1.0433)\tAccuracy 87.500 (77.944)\n",
            "Test: [130/177]\tLoss 1.1042 (1.0417)\tAccuracy 68.750 (78.006)\n",
            "Test: [140/177]\tLoss 0.9090 (1.0323)\tAccuracy 81.250 (78.413)\n",
            "Test: [150/177]\tLoss 1.1304 (1.0286)\tAccuracy 68.750 (78.394)\n",
            "Test: [160/177]\tLoss 1.4957 (1.0264)\tAccuracy 56.250 (78.416)\n",
            "Test: [170/177]\tLoss 1.0383 (1.0188)\tAccuracy 87.500 (78.655)\n",
            " *** Accuracy 78.599  *** \n",
            "Saved figure\n",
            "Current best accuracy:  78.59922790527344\n",
            "An Epoch Time:  232.75419282913208\n",
            "Current learning rate:  0.001\n",
            "Epoch: [1][  0/710]\tLoss 1.6095 (1.6095)\tAccuracy 56.250 (56.250)\n",
            "Epoch: [1][ 10/710]\tLoss 1.3622 (1.1005)\tAccuracy 68.750 (70.455)\n",
            "Epoch: [1][ 20/710]\tLoss 0.8137 (1.0498)\tAccuracy 87.500 (73.810)\n",
            "Epoch: [1][ 30/710]\tLoss 1.0005 (1.0669)\tAccuracy 81.250 (73.387)\n",
            "Epoch: [1][ 40/710]\tLoss 0.8922 (1.0140)\tAccuracy 81.250 (75.762)\n",
            "Epoch: [1][ 50/710]\tLoss 1.0119 (1.0453)\tAccuracy 75.000 (75.368)\n",
            "Epoch: [1][ 60/710]\tLoss 1.0333 (1.0375)\tAccuracy 75.000 (75.410)\n",
            "Epoch: [1][ 70/710]\tLoss 1.0728 (1.0226)\tAccuracy 68.750 (75.352)\n",
            "Epoch: [1][ 80/710]\tLoss 0.7071 (1.0093)\tAccuracy 93.750 (76.003)\n",
            "Epoch: [1][ 90/710]\tLoss 0.5725 (0.9990)\tAccuracy 93.750 (76.786)\n",
            "Epoch: [1][100/710]\tLoss 0.9260 (1.0089)\tAccuracy 75.000 (76.547)\n",
            "Epoch: [1][110/710]\tLoss 0.7066 (1.0023)\tAccuracy 93.750 (76.577)\n",
            "Epoch: [1][120/710]\tLoss 0.9447 (0.9944)\tAccuracy 87.500 (77.014)\n",
            "Epoch: [1][130/710]\tLoss 0.9114 (0.9926)\tAccuracy 81.250 (76.956)\n",
            "Epoch: [1][140/710]\tLoss 1.0488 (0.9897)\tAccuracy 75.000 (77.083)\n",
            "Epoch: [1][150/710]\tLoss 0.9594 (0.9771)\tAccuracy 75.000 (77.815)\n",
            "Epoch: [1][160/710]\tLoss 1.3540 (0.9705)\tAccuracy 50.000 (77.989)\n",
            "Epoch: [1][170/710]\tLoss 1.4225 (0.9673)\tAccuracy 75.000 (78.143)\n",
            "Epoch: [1][180/710]\tLoss 1.3148 (0.9663)\tAccuracy 62.500 (78.177)\n",
            "Epoch: [1][190/710]\tLoss 0.8788 (0.9601)\tAccuracy 81.250 (78.338)\n",
            "Epoch: [1][200/710]\tLoss 1.1400 (0.9673)\tAccuracy 62.500 (77.830)\n",
            "Epoch: [1][210/710]\tLoss 0.9338 (0.9699)\tAccuracy 87.500 (77.755)\n",
            "Epoch: [1][220/710]\tLoss 0.5123 (0.9703)\tAccuracy 100.000 (77.715)\n",
            "Epoch: [1][230/710]\tLoss 1.1480 (0.9711)\tAccuracy 56.250 (77.787)\n",
            "Epoch: [1][240/710]\tLoss 0.7140 (0.9688)\tAccuracy 87.500 (77.905)\n",
            "Epoch: [1][250/710]\tLoss 0.9789 (0.9643)\tAccuracy 75.000 (78.063)\n",
            "Epoch: [1][260/710]\tLoss 0.8724 (0.9618)\tAccuracy 75.000 (78.137)\n",
            "Epoch: [1][270/710]\tLoss 1.0794 (0.9593)\tAccuracy 68.750 (78.160)\n",
            "Epoch: [1][280/710]\tLoss 0.8504 (0.9570)\tAccuracy 75.000 (78.181)\n",
            "Epoch: [1][290/710]\tLoss 0.6959 (0.9489)\tAccuracy 75.000 (78.393)\n",
            "Epoch: [1][300/710]\tLoss 0.8318 (0.9433)\tAccuracy 81.250 (78.592)\n",
            "Epoch: [1][310/710]\tLoss 0.4763 (0.9438)\tAccuracy 100.000 (78.537)\n",
            "Epoch: [1][320/710]\tLoss 0.7524 (0.9427)\tAccuracy 87.500 (78.505)\n",
            "Epoch: [1][330/710]\tLoss 0.9935 (0.9435)\tAccuracy 75.000 (78.531)\n",
            "Epoch: [1][340/710]\tLoss 1.1381 (0.9442)\tAccuracy 75.000 (78.574)\n",
            "Epoch: [1][350/710]\tLoss 0.8020 (0.9462)\tAccuracy 81.250 (78.454)\n",
            "Epoch: [1][360/710]\tLoss 0.6086 (0.9443)\tAccuracy 75.000 (78.515)\n",
            "Epoch: [1][370/710]\tLoss 0.7763 (0.9421)\tAccuracy 87.500 (78.588)\n",
            "Epoch: [1][380/710]\tLoss 0.8606 (0.9392)\tAccuracy 75.000 (78.658)\n",
            "Epoch: [1][390/710]\tLoss 0.8448 (0.9393)\tAccuracy 81.250 (78.692)\n",
            "Epoch: [1][400/710]\tLoss 0.8648 (0.9385)\tAccuracy 81.250 (78.725)\n",
            "Epoch: [1][410/710]\tLoss 1.1419 (0.9379)\tAccuracy 75.000 (78.802)\n",
            "Epoch: [1][420/710]\tLoss 0.9524 (0.9353)\tAccuracy 75.000 (78.815)\n",
            "Epoch: [1][430/710]\tLoss 1.0612 (0.9333)\tAccuracy 68.750 (78.857)\n",
            "Epoch: [1][440/710]\tLoss 0.6208 (0.9275)\tAccuracy 87.500 (79.039)\n",
            "Epoch: [1][450/710]\tLoss 1.6794 (0.9274)\tAccuracy 56.250 (79.047)\n",
            "Epoch: [1][460/710]\tLoss 0.8229 (0.9270)\tAccuracy 81.250 (78.932)\n",
            "Epoch: [1][470/710]\tLoss 0.8600 (0.9286)\tAccuracy 75.000 (78.861)\n",
            "Epoch: [1][480/710]\tLoss 0.7142 (0.9273)\tAccuracy 81.250 (78.807)\n",
            "Epoch: [1][490/710]\tLoss 1.8507 (0.9305)\tAccuracy 68.750 (78.781)\n",
            "Epoch: [1][500/710]\tLoss 0.9767 (0.9306)\tAccuracy 75.000 (78.780)\n",
            "Epoch: [1][510/710]\tLoss 0.7428 (0.9331)\tAccuracy 87.500 (78.767)\n",
            "Epoch: [1][520/710]\tLoss 0.6109 (0.9311)\tAccuracy 93.750 (78.815)\n",
            "Epoch: [1][530/710]\tLoss 1.5805 (0.9322)\tAccuracy 56.250 (78.696)\n",
            "Epoch: [1][540/710]\tLoss 0.7595 (0.9307)\tAccuracy 81.250 (78.743)\n",
            "Epoch: [1][550/710]\tLoss 0.8347 (0.9283)\tAccuracy 75.000 (78.823)\n",
            "Epoch: [1][560/710]\tLoss 0.7509 (0.9263)\tAccuracy 81.250 (78.810)\n",
            "Epoch: [1][570/710]\tLoss 0.9375 (0.9242)\tAccuracy 87.500 (78.886)\n",
            "Epoch: [1][580/710]\tLoss 1.0241 (0.9248)\tAccuracy 81.250 (78.894)\n",
            "Epoch: [1][590/710]\tLoss 0.7739 (0.9213)\tAccuracy 81.250 (79.008)\n",
            "Epoch: [1][600/710]\tLoss 1.1699 (0.9221)\tAccuracy 68.750 (78.900)\n",
            "Epoch: [1][610/710]\tLoss 1.0266 (0.9211)\tAccuracy 75.000 (78.928)\n",
            "Epoch: [1][620/710]\tLoss 0.7853 (0.9217)\tAccuracy 87.500 (78.935)\n",
            "Epoch: [1][630/710]\tLoss 0.8481 (0.9202)\tAccuracy 87.500 (78.972)\n",
            "Epoch: [1][640/710]\tLoss 1.2822 (0.9197)\tAccuracy 68.750 (78.988)\n",
            "Epoch: [1][650/710]\tLoss 0.6488 (0.9200)\tAccuracy 93.750 (78.936)\n",
            "Epoch: [1][660/710]\tLoss 0.9736 (0.9182)\tAccuracy 68.750 (78.962)\n",
            "Epoch: [1][670/710]\tLoss 1.2661 (0.9180)\tAccuracy 62.500 (78.968)\n",
            "Epoch: [1][680/710]\tLoss 0.6728 (0.9198)\tAccuracy 87.500 (78.910)\n",
            "Epoch: [1][690/710]\tLoss 1.2310 (0.9194)\tAccuracy 68.750 (78.916)\n",
            "Epoch: [1][700/710]\tLoss 0.6745 (0.9179)\tAccuracy 81.250 (78.959)\n",
            "Test: [  0/177]\tLoss 0.5759 (0.5759)\tAccuracy 87.500 (87.500)\n",
            "Test: [ 10/177]\tLoss 1.0367 (0.7862)\tAccuracy 68.750 (80.114)\n",
            "Test: [ 20/177]\tLoss 0.8863 (0.7954)\tAccuracy 81.250 (80.357)\n",
            "Test: [ 30/177]\tLoss 0.8225 (0.7767)\tAccuracy 75.000 (81.653)\n",
            "Test: [ 40/177]\tLoss 0.7947 (0.7693)\tAccuracy 81.250 (82.470)\n",
            "Test: [ 50/177]\tLoss 0.9979 (0.7755)\tAccuracy 93.750 (82.353)\n",
            "Test: [ 60/177]\tLoss 1.0925 (0.7825)\tAccuracy 75.000 (81.865)\n",
            "Test: [ 70/177]\tLoss 0.7290 (0.8079)\tAccuracy 81.250 (81.074)\n",
            "Test: [ 80/177]\tLoss 0.7124 (0.7993)\tAccuracy 87.500 (81.173)\n",
            "Test: [ 90/177]\tLoss 0.6719 (0.8011)\tAccuracy 87.500 (81.181)\n",
            "Test: [100/177]\tLoss 1.6671 (0.8090)\tAccuracy 68.750 (81.002)\n",
            "Test: [110/177]\tLoss 1.5385 (0.8156)\tAccuracy 68.750 (81.025)\n",
            "Test: [120/177]\tLoss 0.7636 (0.8159)\tAccuracy 87.500 (81.043)\n",
            "Test: [130/177]\tLoss 0.5725 (0.8064)\tAccuracy 87.500 (81.155)\n",
            "Test: [140/177]\tLoss 0.4814 (0.8056)\tAccuracy 81.250 (81.117)\n",
            "Test: [150/177]\tLoss 0.5933 (0.8109)\tAccuracy 87.500 (81.043)\n",
            "Test: [160/177]\tLoss 0.6570 (0.8135)\tAccuracy 87.500 (81.172)\n",
            "Test: [170/177]\tLoss 0.6839 (0.8067)\tAccuracy 81.250 (81.250)\n",
            " *** Accuracy 81.075  *** \n",
            "Saved figure\n",
            "Current best accuracy:  81.07534790039062\n",
            "An Epoch Time:  216.8965141773224\n",
            "Current learning rate:  0.001\n",
            "Epoch: [2][  0/710]\tLoss 0.7446 (0.7446)\tAccuracy 75.000 (75.000)\n",
            "Epoch: [2][ 10/710]\tLoss 0.6796 (0.8097)\tAccuracy 87.500 (81.818)\n",
            "Epoch: [2][ 20/710]\tLoss 1.0838 (0.7842)\tAccuracy 81.250 (81.548)\n",
            "Epoch: [2][ 30/710]\tLoss 0.9400 (0.7742)\tAccuracy 81.250 (81.452)\n",
            "Epoch: [2][ 40/710]\tLoss 0.7162 (0.7860)\tAccuracy 87.500 (82.317)\n",
            "Epoch: [2][ 50/710]\tLoss 0.5857 (0.7890)\tAccuracy 87.500 (81.495)\n",
            "Epoch: [2][ 60/710]\tLoss 0.9916 (0.7853)\tAccuracy 75.000 (81.967)\n",
            "Epoch: [2][ 70/710]\tLoss 0.4923 (0.7682)\tAccuracy 87.500 (82.570)\n",
            "Epoch: [2][ 80/710]\tLoss 0.6573 (0.7731)\tAccuracy 81.250 (82.330)\n",
            "Epoch: [2][ 90/710]\tLoss 1.2457 (0.7746)\tAccuracy 68.750 (82.624)\n",
            "Epoch: [2][100/710]\tLoss 1.0547 (0.7672)\tAccuracy 68.750 (82.921)\n",
            "Epoch: [2][110/710]\tLoss 0.5132 (0.7782)\tAccuracy 100.000 (82.545)\n",
            "Epoch: [2][120/710]\tLoss 1.0236 (0.7840)\tAccuracy 75.000 (82.386)\n",
            "Epoch: [2][130/710]\tLoss 0.6397 (0.7827)\tAccuracy 87.500 (82.490)\n",
            "Epoch: [2][140/710]\tLoss 0.8409 (0.7956)\tAccuracy 75.000 (82.048)\n",
            "Epoch: [2][150/710]\tLoss 0.5891 (0.7867)\tAccuracy 93.750 (82.202)\n",
            "Epoch: [2][160/710]\tLoss 0.8985 (0.7814)\tAccuracy 81.250 (82.337)\n",
            "Epoch: [2][170/710]\tLoss 0.8214 (0.7797)\tAccuracy 75.000 (82.237)\n",
            "Epoch: [2][180/710]\tLoss 0.5977 (0.7766)\tAccuracy 87.500 (82.493)\n",
            "Epoch: [2][190/710]\tLoss 1.1260 (0.7837)\tAccuracy 87.500 (82.232)\n",
            "Epoch: [2][200/710]\tLoss 2.2565 (0.7899)\tAccuracy 68.750 (82.400)\n",
            "Epoch: [2][210/710]\tLoss 0.6145 (0.7918)\tAccuracy 87.500 (82.316)\n",
            "Epoch: [2][220/710]\tLoss 0.5621 (0.7883)\tAccuracy 87.500 (82.353)\n",
            "Epoch: [2][230/710]\tLoss 0.6524 (0.7822)\tAccuracy 87.500 (82.549)\n",
            "Epoch: [2][240/710]\tLoss 0.9419 (0.7787)\tAccuracy 68.750 (82.547)\n",
            "Epoch: [2][250/710]\tLoss 0.6830 (0.7816)\tAccuracy 81.250 (82.296)\n",
            "Epoch: [2][260/710]\tLoss 1.0344 (0.7827)\tAccuracy 75.000 (82.256)\n",
            "Epoch: [2][270/710]\tLoss 0.7491 (0.7820)\tAccuracy 81.250 (82.311)\n",
            "Epoch: [2][280/710]\tLoss 0.6608 (0.7820)\tAccuracy 93.750 (82.362)\n",
            "Epoch: [2][290/710]\tLoss 0.6835 (0.7810)\tAccuracy 87.500 (82.431)\n",
            "Epoch: [2][300/710]\tLoss 0.7619 (0.7865)\tAccuracy 87.500 (82.288)\n",
            "Epoch: [2][310/710]\tLoss 0.7603 (0.7853)\tAccuracy 87.500 (82.295)\n",
            "Epoch: [2][320/710]\tLoss 0.7129 (0.7825)\tAccuracy 93.750 (82.340)\n",
            "Epoch: [2][330/710]\tLoss 0.9993 (0.7802)\tAccuracy 81.250 (82.458)\n",
            "Epoch: [2][340/710]\tLoss 1.1936 (0.7790)\tAccuracy 75.000 (82.478)\n",
            "Epoch: [2][350/710]\tLoss 0.5755 (0.7753)\tAccuracy 87.500 (82.532)\n",
            "Epoch: [2][360/710]\tLoss 0.9924 (0.7771)\tAccuracy 81.250 (82.514)\n",
            "Epoch: [2][370/710]\tLoss 0.4328 (0.7742)\tAccuracy 100.000 (82.648)\n",
            "Epoch: [2][380/710]\tLoss 0.4762 (0.7758)\tAccuracy 100.000 (82.579)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Min_Upload**"
      ],
      "metadata": {
        "id": "R_iXsbLkeXmg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Main_generate_ortho**"
      ],
      "metadata": {
        "id": "N0cAq65cetl_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# upload a pathon file with some useful functions\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "JiMboOfQezqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import os\n",
        "import time\n",
        "import shutil\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim\n",
        "import torch.utils.data\n",
        "import torch.utils.data.distributed\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import datetime\n",
        "import resnet_v1 as resnet"
      ],
      "metadata": {
        "id": "MhFGht6lgNVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check how many GPUs are available\n",
        "num_gpus = torch.cuda.device_count()\n",
        "print(f'Number of GPUs available: {num_gpus}')"
      ],
      "metadata": {
        "id": "dicuN0Tye57N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "now = datetime.datetime.now()\n",
        "time_str = now.strftime(\"[%m-%d]-[%H-%M]-\")"
      ],
      "metadata": {
        "id": "4pUn-UHZe_Ul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/FER/Dataset/RAFDB/Train.zip"
      ],
      "metadata": {
        "id": "xly2j6IzhAsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/Test_FER/Dataset/RAFDB/Test.zip"
      ],
      "metadata": {
        "id": "h9iKabk_hLjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init():\n",
        "\n",
        "  parser = argparse.ArgumentParser(description=\"PyTorch\")\n",
        "\n",
        "  parser.add_argument('--data', type=str, default='/content/RAFDB/')\n",
        "  parser.add_argument('-j', '--workers', default=4, type=int, metavar='N', help='number of data loading workers')\n",
        "  parser.add_argument('-b', '--batch-size', default=8, type=int, metavar='N')\n",
        "\n",
        "  args = parser.parse_args(args=[])\n",
        "  return args"
      ],
      "metadata": {
        "id": "858rk6wxfbbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
        "    with torch.no_grad():\n",
        "        maxk = max(topk)\n",
        "        batch_size = target.size(0)\n",
        "        _, pred = output.topk(maxk, 1, True, True)\n",
        "        pred = pred.t()\n",
        "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "        res = []\n",
        "        for k in topk:\n",
        "            # .contiguous让地址连续\n",
        "            correct_k = correct[:k].contiguous().view(-1).float().sum(0, keepdim=True)\n",
        "            res.append(correct_k.mul_(100.0 / batch_size))\n",
        "        return res\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self, name, fmt=':f'):\n",
        "        self.name = name\n",
        "        self.fmt = fmt\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "    def __str__(self):\n",
        "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
        "        return fmtstr.format(**self.__dict__)"
      ],
      "metadata": {
        "id": "tVYIHpVihww4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "\n",
        "    args= init()\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    best_acc = 0\n",
        "\n",
        "    print('Training time: ' + now.strftime(\"%m-%d %H:%M\"))\n",
        "\n",
        "    # create model\n",
        "    model_cla = resnet.resnet50()\n",
        "    model_cla = torch.nn.DataParallel(model_cla).cuda()\n",
        "    model_cla.to(device)\n",
        "    checkpoint = torch.load('/content/drive/MyDrive/FER/checkpoint_cnn/RAFDB/[08-15]-[18-09]-model_best.pth.tar')\n",
        "    pre_trained_dict = checkpoint['state_dict']\n",
        "    for k, v in pre_trained_dict.items():\n",
        "        print(k, v.shape)\n",
        "    model_cla.load_state_dict(pre_trained_dict)\n",
        "\n",
        "    # Data loading code\n",
        "    traindir = os.path.join(args.data, 'Train')\n",
        "    valdir = os.path.join(args.data, 'Test')\n",
        "\n",
        "    normalize = transforms.Normalize(mean=[0.5758095, 0.4500876, 0.40176094],\n",
        "                                      std=[0.20888616, 0.19142343, 0.18289249])\n",
        "\n",
        "    train_dataset = datasets.ImageFolder(traindir,\n",
        "                                         transforms.Compose([transforms.Resize((224, 224)),\n",
        "                                                             transforms.RandomHorizontalFlip(),\n",
        "                                                             transforms.ToTensor(),\n",
        "                                                             normalize]))\n",
        "\n",
        "    test_dataset = datasets.ImageFolder(valdir,\n",
        "                                        transforms.Compose([transforms.Resize((224, 224)),\n",
        "                                                            transforms.ToTensor(),\n",
        "                                                            normalize]))\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset,\n",
        "                                               batch_size=args.batch_size,\n",
        "                                               shuffle=True,\n",
        "                                               num_workers=args.workers,\n",
        "                                               pin_memory=True)\n",
        "    val_loader = torch.utils.data.DataLoader(test_dataset,\n",
        "                                             batch_size=args.batch_size,\n",
        "                                             shuffle=False,\n",
        "                                             num_workers=args.workers,\n",
        "                                             pin_memory=True)\n",
        "    model_cla.eval()\n",
        "    feature_1 = []\n",
        "    feature_2 = []\n",
        "    feature_3 = []\n",
        "    label = []\n",
        "    top1 = AverageMeter('Accuracy', ':6.3f')\n",
        "    with torch.no_grad():\n",
        "        for i, (images, target) in enumerate(val_loader):\n",
        "            images = images.cuda()\n",
        "            target = target.cuda()\n",
        "            # compute output\n",
        "            x_1, x_2, x_3, x_fc1, x_fc2, x_fc3, output = model_cla(images)\n",
        "            acc1, _ = accuracy(output, target, topk=(1, 5))\n",
        "            top1.update(acc1[0], images.size(0))\n",
        "            x_1 = x_1.permute(0, 2, 3, 1)\n",
        "            x_2 = x_2.permute(0, 2, 3, 1)\n",
        "            x_3 = x_3.permute(0, 2, 3, 1)\n",
        "            if i == 0:\n",
        "                feature_1 = x_1.cpu().numpy()\n",
        "                feature_2 = x_2.cpu().numpy()\n",
        "                feature_3 = x_3.cpu().numpy()\n",
        "                label = target.cpu().numpy()\n",
        "            else:\n",
        "                feature_1 = np.concatenate((feature_1, x_1.cpu().numpy()),axis=0)\n",
        "                feature_2 = np.concatenate((feature_2, x_2.cpu().numpy()),axis=0)\n",
        "                feature_3 = np.concatenate((feature_3, x_3.cpu().numpy()),axis=0)\n",
        "                label = np.concatenate((label, target.cpu().numpy()),axis=0)\n",
        "\n",
        "        print(' *** Accuracy {top1.avg:.3f}  *** '.format(top1=top1))\n",
        "    # train\n",
        "    # np.save(\"/content/drive/MyDrive/FER/Orthognal_npy/train_1_RAFDB2_v1.npy\",feature_1)\n",
        "    # np.save(\"/content/drive/MyDrive/FER/Orthognal_npy/train_2_RAFDB2_v1.npy\",feature_2)\n",
        "    # np.save(\"/content/drive/MyDrive/FER/Orthognal_npy/train_3_RAFDB2_v1.npy\",feature_3)\n",
        "    # np.save(\"/content/drive/MyDrive/FER/Orthognal_npy/train_label_RAFDB2_v1.npy\",label)\n",
        "    # # test\n",
        "    np.save(\"/content/drive/MyDrive/FER/Orthognal_npy/test_1_RAFDB2_v1.npy\",feature_1)\n",
        "    np.save(\"/content/drive/MyDrive/FER/Orthognal_npy/test_2_RAFDB2_v1.npy\",feature_2)\n",
        "    np.save(\"/content/drive/MyDrive/FER/Orthognal_npy/test_3_RAFDB2_v1.npy\",feature_3)\n",
        "    np.save(\"/content/drive/MyDrive/FER/Orthognal_npy/test_label_RAFDB2_v1.npy\",label)\n"
      ],
      "metadata": {
        "id": "YNTnrMj6hru2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "BhQaqLAeg_4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q-vit_RAFDB_Upload**"
      ],
      "metadata": {
        "id": "Hi74EF8ylgur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/FER/complexnn.zip"
      ],
      "metadata": {
        "id": "rCFZJdpNwFBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wsgiref import validate\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(42)\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "id": "BKcl998flmlb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U tensorflow-addons"
      ],
      "metadata": {
        "id": "rWr8K7ITmKPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_addons as tfa\n",
        "import os"
      ],
      "metadata": {
        "id": "oTEWIYm1lsjU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
        "config = tf.compat.v1.ConfigProto()\n",
        "config.gpu_options.allow_growth=True\n",
        "sess = tf.compat.v1.Session(config=config)"
      ],
      "metadata": {
        "id": "erOuOZqLnJh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.random.seed(42)"
      ],
      "metadata": {
        "id": "ro9TeNg-nTcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load orthogonal features\n",
        "train_1=np.load('/content/drive/MyDrive/FER/Orthognal_npy/train_1_RAFDB2_v1.npy',encoding = \"latin1\")\n",
        "train_2=np.load('/content/drive/MyDrive/FER/Orthognal_npy/train_2_RAFDB2_v1.npy',encoding = \"latin1\")\n",
        "train_3=np.load('/content/drive/MyDrive/FER/Orthognal_npy/train_3_RAFDB2_v1.npy',encoding = \"latin1\")\n",
        "train_label=np.load('/content/drive/MyDrive/FER/Orthognal_npy/train_label_RAFDB2_v1.npy',encoding = \"latin1\")\n",
        "test_1=np.load('/content/drive/MyDrive/FER/Orthognal_npy/test_1_RAFDB2_v1.npy',encoding = \"latin1\")\n",
        "test_2=np.load('/content/drive/MyDrive/FER/Orthognal_npy/test_2_RAFDB2_v1.npy',encoding = \"latin1\")\n",
        "test_3=np.load('/content/drive/MyDrive/FER/Orthognal_npy/test_3_RAFDB2_v1.npy',encoding = \"latin1\")\n",
        "test_label=np.load('/content/drive/MyDrive/FER/Orthognal_npy/test_label_RAFDB2_v1.npy',encoding = \"latin1\")"
      ],
      "metadata": {
        "id": "Ht1ShW_gnWfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# average the three sub-features and put them into a quaternion matrix\n",
        "q_train=np.zeros([train_1.shape[0],train_1.shape[1],train_1.shape[2],train_1.shape[-1]*4])\n",
        "train_r=(train_1+train_2+train_3)/3\n",
        "q_train[:,:,:,:train_1.shape[-1]]=train_r\n",
        "q_train[:,:,:,train_1.shape[-1]:2*train_1.shape[-1]]=train_1\n",
        "q_train[:,:,:,2*train_1.shape[-1]:3*train_1.shape[-1]]=train_2\n",
        "q_train[:,:,:,3*train_1.shape[-1]:]=train_3\n",
        "train = np.transpose(q_train,(0,3,1,2))\n",
        "train = np.reshape(train,(train_1.shape[0],64*4,49))  # 256 --> 64\n",
        "\n",
        "q_test=np.zeros([test_1.shape[0],test_1.shape[1],test_1.shape[2],test_1.shape[-1]*4])\n",
        "test_r=(test_1+test_2+test_3)/3\n",
        "q_test[:,:,:,:test_1.shape[-1]]=test_r\n",
        "q_test[:,:,:,test_1.shape[-1]:2*test_1.shape[-1]]=test_1\n",
        "q_test[:,:,:,2*test_1.shape[-1]:3*test_1.shape[-1]]=test_2\n",
        "q_test[:,:,:,3*test_1.shape[-1]:]=test_3\n",
        "test = np.transpose(q_test,(0,3,1,2))\n",
        "test = np.reshape(test,(test_1.shape[0],64*4,49)) # 256 --> 64\n",
        "\n",
        "\n",
        "input_shape = (64*4, 49) # 256 --> 64\n",
        "num_classes = 7\n",
        "learning_rate = 0.00001\n",
        "weight_decay = 0.0001\n",
        "batch_size = 8\n",
        "num_epochs = 100  # 400 >>> 100\n",
        "num_patches = 64*4  # 256 --> 64\n",
        "projection_dim = 48\n",
        "num_heads = 8\n",
        "transformer_units = [\n",
        "    projection_dim * 2,\n",
        "    projection_dim,\n",
        "]\n",
        "transformer_layers = 4\n",
        "mlp_head_units = [2048, 256]  # 1024 --> 256\n",
        "\n",
        "## go complexnn/init.py and change the \"from keras.utils.generic_utils.....\" to\n",
        "## \"from tensorflow.keras.utils import (serialize_keras_object, deserialize_keras_object)\"\n",
        "from   complexnn      import *\n",
        "from tensorflow.keras.layers import (\n",
        "    Dense,\n",
        ")"
      ],
      "metadata": {
        "id": "9JYGgkg-sq7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q-MHSA module\n",
        "class MultiHeadSelfAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        if embed_dim % num_heads != 0:\n",
        "            raise ValueError(\n",
        "                f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\"\n",
        "            )\n",
        "        self.projection_dim = embed_dim // num_heads\n",
        "        self.query_dense = QuaternionDense(embed_dim)\n",
        "        self.key_dense = QuaternionDense(embed_dim)\n",
        "        self.value_dense = QuaternionDense(embed_dim)\n",
        "        self.combine_heads = QuaternionDense(embed_dim)\n",
        "\n",
        "    def attention(self, query, key, value):\n",
        "        score = tf.matmul(query, key, transpose_b=True)\n",
        "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
        "        scaled_score = score / tf.math.sqrt(dim_key)\n",
        "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
        "        output = tf.matmul(weights, value)\n",
        "        return output, weights\n",
        "\n",
        "    def separate_heads(self, x, batch_size):\n",
        "        x = tf.reshape(\n",
        "            x, (batch_size, -1, self.num_heads, self.projection_dim)\n",
        "        )\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, inputs):\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        query = self.query_dense(inputs)\n",
        "        key = self.key_dense(inputs)\n",
        "        value = self.value_dense(inputs)\n",
        "\n",
        "        query = self.separate_heads(query, batch_size)\n",
        "        key = self.separate_heads(key, batch_size)\n",
        "        value = self.separate_heads(value, batch_size)\n",
        "\n",
        "        attention, weights = self.attention(query, key, value)\n",
        "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
        "        concat_attention = tf.reshape(\n",
        "            attention, (batch_size, -1, self.embed_dim)\n",
        "        )\n",
        "        output = self.combine_heads(concat_attention)\n",
        "        return output"
      ],
      "metadata": {
        "id": "g0ROyhZswSz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def QF_Net(x, hidden_units, dropout_rate):\n",
        "    for units in hidden_units:\n",
        "        x = QuaternionConv2D(int(units/4), 3, strides=1, padding=\"same\")(x)\n",
        "        x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "        x = layers.Activation(tf.nn.gelu)(x)\n",
        "        x = QuaternionConv2D(int(units/4), 3, strides=1, padding=\"same\")(x)\n",
        "    return x\n",
        "\n",
        "def multilayer_perceptron(x, hidden_units, dropout_rate):\n",
        "    for units in hidden_units:\n",
        "        x = QuaternionDense(units, activation='relu')(x)\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "class PatchEncoder(layers.Layer):\n",
        "    def __init__(self, num_patches, projection_dim):\n",
        "        super(PatchEncoder, self).__init__()\n",
        "        self.num_patches = num_patches\n",
        "        # 降为全连接\n",
        "        self.projection = layers.Dense(units=projection_dim)\n",
        "        self.position_embedding = layers.Embedding(\n",
        "            input_dim=num_patches, output_dim=projection_dim\n",
        "        )\n",
        "\n",
        "    def call(self, patch):\n",
        "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
        "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
        "        # encoded = patch + self.position_embedding(positions)\n",
        "        return encoded\n",
        "\n",
        "\n",
        "def create_qvit_classifier():\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "\n",
        "    # position embedding\n",
        "    encoded_patches = PatchEncoder(num_patches, projection_dim)(inputs)\n",
        "\n",
        "    for _ in range(transformer_layers):\n",
        "\n",
        "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "\n",
        "        attention_output = MultiHeadSelfAttention(projection_dim, num_heads)(x1)\n",
        "\n",
        "        x2 = layers.Add()([attention_output, encoded_patches])\n",
        "\n",
        "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "\n",
        "        x4 = tf.keras.layers.Reshape((16,16,48))(x3) # 32*32 --> 16*16\n",
        "\n",
        "        x5 = QF_Net(x4, hidden_units=transformer_units, dropout_rate=0.3)\n",
        "\n",
        "        x6 = tf.keras.layers.Reshape((64*4, 48))(x5) #256-->64\n",
        "\n",
        "        encoded_patches = layers.Add()([x6, x2])\n",
        "\n",
        "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "    representation = layers.Flatten()(representation)\n",
        "    representation = layers.Dropout(0.5)(representation)\n",
        "\n",
        "    features = multilayer_perceptron(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
        "\n",
        "    logits = layers.Dense(num_classes)(features)\n",
        "\n",
        "    model = keras.Model(inputs=inputs, outputs=logits)\n",
        "    return model\n",
        "\n",
        "def run_experiment(model):\n",
        "    optimizer = tf.optimizers.Adam(\n",
        "        learning_rate=learning_rate\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        metrics=[\n",
        "            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
        "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    checkpoint_filepath = \"./tmp/RAFDB/model_{epoch:03d}-{val_accuracy:.4f}.h5\"\n",
        "    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "        checkpoint_filepath,\n",
        "        monitor=\"val_accuracy\",\n",
        "        save_best_only=True,\n",
        "        save_weights_only=True,\n",
        "    )\n",
        "\n",
        "    history = model.fit(\n",
        "        x=train,\n",
        "        y=train_label,\n",
        "        batch_size=batch_size,\n",
        "        epochs=num_epochs,\n",
        "        validation_data=(test, test_label),\n",
        "        callbacks=[checkpoint_callback],\n",
        "    )\n",
        "\n",
        "    return history"
      ],
      "metadata": {
        "id": "v_LFR15gz4hf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vit_classifier = create_qvit_classifier()\n",
        "history = run_experiment(vit_classifier)"
      ],
      "metadata": {
        "id": "7xJRn5GD03aM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}