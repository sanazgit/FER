{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanazgit/FER/blob/main/FER.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/Gabrella/QOT/blob/main/main_Upload.py"
      ],
      "metadata": {
        "id": "ERn_LJEI16nk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sByzx98bI4HG",
        "outputId": "3a279fe5-c22e-46c1-8f99-05832484c958"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ“**Properties**"
      ],
      "metadata": {
        "id": "IXmCBgvxhw42"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://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@github.com/sanazgit/FER.git"
      ],
      "metadata": {
        "id": "fDA_pkMOe-dE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchsnooper"
      ],
      "metadata": {
        "id": "tU1aNkSQjYF7",
        "outputId": "3b181323-9526-4e9d-9fc8-999fe2d97d91",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchsnooper\n",
            "  Downloading TorchSnooper-0.8-py3-none-any.whl (7.2 kB)\n",
            "Collecting pysnooper>=0.1.0 (from torchsnooper)\n",
            "  Downloading PySnooper-1.2.0-py2.py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchsnooper) (1.23.5)\n",
            "Installing collected packages: pysnooper, torchsnooper\n",
            "Successfully installed pysnooper-1.2.0 torchsnooper-0.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# upload a pathon file with some useful functions\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "vO8ThopFMv1b",
        "outputId": "fee1f20d-24e2-4e92-e463-34e165b0adb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-8505be09-fd23-402d-a584-7e7952c1564b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-8505be09-fd23-402d-a584-7e7952c1564b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving resnet_pose_v2.py to resnet_pose_v2.py\n",
            "Saving option.py to option.py\n",
            "Saving util.py to util.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('./FER')"
      ],
      "metadata": {
        "id": "zirx7OrlfgxN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torchsummary import summary\n",
        "import random\n",
        "import time\n",
        "from torchvision import datasets, transforms\n",
        "import torch.utils.data as data\n",
        "import torchvision.models as models\n",
        "import os\n",
        "import torch.nn as nn\n",
        "import torch.backends.cudnn as cudnn\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "from util import *\n",
        "from PIL import Image\n",
        "import shutil\n",
        "import torch.nn.parallel\n",
        "import torch.optim\n",
        "import random\n",
        "import numbers\n",
        "import torch.nn.functional as F\n",
        "#import resnet_pose_estimation_model as resnet\n",
        "import resnet_pose_v2 as resnet\n",
        "import option"
      ],
      "metadata": {
        "id": "B7VgGvVhhREc"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ“‘**Build Dataset**"
      ],
      "metadata": {
        "id": "kiYuNXfB1oHl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/Test_FER/Dataset/RAFDB/RAFDB.zip"
      ],
      "metadata": {
        "id": "OrrBnGBqierI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell is for splitting dataset..."
      ],
      "metadata": {
        "id": "O3b87arFRDi6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def select_random_files(src_folder, dst_folder, percentage=0.1):\n",
        "    # Get list of all files in the source folder\n",
        "    all_files = [f for f in os.listdir(src_folder) if os.path.isfile(os.path.join(src_folder, f))]\n",
        "\n",
        "    # Calculate number of files to select\n",
        "    select_count = int(len(all_files) * percentage)\n",
        "\n",
        "    # Randomly select files\n",
        "    selected_files = random.sample(all_files, select_count)\n",
        "\n",
        "    # Copy the selected files to the destination folder\n",
        "    for file_name in selected_files:\n",
        "        src_path = os.path.join(src_folder, file_name)\n",
        "        dst_path = os.path.join(dst_folder, file_name)\n",
        "\n",
        "        shutil.copy2(src_path, dst_path)\n",
        "\n",
        "# Path to your image folders\n",
        "folder1 = '/content/RAFDB/train/'\n",
        "folder2 = '/content/RAFDB/test/'\n",
        "\n",
        "# Create the new folder if it doesn't exist\n",
        "selected_images = 'dataset'\n",
        "if not os.path.exists(selected_images):\n",
        "    os.makedirs(selected_images)\n",
        "\n",
        "# Select and copy images\n",
        "select_random_files(folder1,selected_images)\n",
        "select_random_files(folder2, selected_images)"
      ],
      "metadata": {
        "id": "s1QL1HkE4nHW"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell is for moving datas to dataset folder. so first you must make a folder an then run the code for train and test folder"
      ],
      "metadata": {
        "id": "OnRJP195RQiL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# Define the source and destination folders\n",
        "src_folder = './RAFDB/train/'\n",
        "dst_folder = './RAFDB/dataset/' # make dataset fplder into RAFDB\n",
        "\n",
        "# List all files in the source folder\n",
        "files = os.listdir(src_folder)\n",
        "\n",
        "# Loop through each file and move it to the destination folder\n",
        "for file in files:\n",
        "    # Construct full file path\n",
        "    src_path = os.path.join(src_folder, file)\n",
        "    dst_path = os.path.join(dst_folder, file)\n",
        "\n",
        "    # Check if the file is an image (e.g., .jpg, .png)\n",
        "    if file.endswith(('.jpg', '.png', '.jpeg')):\n",
        "        shutil.move(src_path, dst_path)\n",
        "\n",
        "print(\"Images moved successfully!\")\n"
      ],
      "metadata": {
        "id": "g1SSptcOn8Z-",
        "outputId": "84e3d5c0-aebe-44bc-871f-9124097f46c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Images moved successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_folder = './RAFDB/dataset/'\n",
        "image_files = [f for f in os.listdir(image_folder) if f.endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff'))]\n",
        "print(f\"There are {len(image_files)} images in the folder.\")"
      ],
      "metadata": {
        "id": "Q3quVgEePrA5",
        "outputId": "7b77b276-1aa3-43c3-e783-11c91bb37b30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 15339 images in the folder.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Do for dataset AND data_facial\n",
        "\n",
        "# Specify the path to your image folder\n",
        "image_folder = '/content/RAFDB/dataset/'\n",
        "\n",
        "# Iterate over the images in the folder\n",
        "for image_file in os.listdir(image_folder):\n",
        "    # Check if \"_aligned\" is in the image name\n",
        "    if \"_aligned\" in image_file:\n",
        "        # Create the new name by replacing \"_aligned\" with an empty string\n",
        "        new_image_name = image_file.replace(\"_aligned\", \"\")\n",
        "\n",
        "        # Rename the image\n",
        "        os.rename(os.path.join(image_folder, image_file), os.path.join(image_folder, new_image_name))"
      ],
      "metadata": {
        "id": "mEO9iMBbqtLG"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ–¼ **Read & Pre-Process the Data**"
      ],
      "metadata": {
        "id": "a2iM6-zB2TOC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def select_data(label_path, data_facial):\n",
        "    data_facial_files = [item[0] for item in data_facial]\n",
        "\n",
        "    file = open(label_path)\n",
        "    lines = file.readlines()\n",
        "    train_label = []\n",
        "    test_label = []\n",
        "    for i in range(len(lines)):\n",
        "        num = int(lines[i][-2]) - 1\n",
        "        s1 = list(lines[i])\n",
        "        s1[-2] = str(num)\n",
        "        transformed_file = ''.join(s1)\n",
        "\n",
        "        if transformed_file.split()[0] not in data_facial_files:\n",
        "            continue\n",
        "\n",
        "        if lines[i][0:3] == 'tra':\n",
        "            train_label.append(transformed_file)\n",
        "        elif lines[i][0:3] == 'tes':\n",
        "            test_label.append(transformed_file)\n",
        "\n",
        "    return train_label, test_label  # output the list and delvery it into ImageFolder"
      ],
      "metadata": {
        "id": "4cGwF907tTyD"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args= option.init()\n",
        "\n",
        "data_root = args.data\n",
        "save_path=args.log_path\n",
        "lr=args.lr\n",
        "momentum=args.momentum\n",
        "weight_decay=args.weight_decay\n",
        "epochs=args.epochs\n",
        "batch_size = args.batch_size\n",
        "\n",
        "data_label =  args.data_label\n",
        "data_facial_path=args.land_marks\n",
        "data_facial= np.load(data_facial_path,allow_pickle=True)\n",
        "\n",
        "# Remove \"_aligned\" from each string in the array\n",
        "for i in range(data_facial.shape[0]):\n",
        "    data_facial[i, 0] = data_facial[i, 0].replace('_aligned', '')\n",
        "\n",
        "if args.dataset=='RAF':\n",
        "  train_label, test_label = select_data(data_label, data_facial)\n",
        "else:\n",
        "  train_label, test_label = random_choose_data(data_label)\n",
        "\n",
        "\n",
        "# RAF-DB\n",
        "normalize = transforms.Normalize(mean=[0.5758095, 0.4500876, 0.40176094],\n",
        "                                 std=[0.20888616, 0.19142343, 0.18289249])\n",
        "\n",
        "mytransform = transforms.Compose([transforms.Resize((224, 224)),\n",
        "                                  transforms.RandomHorizontalFlip(),\n",
        "                                  transforms.ToTensor(),\n",
        "                                  normalize])\n",
        "\n",
        "mytransform1 = transforms.Compose([transforms.Resize((224, 224)),\n",
        "                                   transforms.ToTensor(),\n",
        "                                   normalize])\n",
        "\n",
        "\n",
        "\n",
        "train_data=myImageFloder(root=data_root, label=train_label, transform=mytransform)\n",
        "test_data=myImageFloder(root=data_root, label=test_label, transform=mytransform1)\n",
        "val_data=myImageFloder(root=data_root, label=test_label, transform=mytransform1)\n",
        "\n",
        "# load\n",
        "train_loader = torch.utils.data.DataLoader(train_data,batch_size= batch_size, shuffle=True, num_workers= args.workers, pin_memory=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data,batch_size= batch_size, shuffle=True, num_workers=  args.workers, pin_memory=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_data,batch_size= batch_size, shuffle=True, num_workers= args.workers, pin_memory=True)\n"
      ],
      "metadata": {
        "id": "1zcukcdwfy7t",
        "outputId": "31774774-3a07-4d58-a260-07c89cbcd72e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the total image is 11354\n",
            "['anger', 'disgust', 'fear', 'happy', 'neural', 'sad', 'surprise']\n",
            "the total image is 2827\n",
            "['anger', 'disgust', 'fear', 'happy', 'neural', 'sad', 'surprise']\n",
            "the total image is 2827\n",
            "['anger', 'disgust', 'fear', 'happy', 'neural', 'sad', 'surprise']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "âœ”**To ensure consistency:**\n",
        "\n",
        "```\n",
        " data_name = [item[0] for item in data_facial]\n",
        "train_filenames = [item[0] for item in train_data.imgs]\n",
        "\n",
        "missing_files = [f for f in train_filenames if f not in data_name]\n",
        "\n",
        "if missing_files:\n",
        "    print(\"Files in train_data but not in data_facial:\", missing_files)\n",
        "else:\n",
        "    print(\"All files in train_data are present in data_facial.\")\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "fol-RgiZuB6r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# âœ” **Min_Upload**"
      ],
      "metadata": {
        "id": "R_iXsbLkeXmg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "\n",
        "    args= option.init()\n",
        "    #os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    best_acc = 0\n",
        "    now = datetime.datetime.now()\n",
        "    time_str = now.strftime(\"[%m-%d]-[%H-%M]-\")\n",
        "\n",
        "    print('Training time: ' + now.strftime(\"%m-%d %H:%M\"))\n",
        "\n",
        "    # create model\n",
        "    model_cla = resnet.resnet50()\n",
        "    model_cla.fc = nn.Linear(2048, 12666)\n",
        "    model_cla = torch.nn.DataParallel(model_cla).cuda()\n",
        "    model_cla.to(device)\n",
        "    # pretrianed on msceleb\n",
        "    checkpoint = torch.load('/content/drive/MyDrive/Test_FER/Pre_trained/resnet50_pretrained_on_msceleb.pth.tar')\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "    pre_trained_dict = checkpoint['state_dict']\n",
        "    model_dict = model_cla.state_dict()\n",
        "    for k, v in pre_trained_dict.items():\n",
        "        if k in model_dict:\n",
        "            print(k, v.shape)\n",
        "    pretrained_dict = {k: v for k, v in pre_trained_dict.items() if k in model_dict}\n",
        "    model_dict.update(pretrained_dict)\n",
        "    model_cla.load_state_dict(model_dict)\n",
        "    model_cla.module.fc = nn.Linear(64*3, 7).cuda()\n",
        "    #model_cla.module.fc = nn.Linear(256*3, 7)\n",
        "\n",
        "    # define loss function (criterion) and optimizer\n",
        "\n",
        "    criterion_val = nn.CrossEntropyLoss().cuda()\n",
        "    criterion_train =  nn.CrossEntropyLoss().cuda()\n",
        "\n",
        "\n",
        "\n",
        "    optimizer = torch.optim.SGD(model_cla.parameters(),\n",
        "                                args.lr,\n",
        "                                momentum=args.momentum,\n",
        "                                weight_decay=args.weight_decay\n",
        "                                )\n",
        "\n",
        "    recorder = RecorderMeter(args.epochs)\n",
        "\n",
        "    # optionally resume from a checkpoint\n",
        "    if args.resume:\n",
        "        if os.path.isfile(args.resume):\n",
        "            print(\"=> loading checkpoint '{}'\".format(args.resume))\n",
        "            checkpoint = torch.load(args.resume)\n",
        "            args.start_epoch = checkpoint['epoch']\n",
        "            best_acc = checkpoint['best_acc']\n",
        "            recorder = checkpoint['recorder']\n",
        "            best_acc = best_acc.to()\n",
        "            model_cla.load_state_dict(checkpoint['state_dict'])\n",
        "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "            print(\"=> loaded checkpoint '{}' (epoch {})\".format(args.resume, checkpoint['epoch']))\n",
        "        else:\n",
        "            print(\"=> no checkpoint found at '{}'\".format(args.resume))\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "\n",
        "    if args.evaluate:\n",
        "        checkpoint = torch.load(args.evaluate_path)\n",
        "        model_cla.load_state_dict(checkpoint['state_dict'])\n",
        "        validate(val_loader, model_cla, criterion_val, args)\n",
        "        torch.cuda.empty_cache()\n",
        "        return\n",
        "\n",
        "    for epoch in range(args.start_epoch, args.epochs):\n",
        "        start_time = time.time()\n",
        "        # update learning rate\n",
        "        current_learning_rate = adjust_learning_rate(optimizer, epoch, args)\n",
        "        print('Current learning rate: ', current_learning_rate)\n",
        "        txt_name = args.log_path + '-log.txt'\n",
        "        with open(txt_name, 'a') as f:\n",
        "            f.write('Current learning rate: ' + str(current_learning_rate) + '\\n')\n",
        "\n",
        "        # train for one epoch\n",
        "        train_acc, train_los = train(train_loader, model_cla, criterion_train, optimizer, epoch, args, data_facial)\n",
        "\n",
        "        # evaluate on validation set\n",
        "        val_acc, val_los = validate(val_loader, model_cla, criterion_val, args, data_facial)\n",
        "\n",
        "        recorder.update(epoch, train_los, train_acc, val_los, val_acc)\n",
        "        curve_name = args.log_path + '-log.png'\n",
        "        recorder.plot_curve(curve_name)\n",
        "\n",
        "        # remember best acc and save checkpoint\n",
        "        is_best = val_acc > best_acc\n",
        "        best_acc = max(val_acc, best_acc)\n",
        "\n",
        "        print('Current best accuracy: ', best_acc.item())\n",
        "        txt_name = args.log_path + '-log.txt'\n",
        "        with open(txt_name, 'a') as f:\n",
        "            f.write('Current best accuracy: ' + str(best_acc.item()) + '\\n')\n",
        "\n",
        "        save_checkpoint({'state_dict': model_cla.state_dict()}, is_best, args)\n",
        "        end_time = time.time()\n",
        "        epoch_time = end_time - start_time\n",
        "        print(\"An Epoch Time: \", epoch_time)\n",
        "        txt_name = args.log_path + '-log.txt'\n",
        "        with open(txt_name, 'a') as f:\n",
        "            f.write(str(epoch_time) + '\\n')\n",
        "        # scheduler.step(val_acc)\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "def train(train_loader, model_cla, criterion, optimizer, epoch, args, data_facial):\n",
        "    losses = AverageMeter('Loss', ':.4f')\n",
        "    top1 = AverageMeter('Accuracy', ':6.3f')\n",
        "    top1_1 = AverageMeter('Accuracy', ':6.3f')\n",
        "    top1_2 = AverageMeter('Accuracy', ':6.3f')\n",
        "    progress = ProgressMeter(len(train_loader),\n",
        "                             [losses, top1],\n",
        "                             prefix=\"Epoch: [{}]\".format(epoch))\n",
        "    soft_max = nn.Softmax(dim=1)\n",
        "\n",
        "    # switch mode\n",
        "    model_cla.train()\n",
        "    l=args.range\n",
        "    data_name = [item[0] for item in data_facial]\n",
        "\n",
        "    for i, (images, target, fn) in enumerate(train_loader):\n",
        "\n",
        "      # search\n",
        "      facial_indx = []\n",
        "      for j in range(len(fn)):\n",
        "        facial_indx.append(data_name.index(fn[j]))\n",
        "      facial=data_facial[facial_indx,1]\n",
        "      facial = np.stack(facial, axis=0)\n",
        "      images,rect,rect_local= pre_pro(images,facial,0.8,0.5,l,args.workers)\n",
        "\n",
        "      # length = len(images)\n",
        "      images = images.cuda()\n",
        "      target = target.cuda()\n",
        "\n",
        "      model_cla.module.set_rect(rect)\n",
        "      model_cla.module.set_rect_local(rect_local)\n",
        "\n",
        "\n",
        "\n",
        "      # compute output\n",
        "\n",
        "      _,_,_,_,_,_,x_gl_fc1, x_gl_fc2, x_gl_fc3, x_al_fc1, x_al_fc2, x_al_fc3, out_gl, out_al = model_cla(images)\n",
        "\n",
        "      # .... Global\n",
        "      x_gl_fc1 = soft_max(x_gl_fc1)\n",
        "      x_gl_fc2 = soft_max(x_gl_fc2)\n",
        "      x_gl_fc3 = soft_max(x_gl_fc3)\n",
        "\n",
        "      # .... Local\n",
        "      x_al_fc1 = soft_max(x_al_fc1)\n",
        "      x_al_fc2 = soft_max(x_al_fc2)\n",
        "      x_al_fc3 = soft_max(x_al_fc3)\n",
        "\n",
        "      # compute loss\n",
        "      output =  (args.beta1 * out_gl) + ((1-args.beta1) * out_al)\n",
        "\n",
        "      loss_gl_softmax = criterion(out_gl, target)\n",
        "      loss_al_softmax = criterion(out_al, target)\n",
        "\n",
        "      loss_gl_orthognal = orthognal_loss(x_gl_fc1, x_gl_fc2, x_gl_fc3)\n",
        "      loss_al_orthognal = orthognal_loss(x_al_fc1, x_al_fc2, x_al_fc3)\n",
        "\n",
        "      loss = (args.beta1 * loss_gl_softmax) + (0.2 * loss_gl_orthognal) + ((1-args.beta1) * loss_al_softmax) + (0.2 * loss_al_orthognal)\n",
        "\n",
        "      # measure accuracy and record loss\n",
        "      acc1, _ = accuracy(output, target, topk=(1, 5))\n",
        "      acc2, _ = accuracy(out_gl, target, topk=(1, 5))\n",
        "      acc3, _ = accuracy(out_al, target, topk=(1, 5))\n",
        "\n",
        "      losses.update(loss.item(), images.size(0))\n",
        "      top1.update(acc1[0], images.size(0))\n",
        "      top1_1.update(acc2[0], images.size(0))\n",
        "      top1_2.update(acc3[0], images.size(0))\n",
        "\n",
        "      # compute gradient and do SGD step\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # print loss and accuracy\n",
        "      if i % args.print_freq == 0:\n",
        "        progress.display(i, args)\n",
        "\n",
        "      if i % args.batch_size== 0:  # e.g., some_number can be 100 or 500 based on your dataset size and batch size\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "\n",
        "    return top1.avg, losses.avg\n",
        "\n",
        "\n",
        "def validate(val_loader, model, criterion, args, data_facial):\n",
        "    losses = AverageMeter('Loss', ':.4f')\n",
        "    top1 = AverageMeter('Accuracy', ':6.3f')\n",
        "    top1_1 = AverageMeter('Accuracy', ':6.3f')\n",
        "    top1_2 = AverageMeter('Accuracy', ':6.3f')\n",
        "    progress = ProgressMeter(len(val_loader),\n",
        "                             [losses, top1],\n",
        "                             prefix='Test: ')\n",
        "\n",
        "    # switch to evaluate mode\n",
        "    model.eval()\n",
        "    soft_max = nn.Softmax(dim=1)\n",
        "    l=args.range\n",
        "\n",
        "    with torch.no_grad():\n",
        "      data_name = [item[0] for item in data_facial]\n",
        "\n",
        "      for i, (images, target, fn) in enumerate(val_loader):\n",
        "\n",
        "        # search\n",
        "        facial_indx = []\n",
        "        for j in range(len(fn)):\n",
        "          facial_indx.append(data_name.index(fn[j]))\n",
        "        facial=data_facial[facial_indx,1]\n",
        "        facial = np.stack(facial, axis=0)\n",
        "        images,rect,rect_local= pre_pro(images,facial,0.8,0.5,l,args.workers)\n",
        "\n",
        "        images = images.cuda()\n",
        "        target = target.cuda()\n",
        "\n",
        "        model.module.set_rect(rect)\n",
        "        model.module.set_rect_local(rect_local)\n",
        "\n",
        "\n",
        "        # compute output\n",
        "        _,_,_,_,_,_,x_gl_fc1, x_gl_fc2, x_gl_fc3, x_al_fc1, x_al_fc2, x_al_fc3, out_gl, out_al = model(images)\n",
        "\n",
        "        # .... Global\n",
        "        x_gl_fc1 = soft_max(x_gl_fc1)\n",
        "        x_gl_fc2 = soft_max(x_gl_fc2)\n",
        "        x_gl_fc3 = soft_max(x_gl_fc3)\n",
        "\n",
        "        # .... Local\n",
        "        x_al_fc1 = soft_max(x_al_fc1)\n",
        "        x_al_fc2 = soft_max(x_al_fc2)\n",
        "        x_al_fc3 = soft_max(x_al_fc3)\n",
        "\n",
        "        # compute loss\n",
        "        output =  (args.beta1 * out_gl) + ((1-args.beta1) * out_al)\n",
        "\n",
        "        loss_gl_softmax = criterion(out_gl, target)\n",
        "        loss_al_softmax = criterion(out_al, target)\n",
        "\n",
        "        loss_gl_orthognal = orthognal_loss(x_gl_fc1, x_gl_fc2, x_gl_fc3)\n",
        "        loss_al_orthognal = orthognal_loss(x_al_fc1, x_al_fc2, x_al_fc3)\n",
        "\n",
        "        loss = (args.beta1 * loss_gl_softmax) + (0.2 * loss_gl_orthognal) + ((1-args.beta1) * loss_al_softmax) + (0.2 * loss_al_orthognal)\n",
        "\n",
        "        # measure accuracy and record loss\n",
        "        acc1, _ = accuracy(output, target, topk=(1, 5))\n",
        "        acc2, _ = accuracy(out_gl, target, topk=(1, 5))\n",
        "        acc3, _ = accuracy(out_al, target, topk=(1, 5))\n",
        "        losses.update(loss.item(), images.size(0))\n",
        "        top1.update(acc1[0], images.size(0))\n",
        "        top1_1.update(acc2[0], images.size(0))\n",
        "        top1_2.update(acc3[0], images.size(0))\n",
        "\n",
        "        if i % args.print_freq == 0:\n",
        "            progress.display(i, args)\n",
        "\n",
        "\n",
        "      print(' *** Accuracy {top1.avg:.3f}  *** '.format(top1=top1))\n",
        "      with open(args.log_path + '-log.txt', 'a') as f:\n",
        "          f.write(' * Accuracy {top1.avg:.3f}'.format(top1=top1) + '\\n')\n",
        "      with open(os.path.join(args.log_path + '-log_err_out.txt'), 'a') as f:\n",
        "            f.write(' * vail Accuracy,output1: {top1_1.avg:.3f}'.format(top1_1=top1_1) + ' * vail  Accuracy,output2: {top1_2.avg:.3f}'.format(top1_2=top1_2)  +'\\n')\n",
        "\n",
        "\n",
        "    return top1.avg, losses.avg\n",
        "\n",
        "\n",
        "def save_checkpoint(state, is_best, args):\n",
        "    torch.save(state, args.checkpoint_path)\n",
        "    if is_best:\n",
        "        shutil.copyfile(args.checkpoint_path, args.best_checkpoint_path)\n",
        "\n",
        "\n",
        "def orthognal_loss(x, y, z):\n",
        "    x = F.normalize(x, p=2, dim=1)\n",
        "    y = F.normalize(y, p=2, dim=1)\n",
        "    z = F.normalize(z, p=2, dim=1)\n",
        "    l_12 = torch.sum(x*y, dim=1)\n",
        "    l_13 = torch.sum(x*z, dim=1)\n",
        "    l_23 = torch.sum(y*z, dim=1)\n",
        "    return torch.mean((l_12+l_13+l_23)/3, dim=-1)"
      ],
      "metadata": {
        "id": "WUVTXjutLpn8"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "  main()"
      ],
      "metadata": {
        "id": "saA2iP5bMFhW",
        "outputId": "dadc00c5-3e21-4c2a-dd43-6c9c5fa3087d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training time: 10-10 12:06\n",
            "module.conv1.weight torch.Size([64, 3, 7, 7])\n",
            "module.bn1.weight torch.Size([64])\n",
            "module.bn1.bias torch.Size([64])\n",
            "module.bn1.running_mean torch.Size([64])\n",
            "module.bn1.running_var torch.Size([64])\n",
            "module.bn1.num_batches_tracked torch.Size([])\n",
            "module.layer1.0.conv1.weight torch.Size([64, 64, 1, 1])\n",
            "module.layer1.0.bn1.weight torch.Size([64])\n",
            "module.layer1.0.bn1.bias torch.Size([64])\n",
            "module.layer1.0.bn1.running_mean torch.Size([64])\n",
            "module.layer1.0.bn1.running_var torch.Size([64])\n",
            "module.layer1.0.bn1.num_batches_tracked torch.Size([])\n",
            "module.layer1.0.conv2.weight torch.Size([64, 64, 3, 3])\n",
            "module.layer1.0.bn2.weight torch.Size([64])\n",
            "module.layer1.0.bn2.bias torch.Size([64])\n",
            "module.layer1.0.bn2.running_mean torch.Size([64])\n",
            "module.layer1.0.bn2.running_var torch.Size([64])\n",
            "module.layer1.0.bn2.num_batches_tracked torch.Size([])\n",
            "module.layer1.0.conv3.weight torch.Size([256, 64, 1, 1])\n",
            "module.layer1.0.bn3.weight torch.Size([256])\n",
            "module.layer1.0.bn3.bias torch.Size([256])\n",
            "module.layer1.0.bn3.running_mean torch.Size([256])\n",
            "module.layer1.0.bn3.running_var torch.Size([256])\n",
            "module.layer1.0.bn3.num_batches_tracked torch.Size([])\n",
            "module.layer1.0.downsample.0.weight torch.Size([256, 64, 1, 1])\n",
            "module.layer1.0.downsample.1.weight torch.Size([256])\n",
            "module.layer1.0.downsample.1.bias torch.Size([256])\n",
            "module.layer1.0.downsample.1.running_mean torch.Size([256])\n",
            "module.layer1.0.downsample.1.running_var torch.Size([256])\n",
            "module.layer1.0.downsample.1.num_batches_tracked torch.Size([])\n",
            "module.layer1.1.conv1.weight torch.Size([64, 256, 1, 1])\n",
            "module.layer1.1.bn1.weight torch.Size([64])\n",
            "module.layer1.1.bn1.bias torch.Size([64])\n",
            "module.layer1.1.bn1.running_mean torch.Size([64])\n",
            "module.layer1.1.bn1.running_var torch.Size([64])\n",
            "module.layer1.1.bn1.num_batches_tracked torch.Size([])\n",
            "module.layer1.1.conv2.weight torch.Size([64, 64, 3, 3])\n",
            "module.layer1.1.bn2.weight torch.Size([64])\n",
            "module.layer1.1.bn2.bias torch.Size([64])\n",
            "module.layer1.1.bn2.running_mean torch.Size([64])\n",
            "module.layer1.1.bn2.running_var torch.Size([64])\n",
            "module.layer1.1.bn2.num_batches_tracked torch.Size([])\n",
            "module.layer1.1.conv3.weight torch.Size([256, 64, 1, 1])\n",
            "module.layer1.1.bn3.weight torch.Size([256])\n",
            "module.layer1.1.bn3.bias torch.Size([256])\n",
            "module.layer1.1.bn3.running_mean torch.Size([256])\n",
            "module.layer1.1.bn3.running_var torch.Size([256])\n",
            "module.layer1.1.bn3.num_batches_tracked torch.Size([])\n",
            "module.layer1.2.conv1.weight torch.Size([64, 256, 1, 1])\n",
            "module.layer1.2.bn1.weight torch.Size([64])\n",
            "module.layer1.2.bn1.bias torch.Size([64])\n",
            "module.layer1.2.bn1.running_mean torch.Size([64])\n",
            "module.layer1.2.bn1.running_var torch.Size([64])\n",
            "module.layer1.2.bn1.num_batches_tracked torch.Size([])\n",
            "module.layer1.2.conv2.weight torch.Size([64, 64, 3, 3])\n",
            "module.layer1.2.bn2.weight torch.Size([64])\n",
            "module.layer1.2.bn2.bias torch.Size([64])\n",
            "module.layer1.2.bn2.running_mean torch.Size([64])\n",
            "module.layer1.2.bn2.running_var torch.Size([64])\n",
            "module.layer1.2.bn2.num_batches_tracked torch.Size([])\n",
            "module.layer1.2.conv3.weight torch.Size([256, 64, 1, 1])\n",
            "module.layer1.2.bn3.weight torch.Size([256])\n",
            "module.layer1.2.bn3.bias torch.Size([256])\n",
            "module.layer1.2.bn3.running_mean torch.Size([256])\n",
            "module.layer1.2.bn3.running_var torch.Size([256])\n",
            "module.layer1.2.bn3.num_batches_tracked torch.Size([])\n",
            "module.layer2.0.conv1.weight torch.Size([128, 256, 1, 1])\n",
            "module.layer2.0.bn1.weight torch.Size([128])\n",
            "module.layer2.0.bn1.bias torch.Size([128])\n",
            "module.layer2.0.bn1.running_mean torch.Size([128])\n",
            "module.layer2.0.bn1.running_var torch.Size([128])\n",
            "module.layer2.0.bn1.num_batches_tracked torch.Size([])\n",
            "module.layer2.0.conv2.weight torch.Size([128, 128, 3, 3])\n",
            "module.layer2.0.bn2.weight torch.Size([128])\n",
            "module.layer2.0.bn2.bias torch.Size([128])\n",
            "module.layer2.0.bn2.running_mean torch.Size([128])\n",
            "module.layer2.0.bn2.running_var torch.Size([128])\n",
            "module.layer2.0.bn2.num_batches_tracked torch.Size([])\n",
            "module.layer2.0.conv3.weight torch.Size([512, 128, 1, 1])\n",
            "module.layer2.0.bn3.weight torch.Size([512])\n",
            "module.layer2.0.bn3.bias torch.Size([512])\n",
            "module.layer2.0.bn3.running_mean torch.Size([512])\n",
            "module.layer2.0.bn3.running_var torch.Size([512])\n",
            "module.layer2.0.bn3.num_batches_tracked torch.Size([])\n",
            "module.layer2.0.downsample.0.weight torch.Size([512, 256, 1, 1])\n",
            "module.layer2.0.downsample.1.weight torch.Size([512])\n",
            "module.layer2.0.downsample.1.bias torch.Size([512])\n",
            "module.layer2.0.downsample.1.running_mean torch.Size([512])\n",
            "module.layer2.0.downsample.1.running_var torch.Size([512])\n",
            "module.layer2.0.downsample.1.num_batches_tracked torch.Size([])\n",
            "module.layer2.1.conv1.weight torch.Size([128, 512, 1, 1])\n",
            "module.layer2.1.bn1.weight torch.Size([128])\n",
            "module.layer2.1.bn1.bias torch.Size([128])\n",
            "module.layer2.1.bn1.running_mean torch.Size([128])\n",
            "module.layer2.1.bn1.running_var torch.Size([128])\n",
            "module.layer2.1.bn1.num_batches_tracked torch.Size([])\n",
            "module.layer2.1.conv2.weight torch.Size([128, 128, 3, 3])\n",
            "module.layer2.1.bn2.weight torch.Size([128])\n",
            "module.layer2.1.bn2.bias torch.Size([128])\n",
            "module.layer2.1.bn2.running_mean torch.Size([128])\n",
            "module.layer2.1.bn2.running_var torch.Size([128])\n",
            "module.layer2.1.bn2.num_batches_tracked torch.Size([])\n",
            "module.layer2.1.conv3.weight torch.Size([512, 128, 1, 1])\n",
            "module.layer2.1.bn3.weight torch.Size([512])\n",
            "module.layer2.1.bn3.bias torch.Size([512])\n",
            "module.layer2.1.bn3.running_mean torch.Size([512])\n",
            "module.layer2.1.bn3.running_var torch.Size([512])\n",
            "module.layer2.1.bn3.num_batches_tracked torch.Size([])\n",
            "module.layer2.2.conv1.weight torch.Size([128, 512, 1, 1])\n",
            "module.layer2.2.bn1.weight torch.Size([128])\n",
            "module.layer2.2.bn1.bias torch.Size([128])\n",
            "module.layer2.2.bn1.running_mean torch.Size([128])\n",
            "module.layer2.2.bn1.running_var torch.Size([128])\n",
            "module.layer2.2.bn1.num_batches_tracked torch.Size([])\n",
            "module.layer2.2.conv2.weight torch.Size([128, 128, 3, 3])\n",
            "module.layer2.2.bn2.weight torch.Size([128])\n",
            "module.layer2.2.bn2.bias torch.Size([128])\n",
            "module.layer2.2.bn2.running_mean torch.Size([128])\n",
            "module.layer2.2.bn2.running_var torch.Size([128])\n",
            "module.layer2.2.bn2.num_batches_tracked torch.Size([])\n",
            "module.layer2.2.conv3.weight torch.Size([512, 128, 1, 1])\n",
            "module.layer2.2.bn3.weight torch.Size([512])\n",
            "module.layer2.2.bn3.bias torch.Size([512])\n",
            "module.layer2.2.bn3.running_mean torch.Size([512])\n",
            "module.layer2.2.bn3.running_var torch.Size([512])\n",
            "module.layer2.2.bn3.num_batches_tracked torch.Size([])\n",
            "module.layer2.3.conv1.weight torch.Size([128, 512, 1, 1])\n",
            "module.layer2.3.bn1.weight torch.Size([128])\n",
            "module.layer2.3.bn1.bias torch.Size([128])\n",
            "module.layer2.3.bn1.running_mean torch.Size([128])\n",
            "module.layer2.3.bn1.running_var torch.Size([128])\n",
            "module.layer2.3.bn1.num_batches_tracked torch.Size([])\n",
            "module.layer2.3.conv2.weight torch.Size([128, 128, 3, 3])\n",
            "module.layer2.3.bn2.weight torch.Size([128])\n",
            "module.layer2.3.bn2.bias torch.Size([128])\n",
            "module.layer2.3.bn2.running_mean torch.Size([128])\n",
            "module.layer2.3.bn2.running_var torch.Size([128])\n",
            "module.layer2.3.bn2.num_batches_tracked torch.Size([])\n",
            "module.layer2.3.conv3.weight torch.Size([512, 128, 1, 1])\n",
            "module.layer2.3.bn3.weight torch.Size([512])\n",
            "module.layer2.3.bn3.bias torch.Size([512])\n",
            "module.layer2.3.bn3.running_mean torch.Size([512])\n",
            "module.layer2.3.bn3.running_var torch.Size([512])\n",
            "module.layer2.3.bn3.num_batches_tracked torch.Size([])\n",
            "module.layer3.0.conv1.weight torch.Size([256, 512, 1, 1])\n",
            "module.layer3.0.bn1.weight torch.Size([256])\n",
            "module.layer3.0.bn1.bias torch.Size([256])\n",
            "module.layer3.0.bn1.running_mean torch.Size([256])\n",
            "module.layer3.0.bn1.running_var torch.Size([256])\n",
            "module.layer3.0.bn1.num_batches_tracked torch.Size([])\n",
            "module.layer3.0.conv2.weight torch.Size([256, 256, 3, 3])\n",
            "module.layer3.0.bn2.weight torch.Size([256])\n",
            "module.layer3.0.bn2.bias torch.Size([256])\n",
            "module.layer3.0.bn2.running_mean torch.Size([256])\n",
            "module.layer3.0.bn2.running_var torch.Size([256])\n",
            "module.layer3.0.bn2.num_batches_tracked torch.Size([])\n",
            "module.layer3.0.conv3.weight torch.Size([1024, 256, 1, 1])\n",
            "module.layer3.0.bn3.weight torch.Size([1024])\n",
            "module.layer3.0.bn3.bias torch.Size([1024])\n",
            "module.layer3.0.bn3.running_mean torch.Size([1024])\n",
            "module.layer3.0.bn3.running_var torch.Size([1024])\n",
            "module.layer3.0.bn3.num_batches_tracked torch.Size([])\n",
            "module.layer3.0.downsample.0.weight torch.Size([1024, 512, 1, 1])\n",
            "module.layer3.0.downsample.1.weight torch.Size([1024])\n",
            "module.layer3.0.downsample.1.bias torch.Size([1024])\n",
            "module.layer3.0.downsample.1.running_mean torch.Size([1024])\n",
            "module.layer3.0.downsample.1.running_var torch.Size([1024])\n",
            "module.layer3.0.downsample.1.num_batches_tracked torch.Size([])\n",
            "module.layer3.1.conv1.weight torch.Size([256, 1024, 1, 1])\n",
            "module.layer3.1.bn1.weight torch.Size([256])\n",
            "module.layer3.1.bn1.bias torch.Size([256])\n",
            "module.layer3.1.bn1.running_mean torch.Size([256])\n",
            "module.layer3.1.bn1.running_var torch.Size([256])\n",
            "module.layer3.1.bn1.num_batches_tracked torch.Size([])\n",
            "module.layer3.1.conv2.weight torch.Size([256, 256, 3, 3])\n",
            "module.layer3.1.bn2.weight torch.Size([256])\n",
            "module.layer3.1.bn2.bias torch.Size([256])\n",
            "module.layer3.1.bn2.running_mean torch.Size([256])\n",
            "module.layer3.1.bn2.running_var torch.Size([256])\n",
            "module.layer3.1.bn2.num_batches_tracked torch.Size([])\n",
            "module.layer3.1.conv3.weight torch.Size([1024, 256, 1, 1])\n",
            "module.layer3.1.bn3.weight torch.Size([1024])\n",
            "module.layer3.1.bn3.bias torch.Size([1024])\n",
            "module.layer3.1.bn3.running_mean torch.Size([1024])\n",
            "module.layer3.1.bn3.running_var torch.Size([1024])\n",
            "module.layer3.1.bn3.num_batches_tracked torch.Size([])\n",
            "module.layer3.2.conv1.weight torch.Size([256, 1024, 1, 1])\n",
            "module.layer3.2.bn1.weight torch.Size([256])\n",
            "module.layer3.2.bn1.bias torch.Size([256])\n",
            "module.layer3.2.bn1.running_mean torch.Size([256])\n",
            "module.layer3.2.bn1.running_var torch.Size([256])\n",
            "module.layer3.2.bn1.num_batches_tracked torch.Size([])\n",
            "module.layer3.2.conv2.weight torch.Size([256, 256, 3, 3])\n",
            "module.layer3.2.bn2.weight torch.Size([256])\n",
            "module.layer3.2.bn2.bias torch.Size([256])\n",
            "module.layer3.2.bn2.running_mean torch.Size([256])\n",
            "module.layer3.2.bn2.running_var torch.Size([256])\n",
            "module.layer3.2.bn2.num_batches_tracked torch.Size([])\n",
            "module.layer3.2.conv3.weight torch.Size([1024, 256, 1, 1])\n",
            "module.layer3.2.bn3.weight torch.Size([1024])\n",
            "module.layer3.2.bn3.bias torch.Size([1024])\n",
            "module.layer3.2.bn3.running_mean torch.Size([1024])\n",
            "module.layer3.2.bn3.running_var torch.Size([1024])\n",
            "module.layer3.2.bn3.num_batches_tracked torch.Size([])\n",
            "module.layer3.3.conv1.weight torch.Size([256, 1024, 1, 1])\n",
            "module.layer3.3.bn1.weight torch.Size([256])\n",
            "module.layer3.3.bn1.bias torch.Size([256])\n",
            "module.layer3.3.bn1.running_mean torch.Size([256])\n",
            "module.layer3.3.bn1.running_var torch.Size([256])\n",
            "module.layer3.3.bn1.num_batches_tracked torch.Size([])\n",
            "module.layer3.3.conv2.weight torch.Size([256, 256, 3, 3])\n",
            "module.layer3.3.bn2.weight torch.Size([256])\n",
            "module.layer3.3.bn2.bias torch.Size([256])\n",
            "module.layer3.3.bn2.running_mean torch.Size([256])\n",
            "module.layer3.3.bn2.running_var torch.Size([256])\n",
            "module.layer3.3.bn2.num_batches_tracked torch.Size([])\n",
            "module.layer3.3.conv3.weight torch.Size([1024, 256, 1, 1])\n",
            "module.layer3.3.bn3.weight torch.Size([1024])\n",
            "module.layer3.3.bn3.bias torch.Size([1024])\n",
            "module.layer3.3.bn3.running_mean torch.Size([1024])\n",
            "module.layer3.3.bn3.running_var torch.Size([1024])\n",
            "module.layer3.3.bn3.num_batches_tracked torch.Size([])\n",
            "module.layer3.4.conv1.weight torch.Size([256, 1024, 1, 1])\n",
            "module.layer3.4.bn1.weight torch.Size([256])\n",
            "module.layer3.4.bn1.bias torch.Size([256])\n",
            "module.layer3.4.bn1.running_mean torch.Size([256])\n",
            "module.layer3.4.bn1.running_var torch.Size([256])\n",
            "module.layer3.4.bn1.num_batches_tracked torch.Size([])\n",
            "module.layer3.4.conv2.weight torch.Size([256, 256, 3, 3])\n",
            "module.layer3.4.bn2.weight torch.Size([256])\n",
            "module.layer3.4.bn2.bias torch.Size([256])\n",
            "module.layer3.4.bn2.running_mean torch.Size([256])\n",
            "module.layer3.4.bn2.running_var torch.Size([256])\n",
            "module.layer3.4.bn2.num_batches_tracked torch.Size([])\n",
            "module.layer3.4.conv3.weight torch.Size([1024, 256, 1, 1])\n",
            "module.layer3.4.bn3.weight torch.Size([1024])\n",
            "module.layer3.4.bn3.bias torch.Size([1024])\n",
            "module.layer3.4.bn3.running_mean torch.Size([1024])\n",
            "module.layer3.4.bn3.running_var torch.Size([1024])\n",
            "module.layer3.4.bn3.num_batches_tracked torch.Size([])\n",
            "module.layer3.5.conv1.weight torch.Size([256, 1024, 1, 1])\n",
            "module.layer3.5.bn1.weight torch.Size([256])\n",
            "module.layer3.5.bn1.bias torch.Size([256])\n",
            "module.layer3.5.bn1.running_mean torch.Size([256])\n",
            "module.layer3.5.bn1.running_var torch.Size([256])\n",
            "module.layer3.5.bn1.num_batches_tracked torch.Size([])\n",
            "module.layer3.5.conv2.weight torch.Size([256, 256, 3, 3])\n",
            "module.layer3.5.bn2.weight torch.Size([256])\n",
            "module.layer3.5.bn2.bias torch.Size([256])\n",
            "module.layer3.5.bn2.running_mean torch.Size([256])\n",
            "module.layer3.5.bn2.running_var torch.Size([256])\n",
            "module.layer3.5.bn2.num_batches_tracked torch.Size([])\n",
            "module.layer3.5.conv3.weight torch.Size([1024, 256, 1, 1])\n",
            "module.layer3.5.bn3.weight torch.Size([1024])\n",
            "module.layer3.5.bn3.bias torch.Size([1024])\n",
            "module.layer3.5.bn3.running_mean torch.Size([1024])\n",
            "module.layer3.5.bn3.running_var torch.Size([1024])\n",
            "module.layer3.5.bn3.num_batches_tracked torch.Size([])\n",
            "module.layer4.0.conv1.weight torch.Size([512, 1024, 1, 1])\n",
            "module.layer4.0.bn1.weight torch.Size([512])\n",
            "module.layer4.0.bn1.bias torch.Size([512])\n",
            "module.layer4.0.bn1.running_mean torch.Size([512])\n",
            "module.layer4.0.bn1.running_var torch.Size([512])\n",
            "module.layer4.0.bn1.num_batches_tracked torch.Size([])\n",
            "module.layer4.0.conv2.weight torch.Size([512, 512, 3, 3])\n",
            "module.layer4.0.bn2.weight torch.Size([512])\n",
            "module.layer4.0.bn2.bias torch.Size([512])\n",
            "module.layer4.0.bn2.running_mean torch.Size([512])\n",
            "module.layer4.0.bn2.running_var torch.Size([512])\n",
            "module.layer4.0.bn2.num_batches_tracked torch.Size([])\n",
            "module.layer4.0.conv3.weight torch.Size([2048, 512, 1, 1])\n",
            "module.layer4.0.bn3.weight torch.Size([2048])\n",
            "module.layer4.0.bn3.bias torch.Size([2048])\n",
            "module.layer4.0.bn3.running_mean torch.Size([2048])\n",
            "module.layer4.0.bn3.running_var torch.Size([2048])\n",
            "module.layer4.0.bn3.num_batches_tracked torch.Size([])\n",
            "module.layer4.0.downsample.0.weight torch.Size([2048, 1024, 1, 1])\n",
            "module.layer4.0.downsample.1.weight torch.Size([2048])\n",
            "module.layer4.0.downsample.1.bias torch.Size([2048])\n",
            "module.layer4.0.downsample.1.running_mean torch.Size([2048])\n",
            "module.layer4.0.downsample.1.running_var torch.Size([2048])\n",
            "module.layer4.0.downsample.1.num_batches_tracked torch.Size([])\n",
            "module.layer4.1.conv1.weight torch.Size([512, 2048, 1, 1])\n",
            "module.layer4.1.bn1.weight torch.Size([512])\n",
            "module.layer4.1.bn1.bias torch.Size([512])\n",
            "module.layer4.1.bn1.running_mean torch.Size([512])\n",
            "module.layer4.1.bn1.running_var torch.Size([512])\n",
            "module.layer4.1.bn1.num_batches_tracked torch.Size([])\n",
            "module.layer4.1.conv2.weight torch.Size([512, 512, 3, 3])\n",
            "module.layer4.1.bn2.weight torch.Size([512])\n",
            "module.layer4.1.bn2.bias torch.Size([512])\n",
            "module.layer4.1.bn2.running_mean torch.Size([512])\n",
            "module.layer4.1.bn2.running_var torch.Size([512])\n",
            "module.layer4.1.bn2.num_batches_tracked torch.Size([])\n",
            "module.layer4.1.conv3.weight torch.Size([2048, 512, 1, 1])\n",
            "module.layer4.1.bn3.weight torch.Size([2048])\n",
            "module.layer4.1.bn3.bias torch.Size([2048])\n",
            "module.layer4.1.bn3.running_mean torch.Size([2048])\n",
            "module.layer4.1.bn3.running_var torch.Size([2048])\n",
            "module.layer4.1.bn3.num_batches_tracked torch.Size([])\n",
            "module.layer4.2.conv1.weight torch.Size([512, 2048, 1, 1])\n",
            "module.layer4.2.bn1.weight torch.Size([512])\n",
            "module.layer4.2.bn1.bias torch.Size([512])\n",
            "module.layer4.2.bn1.running_mean torch.Size([512])\n",
            "module.layer4.2.bn1.running_var torch.Size([512])\n",
            "module.layer4.2.bn1.num_batches_tracked torch.Size([])\n",
            "module.layer4.2.conv2.weight torch.Size([512, 512, 3, 3])\n",
            "module.layer4.2.bn2.weight torch.Size([512])\n",
            "module.layer4.2.bn2.bias torch.Size([512])\n",
            "module.layer4.2.bn2.running_mean torch.Size([512])\n",
            "module.layer4.2.bn2.running_var torch.Size([512])\n",
            "module.layer4.2.bn2.num_batches_tracked torch.Size([])\n",
            "module.layer4.2.conv3.weight torch.Size([2048, 512, 1, 1])\n",
            "module.layer4.2.bn3.weight torch.Size([2048])\n",
            "module.layer4.2.bn3.bias torch.Size([2048])\n",
            "module.layer4.2.bn3.running_mean torch.Size([2048])\n",
            "module.layer4.2.bn3.running_var torch.Size([2048])\n",
            "module.layer4.2.bn3.num_batches_tracked torch.Size([])\n",
            "module.fc.weight torch.Size([12666, 2048])\n",
            "module.fc.bias torch.Size([12666])\n",
            "Current learning rate:  0.001\n",
            "Epoch: [0][  0/710]\tLoss 3.3400 (3.3400)\tAccuracy 31.250 (31.250)\n",
            "Epoch: [0][ 10/710]\tLoss 4.2452 (3.4328)\tAccuracy 25.000 (28.409)\n",
            "Epoch: [0][ 20/710]\tLoss 2.7342 (3.0559)\tAccuracy 43.750 (33.929)\n",
            "Epoch: [0][ 30/710]\tLoss 2.7988 (2.7427)\tAccuracy 43.750 (40.323)\n",
            "Epoch: [0][ 40/710]\tLoss 1.1165 (2.5798)\tAccuracy 81.250 (45.579)\n",
            "Epoch: [0][ 50/710]\tLoss 1.8039 (2.4841)\tAccuracy 62.500 (48.162)\n",
            "Epoch: [0][ 60/710]\tLoss 2.2869 (2.3717)\tAccuracy 56.250 (49.898)\n",
            "Epoch: [0][ 70/710]\tLoss 2.4376 (2.3133)\tAccuracy 43.750 (51.408)\n",
            "Epoch: [0][ 80/710]\tLoss 2.3193 (2.2703)\tAccuracy 43.750 (52.546)\n",
            "Epoch: [0][ 90/710]\tLoss 1.4440 (2.2053)\tAccuracy 68.750 (53.640)\n",
            "Epoch: [0][100/710]\tLoss 1.4736 (2.1912)\tAccuracy 56.250 (53.837)\n",
            "Epoch: [0][110/710]\tLoss 1.7100 (2.1450)\tAccuracy 68.750 (54.673)\n",
            "Epoch: [0][120/710]\tLoss 1.8553 (2.1053)\tAccuracy 56.250 (55.165)\n",
            "Epoch: [0][130/710]\tLoss 2.2755 (2.0839)\tAccuracy 50.000 (55.677)\n",
            "Epoch: [0][140/710]\tLoss 3.1183 (2.0552)\tAccuracy 37.500 (56.161)\n",
            "Epoch: [0][150/710]\tLoss 1.3044 (2.0157)\tAccuracy 68.750 (57.202)\n",
            "Epoch: [0][160/710]\tLoss 1.9884 (1.9987)\tAccuracy 37.500 (57.065)\n",
            "Epoch: [0][170/710]\tLoss 1.7009 (1.9665)\tAccuracy 56.250 (57.675)\n",
            "Epoch: [0][180/710]\tLoss 1.4635 (1.9451)\tAccuracy 50.000 (58.218)\n",
            "Epoch: [0][190/710]\tLoss 1.1435 (1.9218)\tAccuracy 81.250 (58.704)\n",
            "Epoch: [0][200/710]\tLoss 2.3298 (1.8989)\tAccuracy 56.250 (59.111)\n",
            "Epoch: [0][210/710]\tLoss 1.6832 (1.8937)\tAccuracy 68.750 (59.064)\n",
            "Epoch: [0][220/710]\tLoss 1.6269 (1.8784)\tAccuracy 56.250 (59.333)\n",
            "Epoch: [0][230/710]\tLoss 1.6089 (1.8684)\tAccuracy 62.500 (59.416)\n",
            "Epoch: [0][240/710]\tLoss 1.8078 (1.8582)\tAccuracy 56.250 (59.466)\n",
            "Epoch: [0][250/710]\tLoss 1.5842 (1.8430)\tAccuracy 62.500 (59.836)\n",
            "Epoch: [0][260/710]\tLoss 1.3924 (1.8243)\tAccuracy 81.250 (60.249)\n",
            "Epoch: [0][270/710]\tLoss 1.7038 (1.8000)\tAccuracy 50.000 (60.701)\n",
            "Epoch: [0][280/710]\tLoss 0.9855 (1.7815)\tAccuracy 81.250 (60.921)\n",
            "Epoch: [0][290/710]\tLoss 1.3289 (1.7629)\tAccuracy 68.750 (61.319)\n",
            "Epoch: [0][300/710]\tLoss 1.1129 (1.7511)\tAccuracy 62.500 (61.483)\n",
            "Epoch: [0][310/710]\tLoss 1.3508 (1.7377)\tAccuracy 62.500 (61.736)\n",
            "Epoch: [0][320/710]\tLoss 1.2305 (1.7335)\tAccuracy 68.750 (61.780)\n",
            "Epoch: [0][330/710]\tLoss 1.3103 (1.7137)\tAccuracy 68.750 (62.236)\n",
            "Epoch: [0][340/710]\tLoss 1.3602 (1.7044)\tAccuracy 68.750 (62.335)\n",
            "Epoch: [0][350/710]\tLoss 1.1523 (1.6945)\tAccuracy 68.750 (62.464)\n",
            "Epoch: [0][360/710]\tLoss 1.1177 (1.6825)\tAccuracy 75.000 (62.569)\n",
            "Epoch: [0][370/710]\tLoss 0.8119 (1.6678)\tAccuracy 87.500 (62.904)\n",
            "Epoch: [0][380/710]\tLoss 1.5190 (1.6605)\tAccuracy 50.000 (62.894)\n",
            "Epoch: [0][390/710]\tLoss 0.9497 (1.6488)\tAccuracy 87.500 (63.139)\n",
            "Epoch: [0][400/710]\tLoss 1.1131 (1.6422)\tAccuracy 75.000 (63.279)\n",
            "Epoch: [0][410/710]\tLoss 1.1283 (1.6275)\tAccuracy 75.000 (63.777)\n",
            "Epoch: [0][420/710]\tLoss 0.9880 (1.6159)\tAccuracy 81.250 (63.999)\n",
            "Epoch: [0][430/710]\tLoss 1.9002 (1.6063)\tAccuracy 68.750 (64.197)\n",
            "Epoch: [0][440/710]\tLoss 1.3153 (1.5998)\tAccuracy 62.500 (64.371)\n",
            "Epoch: [0][450/710]\tLoss 1.7657 (1.5938)\tAccuracy 43.750 (64.371)\n",
            "Epoch: [0][460/710]\tLoss 1.3509 (1.5863)\tAccuracy 50.000 (64.439)\n",
            "Epoch: [0][470/710]\tLoss 1.1926 (1.5770)\tAccuracy 62.500 (64.650)\n",
            "Epoch: [0][480/710]\tLoss 1.3093 (1.5724)\tAccuracy 62.500 (64.709)\n",
            "Epoch: [0][490/710]\tLoss 1.3014 (1.5674)\tAccuracy 68.750 (64.779)\n",
            "Epoch: [0][500/710]\tLoss 1.2244 (1.5583)\tAccuracy 62.500 (64.920)\n",
            "Epoch: [0][510/710]\tLoss 1.2010 (1.5513)\tAccuracy 68.750 (65.056)\n",
            "Epoch: [0][520/710]\tLoss 0.9513 (1.5412)\tAccuracy 62.500 (65.211)\n",
            "Epoch: [0][530/710]\tLoss 1.3464 (1.5339)\tAccuracy 81.250 (65.431)\n",
            "Epoch: [0][540/710]\tLoss 1.1752 (1.5252)\tAccuracy 75.000 (65.677)\n",
            "Epoch: [0][550/710]\tLoss 1.0803 (1.5171)\tAccuracy 75.000 (65.869)\n",
            "Epoch: [0][560/710]\tLoss 1.3311 (1.5087)\tAccuracy 62.500 (65.987)\n",
            "Epoch: [0][570/710]\tLoss 1.1342 (1.5037)\tAccuracy 68.750 (66.014)\n",
            "Epoch: [0][580/710]\tLoss 1.0801 (1.4952)\tAccuracy 68.750 (66.244)\n",
            "Epoch: [0][590/710]\tLoss 2.2411 (1.4885)\tAccuracy 18.750 (66.349)\n",
            "Epoch: [0][600/710]\tLoss 0.7169 (1.4813)\tAccuracy 87.500 (66.504)\n",
            "Epoch: [0][610/710]\tLoss 0.7377 (1.4735)\tAccuracy 100.000 (66.725)\n",
            "Epoch: [0][620/710]\tLoss 1.0937 (1.4693)\tAccuracy 68.750 (66.828)\n",
            "Epoch: [0][630/710]\tLoss 1.1403 (1.4629)\tAccuracy 62.500 (66.957)\n",
            "Epoch: [0][640/710]\tLoss 1.3632 (1.4586)\tAccuracy 75.000 (67.044)\n",
            "Epoch: [0][650/710]\tLoss 0.6135 (1.4521)\tAccuracy 93.750 (67.185)\n",
            "Epoch: [0][660/710]\tLoss 1.1824 (1.4466)\tAccuracy 56.250 (67.303)\n",
            "Epoch: [0][670/710]\tLoss 0.9837 (1.4399)\tAccuracy 81.250 (67.437)\n",
            "Epoch: [0][680/710]\tLoss 1.1273 (1.4330)\tAccuracy 68.750 (67.603)\n",
            "Epoch: [0][690/710]\tLoss 0.9393 (1.4279)\tAccuracy 62.500 (67.728)\n",
            "Epoch: [0][700/710]\tLoss 1.1197 (1.4228)\tAccuracy 75.000 (67.823)\n",
            "Test: [  0/177]\tLoss 0.5549 (0.5549)\tAccuracy 100.000 (100.000)\n",
            "Test: [ 10/177]\tLoss 1.2555 (0.9966)\tAccuracy 75.000 (76.136)\n",
            "Test: [ 20/177]\tLoss 1.3652 (1.0165)\tAccuracy 56.250 (76.786)\n",
            "Test: [ 30/177]\tLoss 1.2261 (1.0262)\tAccuracy 81.250 (76.613)\n",
            "Test: [ 40/177]\tLoss 0.8661 (1.0050)\tAccuracy 81.250 (76.524)\n",
            "Test: [ 50/177]\tLoss 1.1532 (1.0233)\tAccuracy 81.250 (77.328)\n",
            "Test: [ 60/177]\tLoss 0.8288 (0.9880)\tAccuracy 87.500 (78.484)\n",
            "Test: [ 70/177]\tLoss 1.1177 (0.9861)\tAccuracy 68.750 (78.257)\n",
            "Test: [ 80/177]\tLoss 1.0178 (0.9933)\tAccuracy 87.500 (77.932)\n",
            "Test: [ 90/177]\tLoss 0.7153 (0.9868)\tAccuracy 87.500 (78.022)\n",
            "Test: [100/177]\tLoss 1.2474 (0.9750)\tAccuracy 56.250 (78.156)\n",
            "Test: [110/177]\tLoss 0.7166 (0.9685)\tAccuracy 81.250 (78.153)\n",
            "Test: [120/177]\tLoss 0.5793 (0.9603)\tAccuracy 100.000 (78.822)\n",
            "Test: [130/177]\tLoss 1.0117 (0.9611)\tAccuracy 75.000 (78.721)\n",
            "Test: [140/177]\tLoss 0.8717 (0.9586)\tAccuracy 75.000 (78.989)\n",
            "Test: [150/177]\tLoss 0.9354 (0.9589)\tAccuracy 81.250 (79.015)\n",
            "Test: [160/177]\tLoss 0.8712 (0.9607)\tAccuracy 75.000 (78.843)\n",
            "Test: [170/177]\tLoss 0.8929 (0.9646)\tAccuracy 81.250 (78.838)\n",
            " *** Accuracy 78.811  *** \n",
            "Saved figure\n",
            "Current best accuracy:  78.81146240234375\n",
            "An Epoch Time:  219.15848064422607\n",
            "Current learning rate:  0.001\n",
            "Epoch: [1][  0/710]\tLoss 1.0077 (1.0077)\tAccuracy 81.250 (81.250)\n",
            "Epoch: [1][ 10/710]\tLoss 0.6862 (1.0400)\tAccuracy 87.500 (77.273)\n",
            "Epoch: [1][ 20/710]\tLoss 0.8592 (1.0036)\tAccuracy 81.250 (77.083)\n",
            "Epoch: [1][ 30/710]\tLoss 1.2702 (0.9732)\tAccuracy 81.250 (78.629)\n",
            "Epoch: [1][ 40/710]\tLoss 1.6062 (0.9624)\tAccuracy 43.750 (77.896)\n",
            "Epoch: [1][ 50/710]\tLoss 0.9273 (0.9642)\tAccuracy 75.000 (78.186)\n",
            "Epoch: [1][ 60/710]\tLoss 1.1783 (0.9718)\tAccuracy 75.000 (77.357)\n",
            "Epoch: [1][ 70/710]\tLoss 0.8814 (0.9657)\tAccuracy 93.750 (77.025)\n",
            "Epoch: [1][ 80/710]\tLoss 1.0863 (0.9736)\tAccuracy 75.000 (76.775)\n",
            "Epoch: [1][ 90/710]\tLoss 0.8783 (0.9579)\tAccuracy 81.250 (77.679)\n",
            "Epoch: [1][100/710]\tLoss 0.7803 (0.9492)\tAccuracy 87.500 (77.908)\n",
            "Epoch: [1][110/710]\tLoss 0.9657 (0.9577)\tAccuracy 81.250 (77.759)\n",
            "Epoch: [1][120/710]\tLoss 0.7503 (0.9545)\tAccuracy 93.750 (77.893)\n",
            "Epoch: [1][130/710]\tLoss 0.8463 (0.9474)\tAccuracy 81.250 (78.149)\n",
            "Epoch: [1][140/710]\tLoss 0.8897 (0.9423)\tAccuracy 87.500 (78.324)\n",
            "Epoch: [1][150/710]\tLoss 1.2586 (0.9488)\tAccuracy 50.000 (78.104)\n",
            "Epoch: [1][160/710]\tLoss 0.9469 (0.9510)\tAccuracy 81.250 (78.106)\n",
            "Epoch: [1][170/710]\tLoss 1.0847 (0.9483)\tAccuracy 62.500 (78.180)\n",
            "Epoch: [1][180/710]\tLoss 0.9361 (0.9431)\tAccuracy 75.000 (78.315)\n",
            "Epoch: [1][190/710]\tLoss 0.7655 (0.9435)\tAccuracy 87.500 (78.207)\n",
            "Epoch: [1][200/710]\tLoss 1.2293 (0.9410)\tAccuracy 62.500 (78.327)\n",
            "Epoch: [1][210/710]\tLoss 0.9162 (0.9393)\tAccuracy 87.500 (78.525)\n",
            "Epoch: [1][220/710]\tLoss 0.8461 (0.9380)\tAccuracy 68.750 (78.422)\n",
            "Epoch: [1][230/710]\tLoss 0.8843 (0.9342)\tAccuracy 81.250 (78.544)\n",
            "Epoch: [1][240/710]\tLoss 1.1805 (0.9320)\tAccuracy 68.750 (78.631)\n",
            "Epoch: [1][250/710]\tLoss 0.8142 (0.9296)\tAccuracy 75.000 (78.611)\n",
            "Epoch: [1][260/710]\tLoss 1.0546 (0.9297)\tAccuracy 75.000 (78.760)\n",
            "Epoch: [1][270/710]\tLoss 1.4716 (0.9304)\tAccuracy 62.500 (78.644)\n",
            "Epoch: [1][280/710]\tLoss 1.6263 (0.9316)\tAccuracy 68.750 (78.648)\n",
            "Epoch: [1][290/710]\tLoss 0.8126 (0.9282)\tAccuracy 81.250 (78.673)\n",
            "Epoch: [1][300/710]\tLoss 0.9139 (0.9290)\tAccuracy 81.250 (78.592)\n",
            "Epoch: [1][310/710]\tLoss 0.9003 (0.9265)\tAccuracy 87.500 (78.738)\n",
            "Epoch: [1][320/710]\tLoss 0.5049 (0.9225)\tAccuracy 93.750 (78.894)\n",
            "Epoch: [1][330/710]\tLoss 0.9147 (0.9196)\tAccuracy 75.000 (79.060)\n",
            "Epoch: [1][340/710]\tLoss 0.6278 (0.9170)\tAccuracy 93.750 (79.087)\n",
            "Epoch: [1][350/710]\tLoss 0.7968 (0.9126)\tAccuracy 87.500 (79.256)\n",
            "Epoch: [1][360/710]\tLoss 0.7816 (0.9158)\tAccuracy 93.750 (79.224)\n",
            "Epoch: [1][370/710]\tLoss 0.8833 (0.9160)\tAccuracy 87.500 (79.279)\n",
            "Epoch: [1][380/710]\tLoss 0.6120 (0.9138)\tAccuracy 93.750 (79.380)\n",
            "Epoch: [1][390/710]\tLoss 0.6786 (0.9094)\tAccuracy 87.500 (79.540)\n",
            "Epoch: [1][400/710]\tLoss 0.8563 (0.9083)\tAccuracy 81.250 (79.567)\n",
            "Epoch: [1][410/710]\tLoss 0.7089 (0.9046)\tAccuracy 81.250 (79.653)\n",
            "Epoch: [1][420/710]\tLoss 0.9568 (0.9044)\tAccuracy 68.750 (79.632)\n",
            "Epoch: [1][430/710]\tLoss 0.9480 (0.9048)\tAccuracy 75.000 (79.495)\n",
            "Epoch: [1][440/710]\tLoss 0.4716 (0.9061)\tAccuracy 87.500 (79.464)\n",
            "Epoch: [1][450/710]\tLoss 1.2253 (0.9052)\tAccuracy 68.750 (79.435)\n",
            "Epoch: [1][460/710]\tLoss 0.9562 (0.9015)\tAccuracy 75.000 (79.582)\n",
            "Epoch: [1][470/710]\tLoss 0.6988 (0.9012)\tAccuracy 87.500 (79.591)\n",
            "Epoch: [1][480/710]\tLoss 0.7385 (0.8988)\tAccuracy 87.500 (79.691)\n",
            "Epoch: [1][490/710]\tLoss 0.9410 (0.8980)\tAccuracy 75.000 (79.608)\n",
            "Epoch: [1][500/710]\tLoss 0.9837 (0.8985)\tAccuracy 62.500 (79.566)\n",
            "Epoch: [1][510/710]\tLoss 0.8594 (0.8986)\tAccuracy 81.250 (79.574)\n",
            "Epoch: [1][520/710]\tLoss 0.8494 (0.8990)\tAccuracy 87.500 (79.523)\n",
            "Epoch: [1][530/710]\tLoss 0.7893 (0.8980)\tAccuracy 81.250 (79.496)\n",
            "Epoch: [1][540/710]\tLoss 0.9704 (0.8958)\tAccuracy 75.000 (79.459)\n",
            "Epoch: [1][550/710]\tLoss 0.5067 (0.8927)\tAccuracy 93.750 (79.583)\n",
            "Epoch: [1][560/710]\tLoss 0.7475 (0.8929)\tAccuracy 87.500 (79.635)\n",
            "Epoch: [1][570/710]\tLoss 0.6267 (0.8934)\tAccuracy 93.750 (79.597)\n",
            "Epoch: [1][580/710]\tLoss 0.6935 (0.8944)\tAccuracy 87.500 (79.550)\n",
            "Epoch: [1][590/710]\tLoss 0.9882 (0.8939)\tAccuracy 75.000 (79.537)\n",
            "Epoch: [1][600/710]\tLoss 1.0533 (0.8930)\tAccuracy 68.750 (79.524)\n",
            "Epoch: [1][610/710]\tLoss 0.8185 (0.8918)\tAccuracy 75.000 (79.511)\n",
            "Epoch: [1][620/710]\tLoss 0.7494 (0.8893)\tAccuracy 87.500 (79.610)\n",
            "Epoch: [1][630/710]\tLoss 0.9573 (0.8871)\tAccuracy 68.750 (79.645)\n",
            "Epoch: [1][640/710]\tLoss 1.1850 (0.8861)\tAccuracy 56.250 (79.661)\n",
            "Epoch: [1][650/710]\tLoss 1.3401 (0.8884)\tAccuracy 68.750 (79.637)\n",
            "Epoch: [1][660/710]\tLoss 0.8274 (0.8873)\tAccuracy 75.000 (79.586)\n",
            "Epoch: [1][670/710]\tLoss 0.5916 (0.8861)\tAccuracy 87.500 (79.583)\n",
            "Epoch: [1][680/710]\tLoss 0.8673 (0.8861)\tAccuracy 75.000 (79.534)\n",
            "Epoch: [1][690/710]\tLoss 0.7882 (0.8851)\tAccuracy 87.500 (79.595)\n",
            "Epoch: [1][700/710]\tLoss 0.5856 (0.8835)\tAccuracy 100.000 (79.645)\n",
            "Test: [  0/177]\tLoss 0.7812 (0.7812)\tAccuracy 81.250 (81.250)\n",
            "Test: [ 10/177]\tLoss 0.5976 (0.7911)\tAccuracy 87.500 (82.386)\n",
            "Test: [ 20/177]\tLoss 0.6974 (0.8219)\tAccuracy 87.500 (80.357)\n",
            "Test: [ 30/177]\tLoss 0.7794 (0.8148)\tAccuracy 87.500 (81.250)\n",
            "Test: [ 40/177]\tLoss 0.7497 (0.7948)\tAccuracy 87.500 (81.860)\n",
            "Test: [ 50/177]\tLoss 0.8935 (0.8117)\tAccuracy 68.750 (81.005)\n",
            "Test: [ 60/177]\tLoss 0.8531 (0.8058)\tAccuracy 75.000 (81.045)\n",
            "Test: [ 70/177]\tLoss 0.6400 (0.7984)\tAccuracy 93.750 (81.778)\n",
            "Test: [ 80/177]\tLoss 0.6130 (0.7917)\tAccuracy 81.250 (81.713)\n",
            "Test: [ 90/177]\tLoss 0.8046 (0.7833)\tAccuracy 93.750 (82.143)\n",
            "Test: [100/177]\tLoss 0.6781 (0.7894)\tAccuracy 87.500 (81.869)\n",
            "Test: [110/177]\tLoss 0.7616 (0.7899)\tAccuracy 87.500 (82.207)\n",
            "Test: [120/177]\tLoss 0.6760 (0.7985)\tAccuracy 81.250 (82.076)\n",
            "Test: [130/177]\tLoss 1.1376 (0.7984)\tAccuracy 68.750 (82.347)\n",
            "Test: [140/177]\tLoss 1.0540 (0.8048)\tAccuracy 81.250 (82.137)\n",
            "Test: [150/177]\tLoss 0.7371 (0.8090)\tAccuracy 87.500 (81.871)\n",
            "Test: [160/177]\tLoss 1.0097 (0.8175)\tAccuracy 87.500 (81.561)\n",
            "Test: [170/177]\tLoss 0.8473 (0.8123)\tAccuracy 81.250 (81.689)\n",
            " *** Accuracy 81.535  *** \n",
            "Saved figure\n",
            "Current best accuracy:  81.53520202636719\n",
            "An Epoch Time:  199.4897882938385\n",
            "Current learning rate:  0.001\n",
            "Epoch: [2][  0/710]\tLoss 0.6571 (0.6571)\tAccuracy 100.000 (100.000)\n",
            "Epoch: [2][ 10/710]\tLoss 0.6204 (0.7154)\tAccuracy 75.000 (80.682)\n",
            "Epoch: [2][ 20/710]\tLoss 0.6760 (0.7397)\tAccuracy 87.500 (80.952)\n",
            "Epoch: [2][ 30/710]\tLoss 0.9239 (0.7376)\tAccuracy 75.000 (81.855)\n",
            "Epoch: [2][ 40/710]\tLoss 0.6249 (0.7476)\tAccuracy 87.500 (82.165)\n",
            "Epoch: [2][ 50/710]\tLoss 0.8666 (0.7512)\tAccuracy 75.000 (82.598)\n",
            "Epoch: [2][ 60/710]\tLoss 0.7744 (0.7702)\tAccuracy 75.000 (82.172)\n",
            "Epoch: [2][ 70/710]\tLoss 0.5369 (0.7740)\tAccuracy 87.500 (82.218)\n",
            "Epoch: [2][ 80/710]\tLoss 0.9328 (0.7757)\tAccuracy 87.500 (82.407)\n",
            "Epoch: [2][ 90/710]\tLoss 0.7715 (0.7678)\tAccuracy 87.500 (82.418)\n",
            "Epoch: [2][100/710]\tLoss 0.6526 (0.7637)\tAccuracy 87.500 (82.488)\n",
            "Epoch: [2][110/710]\tLoss 0.6561 (0.7537)\tAccuracy 81.250 (82.770)\n",
            "Epoch: [2][120/710]\tLoss 1.0108 (0.7647)\tAccuracy 81.250 (82.231)\n",
            "Epoch: [2][130/710]\tLoss 0.5066 (0.7561)\tAccuracy 87.500 (82.443)\n",
            "Epoch: [2][140/710]\tLoss 0.9739 (0.7460)\tAccuracy 75.000 (82.668)\n",
            "Epoch: [2][150/710]\tLoss 0.7304 (0.7455)\tAccuracy 75.000 (82.450)\n",
            "Epoch: [2][160/710]\tLoss 0.8876 (0.7480)\tAccuracy 75.000 (82.220)\n",
            "Epoch: [2][170/710]\tLoss 0.3304 (0.7454)\tAccuracy 100.000 (82.383)\n",
            "Epoch: [2][180/710]\tLoss 0.9810 (0.7561)\tAccuracy 75.000 (82.010)\n",
            "Epoch: [2][190/710]\tLoss 0.6560 (0.7559)\tAccuracy 87.500 (82.003)\n",
            "Epoch: [2][200/710]\tLoss 0.4975 (0.7501)\tAccuracy 87.500 (82.121)\n",
            "Epoch: [2][210/710]\tLoss 0.6206 (0.7487)\tAccuracy 75.000 (82.168)\n",
            "Epoch: [2][220/710]\tLoss 0.9037 (0.7529)\tAccuracy 75.000 (81.985)\n",
            "Epoch: [2][230/710]\tLoss 0.7133 (0.7504)\tAccuracy 81.250 (82.143)\n",
            "Epoch: [2][240/710]\tLoss 0.5859 (0.7550)\tAccuracy 87.500 (81.872)\n",
            "Epoch: [2][250/710]\tLoss 0.7127 (0.7543)\tAccuracy 87.500 (81.798)\n",
            "Epoch: [2][260/710]\tLoss 0.9694 (0.7582)\tAccuracy 75.000 (81.633)\n",
            "Epoch: [2][270/710]\tLoss 0.8922 (0.7610)\tAccuracy 81.250 (81.435)\n",
            "Epoch: [2][280/710]\tLoss 0.7936 (0.7611)\tAccuracy 81.250 (81.428)\n",
            "Epoch: [2][290/710]\tLoss 0.8474 (0.7614)\tAccuracy 87.500 (81.486)\n",
            "Epoch: [2][300/710]\tLoss 0.6594 (0.7594)\tAccuracy 87.500 (81.645)\n",
            "Epoch: [2][310/710]\tLoss 0.6855 (0.7562)\tAccuracy 81.250 (81.652)\n",
            "Epoch: [2][320/710]\tLoss 1.0553 (0.7536)\tAccuracy 81.250 (81.717)\n",
            "Epoch: [2][330/710]\tLoss 0.8151 (0.7523)\tAccuracy 75.000 (81.741)\n",
            "Epoch: [2][340/710]\tLoss 0.5644 (0.7510)\tAccuracy 87.500 (81.818)\n",
            "Epoch: [2][350/710]\tLoss 0.5981 (0.7477)\tAccuracy 81.250 (81.944)\n",
            "Epoch: [2][360/710]\tLoss 0.4305 (0.7457)\tAccuracy 93.750 (81.977)\n",
            "Epoch: [2][370/710]\tLoss 1.1382 (0.7515)\tAccuracy 68.750 (81.806)\n",
            "Epoch: [2][380/710]\tLoss 0.4846 (0.7507)\tAccuracy 81.250 (81.759)\n",
            "Epoch: [2][390/710]\tLoss 0.7708 (0.7529)\tAccuracy 75.000 (81.602)\n",
            "Epoch: [2][400/710]\tLoss 0.9112 (0.7539)\tAccuracy 81.250 (81.593)\n",
            "Epoch: [2][410/710]\tLoss 0.4806 (0.7533)\tAccuracy 87.500 (81.524)\n",
            "Epoch: [2][420/710]\tLoss 0.5666 (0.7535)\tAccuracy 81.250 (81.532)\n",
            "Epoch: [2][430/710]\tLoss 0.6234 (0.7516)\tAccuracy 93.750 (81.584)\n",
            "Epoch: [2][440/710]\tLoss 0.4932 (0.7478)\tAccuracy 81.250 (81.704)\n",
            "Epoch: [2][450/710]\tLoss 0.7793 (0.7464)\tAccuracy 81.250 (81.707)\n",
            "Epoch: [2][460/710]\tLoss 0.8253 (0.7461)\tAccuracy 87.500 (81.779)\n",
            "Epoch: [2][470/710]\tLoss 0.7077 (0.7449)\tAccuracy 81.250 (81.741)\n",
            "Epoch: [2][480/710]\tLoss 0.7696 (0.7456)\tAccuracy 81.250 (81.718)\n",
            "Epoch: [2][490/710]\tLoss 0.8462 (0.7440)\tAccuracy 68.750 (81.734)\n",
            "Epoch: [2][500/710]\tLoss 0.6575 (0.7427)\tAccuracy 81.250 (81.712)\n",
            "Epoch: [2][510/710]\tLoss 0.7315 (0.7416)\tAccuracy 87.500 (81.776)\n",
            "Epoch: [2][520/710]\tLoss 1.2135 (0.7448)\tAccuracy 62.500 (81.706)\n",
            "Epoch: [2][530/710]\tLoss 0.7626 (0.7436)\tAccuracy 75.000 (81.733)\n",
            "Epoch: [2][540/710]\tLoss 0.8846 (0.7447)\tAccuracy 81.250 (81.643)\n",
            "Epoch: [2][550/710]\tLoss 0.8659 (0.7462)\tAccuracy 68.750 (81.534)\n",
            "Epoch: [2][560/710]\tLoss 0.6813 (0.7458)\tAccuracy 87.500 (81.562)\n",
            "Epoch: [2][570/710]\tLoss 0.8473 (0.7435)\tAccuracy 62.500 (81.578)\n",
            "Epoch: [2][580/710]\tLoss 0.4757 (0.7422)\tAccuracy 93.750 (81.627)\n",
            "Epoch: [2][590/710]\tLoss 0.4016 (0.7391)\tAccuracy 93.750 (81.726)\n",
            "Epoch: [2][600/710]\tLoss 0.5039 (0.7362)\tAccuracy 87.500 (81.801)\n",
            "Epoch: [2][610/710]\tLoss 0.9413 (0.7339)\tAccuracy 75.000 (81.864)\n",
            "Epoch: [2][620/710]\tLoss 1.1151 (0.7349)\tAccuracy 68.750 (81.804)\n",
            "Epoch: [2][630/710]\tLoss 0.6291 (0.7335)\tAccuracy 81.250 (81.864)\n",
            "Epoch: [2][640/710]\tLoss 0.5961 (0.7328)\tAccuracy 81.250 (81.903)\n",
            "Epoch: [2][650/710]\tLoss 0.6319 (0.7322)\tAccuracy 87.500 (81.884)\n",
            "Epoch: [2][660/710]\tLoss 0.7940 (0.7321)\tAccuracy 81.250 (81.921)\n",
            "Epoch: [2][670/710]\tLoss 0.6556 (0.7303)\tAccuracy 81.250 (82.004)\n",
            "Epoch: [2][680/710]\tLoss 0.9276 (0.7311)\tAccuracy 75.000 (81.975)\n",
            "Epoch: [2][690/710]\tLoss 0.6788 (0.7307)\tAccuracy 87.500 (82.010)\n",
            "Epoch: [2][700/710]\tLoss 0.5423 (0.7288)\tAccuracy 87.500 (82.044)\n",
            "Test: [  0/177]\tLoss 0.5816 (0.5816)\tAccuracy 81.250 (81.250)\n",
            "Test: [ 10/177]\tLoss 0.6861 (0.5939)\tAccuracy 81.250 (86.364)\n",
            "Test: [ 20/177]\tLoss 0.5401 (0.6372)\tAccuracy 87.500 (84.821)\n",
            "Test: [ 30/177]\tLoss 0.7609 (0.6505)\tAccuracy 81.250 (85.484)\n",
            "Test: [ 40/177]\tLoss 0.5537 (0.6499)\tAccuracy 87.500 (85.366)\n",
            "Test: [ 50/177]\tLoss 0.3871 (0.6599)\tAccuracy 100.000 (85.539)\n",
            "Test: [ 60/177]\tLoss 0.5950 (0.6815)\tAccuracy 87.500 (84.631)\n",
            "Test: [ 70/177]\tLoss 0.5002 (0.6751)\tAccuracy 87.500 (84.859)\n",
            "Test: [ 80/177]\tLoss 0.6434 (0.6622)\tAccuracy 87.500 (85.031)\n",
            "Test: [ 90/177]\tLoss 0.8482 (0.6708)\tAccuracy 68.750 (85.027)\n",
            "Test: [100/177]\tLoss 0.6747 (0.6652)\tAccuracy 87.500 (85.025)\n",
            "Test: [110/177]\tLoss 0.4503 (0.6664)\tAccuracy 93.750 (84.910)\n",
            "Test: [120/177]\tLoss 0.6406 (0.6657)\tAccuracy 81.250 (84.659)\n",
            "Test: [130/177]\tLoss 0.5070 (0.6742)\tAccuracy 93.750 (84.303)\n",
            "Test: [140/177]\tLoss 0.6204 (0.6656)\tAccuracy 93.750 (84.441)\n",
            "Test: [150/177]\tLoss 0.6452 (0.6696)\tAccuracy 81.250 (84.478)\n",
            "Test: [160/177]\tLoss 0.4843 (0.6712)\tAccuracy 87.500 (84.472)\n",
            "Test: [170/177]\tLoss 0.6640 (0.6668)\tAccuracy 81.250 (84.613)\n",
            " *** Accuracy 84.613  *** \n",
            "Saved figure\n",
            "Current best accuracy:  84.61266326904297\n",
            "An Epoch Time:  201.15039110183716\n",
            "Current learning rate:  0.001\n",
            "Epoch: [3][  0/710]\tLoss 0.4230 (0.4230)\tAccuracy 100.000 (100.000)\n",
            "Epoch: [3][ 10/710]\tLoss 0.5065 (0.5807)\tAccuracy 93.750 (85.795)\n",
            "Epoch: [3][ 20/710]\tLoss 0.3592 (0.5812)\tAccuracy 93.750 (86.905)\n",
            "Epoch: [3][ 30/710]\tLoss 0.6705 (0.5808)\tAccuracy 93.750 (86.694)\n",
            "Epoch: [3][ 40/710]\tLoss 0.5687 (0.5930)\tAccuracy 87.500 (86.738)\n",
            "Epoch: [3][ 50/710]\tLoss 0.9526 (0.6499)\tAccuracy 87.500 (85.907)\n",
            "Epoch: [3][ 60/710]\tLoss 0.6556 (0.6507)\tAccuracy 93.750 (85.451)\n",
            "Epoch: [3][ 70/710]\tLoss 0.6576 (0.6374)\tAccuracy 87.500 (85.739)\n",
            "Epoch: [3][ 80/710]\tLoss 1.0043 (0.6423)\tAccuracy 68.750 (85.571)\n",
            "Epoch: [3][ 90/710]\tLoss 0.7278 (0.6346)\tAccuracy 75.000 (85.783)\n",
            "Epoch: [3][100/710]\tLoss 0.5436 (0.6401)\tAccuracy 93.750 (85.334)\n",
            "Epoch: [3][110/710]\tLoss 0.7330 (0.6331)\tAccuracy 81.250 (85.642)\n",
            "Epoch: [3][120/710]\tLoss 0.9251 (0.6333)\tAccuracy 81.250 (85.537)\n",
            "Epoch: [3][130/710]\tLoss 0.6970 (0.6376)\tAccuracy 81.250 (85.258)\n",
            "Epoch: [3][140/710]\tLoss 0.3852 (0.6427)\tAccuracy 87.500 (85.062)\n",
            "Epoch: [3][150/710]\tLoss 0.8932 (0.6428)\tAccuracy 75.000 (85.182)\n",
            "Epoch: [3][160/710]\tLoss 0.6910 (0.6359)\tAccuracy 81.250 (85.365)\n",
            "Epoch: [3][170/710]\tLoss 0.5092 (0.6336)\tAccuracy 87.500 (85.417)\n",
            "Epoch: [3][180/710]\tLoss 0.9376 (0.6372)\tAccuracy 75.000 (85.325)\n",
            "Epoch: [3][190/710]\tLoss 0.8454 (0.6465)\tAccuracy 68.750 (85.079)\n",
            "Epoch: [3][200/710]\tLoss 0.5446 (0.6450)\tAccuracy 100.000 (85.137)\n",
            "Epoch: [3][210/710]\tLoss 0.4122 (0.6489)\tAccuracy 100.000 (85.041)\n",
            "Epoch: [3][220/710]\tLoss 0.4836 (0.6460)\tAccuracy 93.750 (85.124)\n",
            "Epoch: [3][230/710]\tLoss 0.8397 (0.6438)\tAccuracy 68.750 (85.092)\n",
            "Epoch: [3][240/710]\tLoss 0.5283 (0.6437)\tAccuracy 93.750 (85.166)\n",
            "Epoch: [3][250/710]\tLoss 0.5881 (0.6444)\tAccuracy 93.750 (85.159)\n",
            "Epoch: [3][260/710]\tLoss 0.9929 (0.6414)\tAccuracy 68.750 (85.225)\n",
            "Epoch: [3][270/710]\tLoss 1.3670 (0.6464)\tAccuracy 50.000 (84.917)\n",
            "Epoch: [3][280/710]\tLoss 0.5531 (0.6425)\tAccuracy 100.000 (85.053)\n",
            "Epoch: [3][290/710]\tLoss 0.8311 (0.6434)\tAccuracy 75.000 (84.901)\n",
            "Epoch: [3][300/710]\tLoss 0.5163 (0.6429)\tAccuracy 87.500 (84.884)\n",
            "Epoch: [3][310/710]\tLoss 0.6602 (0.6437)\tAccuracy 81.250 (84.707)\n",
            "Epoch: [3][320/710]\tLoss 0.8669 (0.6423)\tAccuracy 62.500 (84.599)\n",
            "Epoch: [3][330/710]\tLoss 0.9353 (0.6425)\tAccuracy 81.250 (84.611)\n",
            "Epoch: [3][340/710]\tLoss 0.2837 (0.6412)\tAccuracy 93.750 (84.586)\n",
            "Epoch: [3][350/710]\tLoss 0.6104 (0.6407)\tAccuracy 81.250 (84.580)\n",
            "Epoch: [3][360/710]\tLoss 0.9087 (0.6441)\tAccuracy 75.000 (84.418)\n",
            "Epoch: [3][370/710]\tLoss 0.5362 (0.6424)\tAccuracy 81.250 (84.417)\n",
            "Epoch: [3][380/710]\tLoss 0.7837 (0.6402)\tAccuracy 81.250 (84.482)\n",
            "Epoch: [3][390/710]\tLoss 0.7019 (0.6379)\tAccuracy 81.250 (84.623)\n",
            "Epoch: [3][400/710]\tLoss 0.8466 (0.6376)\tAccuracy 68.750 (84.554)\n",
            "Epoch: [3][410/710]\tLoss 0.7825 (0.6402)\tAccuracy 75.000 (84.443)\n",
            "Epoch: [3][420/710]\tLoss 0.3942 (0.6389)\tAccuracy 93.750 (84.442)\n",
            "Epoch: [3][430/710]\tLoss 0.3228 (0.6389)\tAccuracy 93.750 (84.498)\n",
            "Epoch: [3][440/710]\tLoss 0.7774 (0.6382)\tAccuracy 75.000 (84.481)\n",
            "Epoch: [3][450/710]\tLoss 0.2451 (0.6370)\tAccuracy 100.000 (84.507)\n",
            "Epoch: [3][460/710]\tLoss 1.2298 (0.6389)\tAccuracy 68.750 (84.490)\n",
            "Epoch: [3][470/710]\tLoss 0.6513 (0.6365)\tAccuracy 87.500 (84.501)\n",
            "Epoch: [3][480/710]\tLoss 0.6502 (0.6375)\tAccuracy 93.750 (84.524)\n",
            "Epoch: [3][490/710]\tLoss 0.6073 (0.6386)\tAccuracy 87.500 (84.483)\n",
            "Epoch: [3][500/710]\tLoss 0.3966 (0.6379)\tAccuracy 81.250 (84.481)\n",
            "Epoch: [3][510/710]\tLoss 0.8430 (0.6380)\tAccuracy 81.250 (84.528)\n",
            "Epoch: [3][520/710]\tLoss 0.4724 (0.6361)\tAccuracy 93.750 (84.597)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# âœ” **Main_generate_ortho**"
      ],
      "metadata": {
        "id": "N0cAq65cetl_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# upload a pathon file with some useful functions\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "JiMboOfQezqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import os\n",
        "import time\n",
        "import shutil\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim\n",
        "import torch.utils.data\n",
        "import torch.utils.data.distributed\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import datetime\n",
        "import resnet_v1 as resnet"
      ],
      "metadata": {
        "id": "MhFGht6lgNVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check how many GPUs are available\n",
        "num_gpus = torch.cuda.device_count()\n",
        "print(f'Number of GPUs available: {num_gpus}')"
      ],
      "metadata": {
        "id": "dicuN0Tye57N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "now = datetime.datetime.now()\n",
        "time_str = now.strftime(\"[%m-%d]-[%H-%M]-\")"
      ],
      "metadata": {
        "id": "4pUn-UHZe_Ul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/FER/Dataset/RAFDB/Train.zip"
      ],
      "metadata": {
        "id": "xly2j6IzhAsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/Test_FER/Dataset/RAFDB/Test.zip"
      ],
      "metadata": {
        "id": "h9iKabk_hLjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init():\n",
        "\n",
        "  parser = argparse.ArgumentParser(description=\"PyTorch\")\n",
        "\n",
        "  parser.add_argument('--data', type=str, default='/content/RAFDB/')\n",
        "  parser.add_argument('-j', '--workers', default=4, type=int, metavar='N', help='number of data loading workers')\n",
        "  parser.add_argument('-b', '--batch-size', default=8, type=int, metavar='N')\n",
        "\n",
        "  args = parser.parse_args(args=[])\n",
        "  return args"
      ],
      "metadata": {
        "id": "858rk6wxfbbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
        "    with torch.no_grad():\n",
        "        maxk = max(topk)\n",
        "        batch_size = target.size(0)\n",
        "        _, pred = output.topk(maxk, 1, True, True)\n",
        "        pred = pred.t()\n",
        "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "        res = []\n",
        "        for k in topk:\n",
        "            # .contiguousè®©åœ°å€è¿žç»­\n",
        "            correct_k = correct[:k].contiguous().view(-1).float().sum(0, keepdim=True)\n",
        "            res.append(correct_k.mul_(100.0 / batch_size))\n",
        "        return res\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self, name, fmt=':f'):\n",
        "        self.name = name\n",
        "        self.fmt = fmt\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "    def __str__(self):\n",
        "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
        "        return fmtstr.format(**self.__dict__)"
      ],
      "metadata": {
        "id": "tVYIHpVihww4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "\n",
        "    args= init()\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    best_acc = 0\n",
        "\n",
        "    print('Training time: ' + now.strftime(\"%m-%d %H:%M\"))\n",
        "\n",
        "    # create model\n",
        "    model_cla = resnet.resnet50()\n",
        "    model_cla = torch.nn.DataParallel(model_cla).cuda()\n",
        "    model_cla.to(device)\n",
        "    checkpoint = torch.load('/content/drive/MyDrive/FER/checkpoint_cnn/RAFDB/[08-15]-[18-09]-model_best.pth.tar')\n",
        "    pre_trained_dict = checkpoint['state_dict']\n",
        "    for k, v in pre_trained_dict.items():\n",
        "        print(k, v.shape)\n",
        "    model_cla.load_state_dict(pre_trained_dict)\n",
        "\n",
        "    # Data loading code\n",
        "    traindir = os.path.join(args.data, 'Train')\n",
        "    valdir = os.path.join(args.data, 'Test')\n",
        "\n",
        "    normalize = transforms.Normalize(mean=[0.5758095, 0.4500876, 0.40176094],\n",
        "                                      std=[0.20888616, 0.19142343, 0.18289249])\n",
        "\n",
        "    train_dataset = datasets.ImageFolder(traindir,\n",
        "                                         transforms.Compose([transforms.Resize((224, 224)),\n",
        "                                                             transforms.RandomHorizontalFlip(),\n",
        "                                                             transforms.ToTensor(),\n",
        "                                                             normalize]))\n",
        "\n",
        "    test_dataset = datasets.ImageFolder(valdir,\n",
        "                                        transforms.Compose([transforms.Resize((224, 224)),\n",
        "                                                            transforms.ToTensor(),\n",
        "                                                            normalize]))\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset,\n",
        "                                               batch_size=args.batch_size,\n",
        "                                               shuffle=True,\n",
        "                                               num_workers=args.workers,\n",
        "                                               pin_memory=True)\n",
        "    val_loader = torch.utils.data.DataLoader(test_dataset,\n",
        "                                             batch_size=args.batch_size,\n",
        "                                             shuffle=False,\n",
        "                                             num_workers=args.workers,\n",
        "                                             pin_memory=True)\n",
        "    model_cla.eval()\n",
        "    feature_1 = []\n",
        "    feature_2 = []\n",
        "    feature_3 = []\n",
        "    label = []\n",
        "    top1 = AverageMeter('Accuracy', ':6.3f')\n",
        "    with torch.no_grad():\n",
        "        for i, (images, target) in enumerate(val_loader):\n",
        "            images = images.cuda()\n",
        "            target = target.cuda()\n",
        "            # compute output\n",
        "            x_1, x_2, x_3, x_fc1, x_fc2, x_fc3, output = model_cla(images)\n",
        "            acc1, _ = accuracy(output, target, topk=(1, 5))\n",
        "            top1.update(acc1[0], images.size(0))\n",
        "            x_1 = x_1.permute(0, 2, 3, 1)\n",
        "            x_2 = x_2.permute(0, 2, 3, 1)\n",
        "            x_3 = x_3.permute(0, 2, 3, 1)\n",
        "            if i == 0:\n",
        "                feature_1 = x_1.cpu().numpy()\n",
        "                feature_2 = x_2.cpu().numpy()\n",
        "                feature_3 = x_3.cpu().numpy()\n",
        "                label = target.cpu().numpy()\n",
        "            else:\n",
        "                feature_1 = np.concatenate((feature_1, x_1.cpu().numpy()),axis=0)\n",
        "                feature_2 = np.concatenate((feature_2, x_2.cpu().numpy()),axis=0)\n",
        "                feature_3 = np.concatenate((feature_3, x_3.cpu().numpy()),axis=0)\n",
        "                label = np.concatenate((label, target.cpu().numpy()),axis=0)\n",
        "\n",
        "        print(' *** Accuracy {top1.avg:.3f}  *** '.format(top1=top1))\n",
        "    # train\n",
        "    # np.save(\"/content/drive/MyDrive/FER/Orthognal_npy/train_1_RAFDB2_v1.npy\",feature_1)\n",
        "    # np.save(\"/content/drive/MyDrive/FER/Orthognal_npy/train_2_RAFDB2_v1.npy\",feature_2)\n",
        "    # np.save(\"/content/drive/MyDrive/FER/Orthognal_npy/train_3_RAFDB2_v1.npy\",feature_3)\n",
        "    # np.save(\"/content/drive/MyDrive/FER/Orthognal_npy/train_label_RAFDB2_v1.npy\",label)\n",
        "    # # test\n",
        "    np.save(\"/content/drive/MyDrive/FER/Orthognal_npy/test_1_RAFDB2_v1.npy\",feature_1)\n",
        "    np.save(\"/content/drive/MyDrive/FER/Orthognal_npy/test_2_RAFDB2_v1.npy\",feature_2)\n",
        "    np.save(\"/content/drive/MyDrive/FER/Orthognal_npy/test_3_RAFDB2_v1.npy\",feature_3)\n",
        "    np.save(\"/content/drive/MyDrive/FER/Orthognal_npy/test_label_RAFDB2_v1.npy\",label)\n"
      ],
      "metadata": {
        "id": "YNTnrMj6hru2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "BhQaqLAeg_4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ”´**Q-vit_RAFDB_Upload**"
      ],
      "metadata": {
        "id": "Hi74EF8ylgur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/FER/complexnn.zip"
      ],
      "metadata": {
        "id": "rCFZJdpNwFBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wsgiref import validate\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(42)\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "id": "BKcl998flmlb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U tensorflow-addons"
      ],
      "metadata": {
        "id": "rWr8K7ITmKPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_addons as tfa\n",
        "import os"
      ],
      "metadata": {
        "id": "oTEWIYm1lsjU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
        "config = tf.compat.v1.ConfigProto()\n",
        "config.gpu_options.allow_growth=True\n",
        "sess = tf.compat.v1.Session(config=config)"
      ],
      "metadata": {
        "id": "erOuOZqLnJh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.random.seed(42)"
      ],
      "metadata": {
        "id": "ro9TeNg-nTcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load orthogonal features\n",
        "train_1=np.load('/content/drive/MyDrive/FER/Orthognal_npy/train_1_RAFDB2_v1.npy',encoding = \"latin1\")\n",
        "train_2=np.load('/content/drive/MyDrive/FER/Orthognal_npy/train_2_RAFDB2_v1.npy',encoding = \"latin1\")\n",
        "train_3=np.load('/content/drive/MyDrive/FER/Orthognal_npy/train_3_RAFDB2_v1.npy',encoding = \"latin1\")\n",
        "train_label=np.load('/content/drive/MyDrive/FER/Orthognal_npy/train_label_RAFDB2_v1.npy',encoding = \"latin1\")\n",
        "test_1=np.load('/content/drive/MyDrive/FER/Orthognal_npy/test_1_RAFDB2_v1.npy',encoding = \"latin1\")\n",
        "test_2=np.load('/content/drive/MyDrive/FER/Orthognal_npy/test_2_RAFDB2_v1.npy',encoding = \"latin1\")\n",
        "test_3=np.load('/content/drive/MyDrive/FER/Orthognal_npy/test_3_RAFDB2_v1.npy',encoding = \"latin1\")\n",
        "test_label=np.load('/content/drive/MyDrive/FER/Orthognal_npy/test_label_RAFDB2_v1.npy',encoding = \"latin1\")"
      ],
      "metadata": {
        "id": "Ht1ShW_gnWfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# average the three sub-features and put them into a quaternion matrix\n",
        "q_train=np.zeros([train_1.shape[0],train_1.shape[1],train_1.shape[2],train_1.shape[-1]*4])\n",
        "train_r=(train_1+train_2+train_3)/3\n",
        "q_train[:,:,:,:train_1.shape[-1]]=train_r\n",
        "q_train[:,:,:,train_1.shape[-1]:2*train_1.shape[-1]]=train_1\n",
        "q_train[:,:,:,2*train_1.shape[-1]:3*train_1.shape[-1]]=train_2\n",
        "q_train[:,:,:,3*train_1.shape[-1]:]=train_3\n",
        "train = np.transpose(q_train,(0,3,1,2))\n",
        "train = np.reshape(train,(train_1.shape[0],64*4,49))  # 256 --> 64\n",
        "\n",
        "q_test=np.zeros([test_1.shape[0],test_1.shape[1],test_1.shape[2],test_1.shape[-1]*4])\n",
        "test_r=(test_1+test_2+test_3)/3\n",
        "q_test[:,:,:,:test_1.shape[-1]]=test_r\n",
        "q_test[:,:,:,test_1.shape[-1]:2*test_1.shape[-1]]=test_1\n",
        "q_test[:,:,:,2*test_1.shape[-1]:3*test_1.shape[-1]]=test_2\n",
        "q_test[:,:,:,3*test_1.shape[-1]:]=test_3\n",
        "test = np.transpose(q_test,(0,3,1,2))\n",
        "test = np.reshape(test,(test_1.shape[0],64*4,49)) # 256 --> 64\n",
        "\n",
        "\n",
        "input_shape = (64*4, 49) # 256 --> 64\n",
        "num_classes = 7\n",
        "learning_rate = 0.00001\n",
        "weight_decay = 0.0001\n",
        "batch_size = 8\n",
        "num_epochs = 100  # 400 >>> 100\n",
        "num_patches = 64*4  # 256 --> 64\n",
        "projection_dim = 48\n",
        "num_heads = 8\n",
        "transformer_units = [\n",
        "    projection_dim * 2,\n",
        "    projection_dim,\n",
        "]\n",
        "transformer_layers = 4\n",
        "mlp_head_units = [2048, 256]  # 1024 --> 256\n",
        "\n",
        "## go complexnn/init.py and change the \"from keras.utils.generic_utils.....\" to\n",
        "## \"from tensorflow.keras.utils import (serialize_keras_object, deserialize_keras_object)\"\n",
        "from   complexnn      import *\n",
        "from tensorflow.keras.layers import (\n",
        "    Dense,\n",
        ")"
      ],
      "metadata": {
        "id": "9JYGgkg-sq7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q-MHSA module\n",
        "class MultiHeadSelfAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        if embed_dim % num_heads != 0:\n",
        "            raise ValueError(\n",
        "                f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\"\n",
        "            )\n",
        "        self.projection_dim = embed_dim // num_heads\n",
        "        self.query_dense = QuaternionDense(embed_dim)\n",
        "        self.key_dense = QuaternionDense(embed_dim)\n",
        "        self.value_dense = QuaternionDense(embed_dim)\n",
        "        self.combine_heads = QuaternionDense(embed_dim)\n",
        "\n",
        "    def attention(self, query, key, value):\n",
        "        score = tf.matmul(query, key, transpose_b=True)\n",
        "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
        "        scaled_score = score / tf.math.sqrt(dim_key)\n",
        "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
        "        output = tf.matmul(weights, value)\n",
        "        return output, weights\n",
        "\n",
        "    def separate_heads(self, x, batch_size):\n",
        "        x = tf.reshape(\n",
        "            x, (batch_size, -1, self.num_heads, self.projection_dim)\n",
        "        )\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, inputs):\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        query = self.query_dense(inputs)\n",
        "        key = self.key_dense(inputs)\n",
        "        value = self.value_dense(inputs)\n",
        "\n",
        "        query = self.separate_heads(query, batch_size)\n",
        "        key = self.separate_heads(key, batch_size)\n",
        "        value = self.separate_heads(value, batch_size)\n",
        "\n",
        "        attention, weights = self.attention(query, key, value)\n",
        "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
        "        concat_attention = tf.reshape(\n",
        "            attention, (batch_size, -1, self.embed_dim)\n",
        "        )\n",
        "        output = self.combine_heads(concat_attention)\n",
        "        return output"
      ],
      "metadata": {
        "id": "g0ROyhZswSz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def QF_Net(x, hidden_units, dropout_rate):\n",
        "    for units in hidden_units:\n",
        "        x = QuaternionConv2D(int(units/4), 3, strides=1, padding=\"same\")(x)\n",
        "        x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "        x = layers.Activation(tf.nn.gelu)(x)\n",
        "        x = QuaternionConv2D(int(units/4), 3, strides=1, padding=\"same\")(x)\n",
        "    return x\n",
        "\n",
        "def multilayer_perceptron(x, hidden_units, dropout_rate):\n",
        "    for units in hidden_units:\n",
        "        x = QuaternionDense(units, activation='relu')(x)\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "class PatchEncoder(layers.Layer):\n",
        "    def __init__(self, num_patches, projection_dim):\n",
        "        super(PatchEncoder, self).__init__()\n",
        "        self.num_patches = num_patches\n",
        "        # é™ä¸ºå…¨è¿žæŽ¥\n",
        "        self.projection = layers.Dense(units=projection_dim)\n",
        "        self.position_embedding = layers.Embedding(\n",
        "            input_dim=num_patches, output_dim=projection_dim\n",
        "        )\n",
        "\n",
        "    def call(self, patch):\n",
        "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
        "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
        "        # encoded = patch + self.position_embedding(positions)\n",
        "        return encoded\n",
        "\n",
        "\n",
        "def create_qvit_classifier():\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "\n",
        "    # position embedding\n",
        "    encoded_patches = PatchEncoder(num_patches, projection_dim)(inputs)\n",
        "\n",
        "    for _ in range(transformer_layers):\n",
        "\n",
        "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "\n",
        "        attention_output = MultiHeadSelfAttention(projection_dim, num_heads)(x1)\n",
        "\n",
        "        x2 = layers.Add()([attention_output, encoded_patches])\n",
        "\n",
        "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "\n",
        "        x4 = tf.keras.layers.Reshape((16,16,48))(x3) # 32*32 --> 16*16\n",
        "\n",
        "        x5 = QF_Net(x4, hidden_units=transformer_units, dropout_rate=0.3)\n",
        "\n",
        "        x6 = tf.keras.layers.Reshape((64*4, 48))(x5) #256-->64\n",
        "\n",
        "        encoded_patches = layers.Add()([x6, x2])\n",
        "\n",
        "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "    representation = layers.Flatten()(representation)\n",
        "    representation = layers.Dropout(0.5)(representation)\n",
        "\n",
        "    features = multilayer_perceptron(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
        "\n",
        "    logits = layers.Dense(num_classes)(features)\n",
        "\n",
        "    model = keras.Model(inputs=inputs, outputs=logits)\n",
        "    return model\n",
        "\n",
        "def run_experiment(model):\n",
        "    optimizer = tf.optimizers.Adam(\n",
        "        learning_rate=learning_rate\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        metrics=[\n",
        "            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
        "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    checkpoint_filepath = \"./tmp/RAFDB/model_{epoch:03d}-{val_accuracy:.4f}.h5\"\n",
        "    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "        checkpoint_filepath,\n",
        "        monitor=\"val_accuracy\",\n",
        "        save_best_only=True,\n",
        "        save_weights_only=True,\n",
        "    )\n",
        "\n",
        "    history = model.fit(\n",
        "        x=train,\n",
        "        y=train_label,\n",
        "        batch_size=batch_size,\n",
        "        epochs=num_epochs,\n",
        "        validation_data=(test, test_label),\n",
        "        callbacks=[checkpoint_callback],\n",
        "    )\n",
        "\n",
        "    return history"
      ],
      "metadata": {
        "id": "v_LFR15gz4hf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vit_classifier = create_qvit_classifier()\n",
        "history = run_experiment(vit_classifier)"
      ],
      "metadata": {
        "id": "7xJRn5GD03aM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}