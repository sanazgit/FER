{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7170447,"sourceType":"datasetVersion","datasetId":4142841},{"sourceId":7184026,"sourceType":"datasetVersion","datasetId":4152817},{"sourceId":13463581,"sourceType":"datasetVersion","datasetId":8546247},{"sourceId":13494427,"sourceType":"datasetVersion","datasetId":8567773},{"sourceId":13495867,"sourceType":"datasetVersion","datasetId":8568712},{"sourceId":151259250,"sourceType":"kernelVersion"},{"sourceId":151303452,"sourceType":"kernelVersion"},{"sourceId":151819745,"sourceType":"kernelVersion"}],"dockerImageVersionId":30636,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import necessary libraries\n!pip install retina-face","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!git clone https://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@github.com/sanazgit/FER.git","metadata":{"execution":{"iopub.status.busy":"2025-10-28T06:45:37.890671Z","iopub.execute_input":"2025-10-28T06:45:37.891015Z","iopub.status.idle":"2025-10-28T06:45:39.612503Z","shell.execute_reply.started":"2025-10-28T06:45:37.890983Z","shell.execute_reply":"2025-10-28T06:45:39.611635Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Cloning into 'FER'...\nremote: Enumerating objects: 344, done.\u001b[K\nremote: Counting objects: 100% (65/65), done.\u001b[K\nremote: Compressing objects: 100% (21/21), done.\u001b[K\nremote: Total 344 (delta 53), reused 44 (delta 44), pack-reused 279 (from 1)\u001b[K\nReceiving objects: 100% (344/344), 1.19 MiB | 9.41 MiB/s, done.\nResolving deltas: 100% (186/186), done.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import sys\nsys.path.append('./FER')","metadata":{"execution":{"iopub.status.busy":"2025-10-28T06:55:28.027455Z","iopub.execute_input":"2025-10-28T06:55:28.027818Z","iopub.status.idle":"2025-10-28T06:55:28.032501Z","shell.execute_reply.started":"2025-10-28T06:55:28.027789Z","shell.execute_reply":"2025-10-28T06:55:28.031487Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import zipfile\nwith zipfile.ZipFile('/kaggle/working/FER/complexnn.zip', 'r') as z:\n    z.extractall('/kaggle/working/')","metadata":{"execution":{"iopub.status.busy":"2025-10-28T06:55:29.706905Z","iopub.execute_input":"2025-10-28T06:55:29.707246Z","iopub.status.idle":"2025-10-28T06:55:29.720338Z","shell.execute_reply.started":"2025-10-28T06:55:29.707222Z","shell.execute_reply":"2025-10-28T06:55:29.719587Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import cv2\nimport numpy as np\nimport tensorflow as tf\nfrom retinaface import RetinaFace\nfrom LA_QVIT import create_qvit_classifier\nfrom torchvision import transforms\nimport torch\nimport random\nimport shutil\nimport math\nimport resnet_pose_attention_v2 as resnet\nimport time\nfrom PIL import Image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T06:55:31.087973Z","iopub.execute_input":"2025-10-28T06:55:31.088320Z","iopub.status.idle":"2025-10-28T06:55:32.702222Z","shell.execute_reply.started":"2025-10-28T06:55:31.088290Z","shell.execute_reply":"2025-10-28T06:55:32.701473Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def preprocess_frame(frame):\n    # Convert BGR (OpenCV format) to RGB\n    if frame.shape[2] == 3:  # Check if the frame has 3 channels\n        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n\n    mytransform = transforms.Compose([\n        transforms.ToPILImage(),  # Convert ndarray to PIL Image\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n\n    # Apply the transformations\n    frame = mytransform(frame)\n    frame = torch.unsqueeze(frame, 0)  # Add batch dimension\n    return frame\n\n\ndef rotate(ps,m):\n    pts = np.float32(ps).reshape([-1, 2])\n    pts = np.hstack([pts, np.ones([len(pts), 1])]).T\n    target_point = np.dot(m, pts)\n    target_point = [[target_point[0][x],target_point[1][x]] for x in range(len(target_point[0]))]\n    return target_point\n\ndef rotate_img_and_point(img,points,angle,center_x,center_y,resize_rate=1.0):\n    h,w,c = img.shape\n    M = cv2.getRotationMatrix2D((center_x,center_y), angle, resize_rate)\n    res_img = cv2.warpAffine(img, M, (w, h))\n    out_points = rotate(points,M)\n    return res_img,out_points\n\n\ndef Feature_Orthogonal(image, rect, rect_local, model_cla):\n    if not rect_local or not isinstance(rect_local[0], list) or len(rect_local[0]) < 2:\n        print(\"Error: rect_local is not in the expected format.\")\n        return None\n\n    with torch.no_grad():\n        image = image.cuda()\n        model_cla.module.set_rect(rect)\n        model_cla.module.set_rect_local(rect_local)\n        x_gf_1, x_gf_2, x_gf_3, _, _, _, _ = model_cla(image)\n\n        # Convert PyTorch tensors to NumPy arrays\n        x_gf_1 = x_gf_1.cpu().numpy()\n        x_gf_2 = x_gf_2.cpu().numpy()\n        x_gf_3 = x_gf_3.cpu().numpy()\n\n        return x_gf_1, x_gf_2, x_gf_3\n\n    \n\ndef load_emotin_model():\n    model = create_qvit_classifier()\n    model.load_weights('/kaggle/input/la-qvit-main4/tmp/RAFDB/model_018-0.8893.h5')\n    return model  \n\ndef load_Orf_model():\n    model = resnet.resnet50()  # Use pretrained=False if not using pretrained model\n    model = torch.nn.DataParallel(model).cuda()\n    checkpoint = torch.load('/kaggle/input/la-qvit-orfe0-1/checkpoint_cnn/[11-18]-[10-35]-model_best.pth.tar')\n    model.load_state_dict(checkpoint['state_dict'])\n    model.eval()\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T06:55:35.247783Z","iopub.execute_input":"2025-10-28T06:55:35.248441Z","iopub.status.idle":"2025-10-28T06:55:35.259099Z","shell.execute_reply.started":"2025-10-28T06:55:35.248410Z","shell.execute_reply":"2025-10-28T06:55:35.258193Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def pre_processing(frame, facial, p, pp, l):\n    im = frame.squeeze(0).transpose((1, 2, 0))  # Remove batch dimension and transpose\n\n    for face_landmarks in facial:\n        single_face_landmarks = np.array(face_landmarks)\n\n        if random.random() < p:  # Image rotation\n            resize_rate = round(random.uniform(0.9, 1.1), 2)\n            angle = random.randint(-15, 15)\n            im, single_face_landmarks = rotate_img_and_point(im, single_face_landmarks, angle, 112, 112, resize_rate)\n\n        if random.random() < p:  # Image translation\n            horizon = random.randint(-20, 20)\n            vertial = random.randint(-20, 20)\n            mat_translation = np.float32([[1, 0, horizon], [0, 1, vertial]])\n            im = cv2.warpAffine(im, mat_translation, (224, 224))\n            \n            for j in range(len(single_face_landmarks)):\n                single_face_landmarks[j][0]= single_face_landmarks[j][0]+horizon\n                single_face_landmarks[j][1] = single_face_landmarks[j][1] + vertial\n                if single_face_landmarks[j][0]<0:\n                    single_face_landmarks[j][0]=0\n                if single_face_landmarks[j][0]>224:\n                    single_face_landmarks[j][0]=224\n                if single_face_landmarks[j][1] < 0:\n                    single_face_landmarks[j][1] = 0\n                if single_face_landmarks[j][1] > 224:\n                    single_face_landmarks[j][1] = 224\n    \n\n\n        if random.random() < pp:  # Image flipping\n            im = np.fliplr(im)\n            landmark=single_face_landmarks\n            single_face_landmarks[0]=  [224-landmark[1][0] ,  landmark[1][1]]\n            single_face_landmarks[1] = [224 - landmark[0][0], landmark[0][1]]\n            single_face_landmarks[2] = [224 - landmark[2][0], landmark[2][1]]\n            single_face_landmarks[3] = [224 - landmark[4][0], landmark[4][1]]\n            single_face_landmarks[4] = [224 - landmark[3][0], landmark[3][1]]\n  \n\n        frame = im.transpose((2, 0, 1))\n    \n    \n    facial=np.array(facial)\n    facial[facial<0]=0\n    facial[facial > 224] = 224\n        \n    rect_all=[]\n        \n    for i in range(len(facial)):\n        rect = []\n        rect_local = []\n\n        land_resize=np.around(facial[i]*(28/224)).astype(int)\n\n        a_width = int((land_resize[0][0] + land_resize[1][0]) / 2)\n        a_high = int(land_resize[2][1])\n        min_length=min(a_high, a_width,28-a_width)\n        if min_length>=28/3:\n            rect.append([a_width - min_length, a_high - min_length, a_width, a_high])\n            rect.append([a_width + min_length, a_high - min_length, a_width, a_high])\n        if min_length<28/3:\n            eyemin=np.array([a_high, a_width,28-a_width])\n            a_width_ind=np.where(eyemin==eyemin.min())[0][0]\n            if a_width_ind==0:\n                rect.append([a_width - min_length, a_high - min_length, a_width, a_high])\n                rect.append([a_width + min_length, a_high - min_length, a_width, a_high])\n            if a_width_ind==1:\n                rect.append([a_width - min_length, a_high - min_length, a_width, a_high])\n                min_eye_length1 = min(a_high,28-a_width, 14)\n                rect.append([a_width + min_eye_length1, a_high - min_eye_length1, a_width, a_high])\n            if a_width_ind==2:\n                min_eye_length2 = min(a_high,a_width, 14)\n                rect.append([a_width - min_eye_length2, a_high - min_eye_length2, a_width, a_high])\n                rect.append([a_width + min_length, a_high - min_length, a_width, a_high])\n\n\n        b_width = int((land_resize[3][0] + land_resize[4][0]) / 2)\n        min_mou_length=min(28-a_high, b_width,28-b_width)\n        if min_mou_length>=28/3:\n            rect.append([b_width - min_mou_length, a_high + min_mou_length, b_width, a_high])\n            rect.append([b_width + min_mou_length, a_high + min_mou_length, b_width, a_high])\n        if min_mou_length<28/3:\n            moumin=np.array([28-a_high, b_width,28-b_width])\n            moumin_ind=np.where(moumin==moumin.min())[0][0]\n            if moumin_ind==0:\n                rect.append([b_width - min_mou_length, a_high + min_mou_length, b_width, a_high])\n                rect.append([b_width + min_mou_length, a_high + min_mou_length, b_width, a_high])\n            if moumin_ind==1:\n                rect.append([b_width - min_mou_length, a_high + min_mou_length, b_width, a_high])\n                min_mou_length1 = min(28 - a_high, 28 - b_width,14)\n                rect.append([b_width + min_mou_length1, a_high + min_mou_length1, b_width, a_high])\n            if moumin_ind==2:\n                min_mou_length2 = min(28 - a_high, b_width,14)\n                rect.append([b_width - min_mou_length2, a_high + min_mou_length2, b_width, a_high])\n                rect.append([b_width + min_mou_length, a_high + min_mou_length, b_width, a_high])\n\n\n        land=land_resize\n        eye1 = land[0]\n        eye2 = land[1]\n        eye_midle = (eye1 + eye2) / 2\n        mouth1 = land[3]\n        mouth2 = land[4]\n        landmark = np.array([eye1, eye2, eye_midle, mouth1, mouth2]).astype(int)\n\n        for j in range(len(landmark)):\n            if landmark[j][0] < l:\n                landmark[j][0] = l\n            if landmark[j][0] + l > 28:\n                landmark[j][0] = 28 - l\n            if landmark[j][1] < l:\n                landmark[j][1] = l\n            if landmark[j][1] > 28 - l:\n                landmark[j][1] = 28 - l\n\n        rect.append(landmark)\n        rect_all.append(rect)\n\n\n    return rect_all\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T06:55:36.464226Z","iopub.execute_input":"2025-10-28T06:55:36.464922Z","iopub.status.idle":"2025-10-28T06:55:36.484726Z","shell.execute_reply.started":"2025-10-28T06:55:36.464875Z","shell.execute_reply":"2025-10-28T06:55:36.483615Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def pre_pro(frame, facial, p, pp, l):\n    images = frame.numpy() if not isinstance(frame, np.ndarray) else frame\n    \n    # Call pre_processing for the single image and facial landmarks\n    rect_all = pre_processing(images, facial, p, pp, l)\n    \n    rect = []\n    for i in range(len(rect_all)):\n        # Convert scalars to 1D arrays and then concatenate\n        rect_part = [np.array([rect_all[i][j]]) for j in range(4)]\n        concatenated = np.concatenate(rect_part, axis=0)\n        rect.append(concatenated)\n\n    # Process rect_local\n    rect_local = []\n    for i in range(len(rect_all)):\n        if len(rect_all[i]) > 4:  # Ensure the 5th element (facial landmarks array) exists\n            landmark_array = rect_all[i][4]\n            local = []\n            for landmark in landmark_array:\n                # Check if landmark is an array or list with at least two elements\n                if isinstance(landmark, (list, np.ndarray)) and len(landmark) > 1:\n                    local.extend([landmark[0] - l, landmark[0] + l, landmark[1] - l, landmark[1] + l])\n            rect_local.append(local)\n\n    # Convert image to a PyTorch tensor\n    image_tensor = torch.tensor(images)\n\n    return image_tensor, rect, rect_local\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T06:55:43.876433Z","iopub.execute_input":"2025-10-28T06:55:43.877303Z","iopub.status.idle":"2025-10-28T06:55:43.883845Z","shell.execute_reply.started":"2025-10-28T06:55:43.877266Z","shell.execute_reply":"2025-10-28T06:55:43.882971Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def pre_pro_infer(frame_tensor_224, face_landmarks_224, l=5):\n\n    image, rect, rect_local = pre_pro(frame_tensor_224, [face_landmarks_224], 0.0, 0.0, l)\n    return image, rect, rect_local","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T06:55:47.607580Z","iopub.execute_input":"2025-10-28T06:55:47.608594Z","iopub.status.idle":"2025-10-28T06:55:47.613222Z","shell.execute_reply.started":"2025-10-28T06:55:47.608562Z","shell.execute_reply":"2025-10-28T06:55:47.612008Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def detect_faces_and_landmarks(frame, face_detector):\n    detections = RetinaFace.detect_faces(frame, model=face_detector)\n    if isinstance(detections, tuple) or detections is None:\n        return [], []\n\n    faces = []\n    landmarks_per_face = []\n\n    for k, face in detections.items():\n        x1, y1, x2, y2 = face['facial_area']\n        x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n        w, h = max(1, x2 - x1), max(1, y2 - y1)\n\n\n        lm = np.array(list(face['landmarks'].values()), dtype=np.float32)  # (5, 2)\n\n       \n        lm_crop = np.stack([\n            (lm[:, 0] - x1) * (224.0 / w),\n            (lm[:, 1] - y1) * (224.0 / h)\n        ], axis=1)\n\n        faces.append((x1, y1, x2, y2))\n        landmarks_per_face.append(lm_crop)\n\n    return faces, landmarks_per_face","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T06:55:48.948213Z","iopub.execute_input":"2025-10-28T06:55:48.948943Z","iopub.status.idle":"2025-10-28T06:55:48.955178Z","shell.execute_reply.started":"2025-10-28T06:55:48.948910Z","shell.execute_reply":"2025-10-28T06:55:48.954297Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"import cv2\n\nvideo = cv2.VideoCapture('./your_video.mp4')\n\n# Get the FPS of the video\nfps = video.get(cv2.CAP_PROP_FPS)\nprint(\"Frames per second:\", fps)\nvideo.release()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T06:55:51.547724Z","iopub.execute_input":"2025-10-28T06:55:51.548071Z","iopub.status.idle":"2025-10-28T06:55:51.612098Z","shell.execute_reply.started":"2025-10-28T06:55:51.548045Z","shell.execute_reply":"2025-10-28T06:55:51.611300Z"}},"outputs":[{"name":"stdout","text":"Frames per second: 29.772408488303697\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"def draw_optimized(frame, x1, y1, x2, y2, box_color=(10, 200, 10), margin_size=10):\n    # Adjust the coordinates for the margin\n    x1_margin = max(x1 - margin_size, 0)  # Ensure x1_margin is not less than 0\n    y1_margin = max(y1 - margin_size, 0)  # Ensure y1_margin is not less than 0\n    x2_margin = min(x2 + margin_size, frame.shape[1])  # Ensure x2_margin does not exceed frame width\n    y2_margin = min(y2 + margin_size, frame.shape[0])  # Ensure y2_margin does not exceed frame height\n\n    # Draw a bounding box with margin\n    cv2.rectangle(frame, (x1_margin, y1_margin), (x2_margin, y2_margin), box_color, 2)\n\n    # Use pre-calculated text size\n\n    # Prepare text background with standard size\n    cv2.rectangle(frame, (x1_margin, y1_margin), (x1_margin, y1_margin), box_color, -1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T08:02:27.741097Z","iopub.execute_input":"2025-10-28T08:02:27.741840Z","iopub.status.idle":"2025-10-28T08:02:27.748298Z","shell.execute_reply.started":"2025-10-28T08:02:27.741808Z","shell.execute_reply":"2025-10-28T08:02:27.747404Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"# --- helper: 5-point similarity alignment to 224x224\ndef align_face_5pt(img_crop_bgr, lm_224):\n    dst = np.float32([\n        [70, 96], [154, 96], [112, 128], [84, 164], [140, 164],\n    ])\n    src = np.float32(lm_224)\n    H, _ = cv2.estimateAffinePartial2D(src.reshape(-1,1,2), dst.reshape(-1,1,2), method=cv2.LMEDS)\n    crop_resized = cv2.resize(img_crop_bgr, (224,224))\n    aligned = cv2.warpAffine(crop_resized, H, (224,224), flags=cv2.INTER_LINEAR)\n    return aligned, H\n\ndef put_probs(frame, x, y, probs, labels, k=7):\n    s = [f\"{labels[i]}: {probs[0,i]:.2f}\" for i in np.argsort(-probs[0])[:k]]\n    y0 = y - 10 if y - 10 > 10 else y + 20\n    for i, t in enumerate(s):\n        cv2.putText(frame, t, (x+5, y0 + 18*i), cv2.FONT_HERSHEY_SIMPLEX, 0.55, (255,255,255), 2, cv2.LINE_AA)\n\ndef draw_lm_on_224(img_bgr_224, lm_224):\n    out = img_bgr_224.copy()\n    for (px,py) in lm_224.astype(int):\n        cv2.circle(out, (int(px),int(py)), 3, (0,255,255), -1)\n    return out\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T08:02:29.048455Z","iopub.execute_input":"2025-10-28T08:02:29.049205Z","iopub.status.idle":"2025-10-28T08:02:29.057111Z","shell.execute_reply.started":"2025-10-28T08:02:29.049171Z","shell.execute_reply":"2025-10-28T08:02:29.056116Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"def main(video_path):\n\n    # ---------------- Models ----------------\n    model = load_emotin_model()   \n    model_cla = load_Orf_model()  \n    torch.set_grad_enabled(False)\n\n    emotion_labels = ['anger','disgust','fear','happy','neutral','sad','surprise']\n\n    # ---- Prior correction (RAF) + Temperature ----\n    raf_priors = np.array([\n        0.0575,  # anger\n        0.0584,  # disgust\n        0.0229,  # fear\n        0.3889,  # happy\n        0.2057,  # neutral\n        0.1615,  # sad\n        0.1051   # surprise\n    ], dtype=np.float32)\n    target_priors = np.ones_like(raf_priors) / len(raf_priors)  \n    SURPRISE_BOOST = 1.05  \n\n    class_thresholds = {\n        'happy': 0.58,\n        'neutral': 0.45,\n        'surprise': 0.28,\n        'anger': 0.35, 'disgust': 0.35, 'fear': 0.35, 'sad': 0.38\n    }\n    to_tensor_and_norm = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n    ])\n\n    text_sizes = {\n        f'Emotion: {lbl}': cv2.getTextSize('Emotion...', cv2.FONT_HERSHEY_SIMPLEX, 0.9, 2)[0]\n        for lbl in emotion_labels + ['Uncertain']\n    }\n\n    # ---------------- Video IO ----------------\n    cap = cv2.VideoCapture(video_path)\n    if not cap.isOpened():\n        print(\"Error: Could not open video.\")\n        return\n\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    fps = cap.get(cv2.CAP_PROP_FPS) or 25\n    W  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    H  = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    out = cv2.VideoWriter('result.mp4', fourcc, fps, (W, H))\n\n \n    face_detector = RetinaFace.build_model()\n\n    # EMA \n    ema_probs = None\n    alpha = 0.6  \n\n    frame_count, total_process_time = 0, 0.0\n\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n        t0 = time.time()\n\n        faces, lm_list = detect_faces_and_landmarks(frame, face_detector)\n        if len(faces) == 0:\n            out.write(frame); frame_count += 1\n            continue\n\n        for (x1, y1, x2, y2), lm_224 in zip(faces, lm_list):\n            face_bgr = frame[y1:y2, x1:x2]\n            if face_bgr.size == 0:\n                continue\n\n            # ---------- Alignment 5-point ----------\n            aligned_bgr_224, _ = align_face_5pt(face_bgr, lm_224)\n\n            # ---------- Preprocess (RGB + Normalize) ----------\n            aligned_rgb_224 = cv2.cvtColor(aligned_bgr_224, cv2.COLOR_BGR2RGB)\n            tens = to_tensor_and_norm(aligned_rgb_224).unsqueeze(0)    # 1×3×224×224\n            frame_prepro = tens\n\n            # ---------- Pre-processing (augmentation) ----------\n            image, rect, rect_local = pre_pro_infer(frame_prepro, lm_224, l=5)\n\n            # ---------- Features ----------\n            orth_f1, orth_f2, orth_f3 = Feature_Orthogonal(image, rect, rect_local, model_cla)\n\n            C = orth_f1.shape[-1]\n            q_test = np.zeros([orth_f1.shape[0], orth_f1.shape[1], orth_f1.shape[2], 4*C], dtype=orth_f1.dtype)\n            temp_sum = (orth_f1 + orth_f2 + orth_f3) / 3.0\n            q_test[:, :, :, 0*C:1*C] = temp_sum\n            q_test[:, :, :, 1*C:2*C] = orth_f1\n            q_test[:, :, :, 2*C:3*C] = orth_f2\n            q_test[:, :, :, 3*C:4*C] = orth_f3\n\n            test = np.transpose(q_test, (0, 3, 1, 2))                 # N, 512, 7, 7 (C=128)\n            test = np.reshape(test, (orth_f1.shape[0], 128*4, 49)).astype(np.float32)\n\n            # ---------- Predict ----------\n            logits = model.predict(test, verbose=0)                    # (1,7)\n            logits_adj = (logits / T) - np.log(raf_priors + 1e-8) + np.log(target_priors + 1e-8)\n\n            idx_surprise = emotion_labels.index('surprise')\n            logits_adj[0, idx_surprise] += np.log(SURPRISE_BOOST)\n\n            probs = tf.nn.softmax(logits_adj).numpy()                  # 1×7\n\n            # ---------- EMA ----------\n            if ema_probs is None:\n                ema_probs = probs.copy()\n            else:\n                ema_probs = alpha * ema_probs + (1 - alpha) * probs\n            probs_use = ema_probs\n\n            # ---------- top-2 logic + class thresholds ----------\n            order = np.argsort(-probs_use[0])\n            top1, top2 = int(order[0]), int(order[1])\n            p1, p2 = float(probs_use[0, top1]), float(probs_use[0, top2])\n            lab1, lab2 = emotion_labels[top1], emotion_labels[top2]\n\n            MIN_FOR_TOP2 = 0.20   \n            DELTA = 0.07          \n\n            if (p1 >= MIN_FOR_TOP2) and (p2 >= MIN_FOR_TOP2) and ((p1 - p2) < DELTA):\n                chosen_label, chosen_prob = lab1, p1\n            else:\n                chosen_label, chosen_prob = lab1, p1\n\n            thr = class_thresholds.get(chosen_label, 0.40)\n            label = chosen_label if chosen_prob >= thr else 'Uncertain'\n\n            # ---------- Draw ----------\n            draw_optimized(frame, x1, y1, x2, y2)\n            put_probs(frame, x1, min(y2 + 20, H - 120), probs_use, emotion_labels)\n\n        total_process_time += (time.time() - t0)\n        out.write(frame)\n        frame_count += 1\n\n    avg_fps = frame_count / total_process_time if total_process_time > 0 else 0.0\n    print(f\"Average FPS: {avg_fps:.2f}\\nTotal Processed Frames: {frame_count}\")\n\n    cap.release()\n    out.release()\n\nvideo_path = '/your_video.mp4'\nmain(video_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T08:38:26.063752Z","iopub.execute_input":"2025-10-28T08:38:26.064102Z","iopub.status.idle":"2025-10-28T08:44:44.149940Z","shell.execute_reply.started":"2025-10-28T08:38:26.064074Z","shell.execute_reply":"2025-10-28T08:44:44.148961Z"}},"outputs":[{"name":"stdout","text":"Average FPS: 3.11\nTotal Processed Frames: 1124\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}